{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87cbd2aa",
   "metadata": {
    "papermill": {
     "duration": 0.010245,
     "end_time": "2025-04-21T00:51:01.848333",
     "exception": false,
     "start_time": "2025-04-21T00:51:01.838088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📝 AI-Driven CV Optimisation with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696261f",
   "metadata": {},
   "source": [
    "This notebook can be seen in Kaggle: [https://www.kaggle.com/code/franciscomunozg/ai-driven-cv-optimisation-with-rag](https://www.kaggle.com/code/franciscomunozg/ai-driven-cv-optimisation-with-rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad24d49",
   "metadata": {
    "papermill": {
     "duration": 0.00828,
     "end_time": "2025-04-21T00:51:01.865453",
     "exception": false,
     "start_time": "2025-04-21T00:51:01.857173",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fe8e3b",
   "metadata": {
    "papermill": {
     "duration": 0.008816,
     "end_time": "2025-04-21T00:51:01.883005",
     "exception": false,
     "start_time": "2025-04-21T00:51:01.874189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Goal:** This Kaggle notebook implements a Retrieval-Augmented Generation (RAG) system to analyze a Curriculum Vitae (CV) against a specific job description. It identifies the most relevant experiences from the CV and rewrites them, incorporating keywords from the job description to improve visibility for Applicant Tracking Systems (ATS).\n",
    "\n",
    "**Target Audience:** Users aiming to tailor their CV for job applications, suitable for both technical and non-technical users (with guidance). This notebook is designed to run within the Kaggle environment.\n",
    "\n",
    "**Key Features:**\n",
    "* Parses PDF CVs into structured JSON format using Google's Gemini model.\n",
    "* Generates vector embeddings for CV items (work experience, projects, etc.) using `text-embedding-004`.\n",
    "* Stores embeddings in a persistent ChromaDB vector database (using a single collection with metadata).\n",
    "* Performs RAG to find CV items most relevant to a given job description.\n",
    "* Utilizes Gemini via a chat interface to rewrite relevant CV items with ATS optimization and keyword integration.\n",
    "* Supports processing and interaction in multiple languages (e.g., English, Spanish - based on configuration and prompts).\n",
    "\n",
    "**Notebook Workflow:**\n",
    "1.  **Setup:** Install dependencies and configure the Google AI API Key via Kaggle Secrets.\n",
    "2.  **User Configuration:** Set paths, language, processing parameters, and job details.\n",
    "3.  **Load & Process CV:** Read the PDF, extract text, and structure it into JSON.\n",
    "4.  **Embed & Store:** Generate embeddings for CV items and store them in ChromaDB.\n",
    "5.  **RAG Retrieval:** Find relevant CV items based on the job description query.\n",
    "6.  **Rewrite & Refine:** Engage in an interactive chat with the LLM to rewrite and optimize CV items.\n",
    "7.  **Conclusion:** Review results and summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc57789",
   "metadata": {
    "papermill": {
     "duration": 0.008258,
     "end_time": "2025-04-21T00:51:01.899782",
     "exception": false,
     "start_time": "2025-04-21T00:51:01.891524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945b6c5",
   "metadata": {
    "papermill": {
     "duration": 0.008222,
     "end_time": "2025-04-21T00:51:01.916465",
     "exception": false,
     "start_time": "2025-04-21T00:51:01.908243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This section covers the initial setup required to run the notebook within the Kaggle environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b77ab",
   "metadata": {
    "papermill": {
     "duration": 0.00801,
     "end_time": "2025-04-21T00:51:01.933143",
     "exception": false,
     "start_time": "2025-04-21T00:51:01.925133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9240366",
   "metadata": {
    "papermill": {
     "duration": 46.074818,
     "end_time": "2025-04-21T00:51:48.016501",
     "exception": false,
     "start_time": "2025-04-21T00:51:01.941683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# Uninstall jupyterlab first to potentially resolve conflicts in the Kaggle environment\n",
    "!pip uninstall -qqy jupyterlab\n",
    "\n",
    "# Install core AI, PDF processing, vector DB, and download libraries\n",
    "# Using specific versions found in the original notebook where available\n",
    "!pip install -U -q \"google-genai==1.7.0\"\n",
    "!pip install -q pdfplumber chromadb gdown PyYAML\n",
    "\n",
    "print(\"Dependencies installed.\")  # noqa: T201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64076413",
   "metadata": {
    "papermill": {
     "duration": 0.010587,
     "end_time": "2025-04-21T00:51:48.038561",
     "exception": false,
     "start_time": "2025-04-21T00:51:48.027974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2. API Key Configuration (Kaggle Secrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a172b04",
   "metadata": {
    "papermill": {
     "duration": 0.010462,
     "end_time": "2025-04-21T00:51:48.059579",
     "exception": false,
     "start_time": "2025-04-21T00:51:48.049117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To use Google Generative AI models (like Gemini), you need an API key. This notebook is configured to securely access your API key using Kaggle Secrets.\n",
    "\n",
    "**Action Required:**\n",
    "1.  Click on **\"Add-ons\"** in the top menu bar of the Kaggle editor.\n",
    "2.  Select **\"Secrets\"**.\n",
    "3.  Add a new secret with the **name** `GOOGLE_API_KEY` and paste your actual Google AI API key as the **value**.\n",
    "\n",
    "The following code cell will then retrieve this secret. Ensure you have added the secret *before* running the next cell. You can get an API key from [Google AI Studio](https://aistudio.google.com/app/apikey)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af7af19",
   "metadata": {
    "papermill": {
     "duration": 0.106451,
     "end_time": "2025-04-21T00:51:48.176706",
     "exception": false,
     "start_time": "2025-04-21T00:51:48.070255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the Kaggle Secrets client and retrieve the API key\n",
    "\n",
    "try:\n",
    "    # This import works specifically in the Kaggle environment\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "    user_secrets = UserSecretsClient()\n",
    "    # Retrieve the secret named \"GOOGLE_API_KEY\"\n",
    "    GOOGLE_API_KEY = user_secrets.get_secret(\"GOOGLE_API_KEY\")\n",
    "    print(\"Successfully retrieved GOOGLE_API_KEY from Kaggle Secrets.\")  # noqa: T201\n",
    "\n",
    "except ImportError:\n",
    "    GOOGLE_API_KEY = None\n",
    "    print(  # noqa: T201\n",
    "        \"Warning: 'kaggle_secrets' library not found. This notebook is designed to run\"\n",
    "        \" on Kaggle.\"\n",
    "    )\n",
    "    print(\"API key could not be loaded.\")  # noqa: T201\n",
    "\n",
    "except Exception as e:\n",
    "    # Handles cases where the secret might not be set by the user\n",
    "    GOOGLE_API_KEY = None\n",
    "    print(  # noqa: T201\n",
    "        \"Warning: Could not retrieve GOOGLE_API_KEY from Kaggle Secrets.\"\n",
    "    )  # noqa: T201\n",
    "    print(f\"Error details: {e}\")  # noqa: T201\n",
    "    print(  # noqa: T201\n",
    "        \"Please ensure you have added the secret named 'GOOGLE_API_KEY' in Kaggle\"\n",
    "        \" Add-ons -> Secrets.\"\n",
    "    )\n",
    "\n",
    "# Verify if the API key was loaded successfully\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(  # noqa: T201\n",
    "        \"\\nERROR: GOOGLE_API_KEY was not loaded. \\nThe notebook cannot proceed without\"\n",
    "        \" a valid API key.\"\n",
    "    )\n",
    "    # Optionally, raise an error to halt execution:\n",
    "    # raise ValueError(\"GOOGLE_API_KEY not found. Please configure it in Kaggle Secrets.\")\n",
    "else:\n",
    "    # Optional: Print a confirmation snippet (masked)\n",
    "    print(  # noqa: T201\n",
    "        f\"API Key loaded successfully (starting with: {GOOGLE_API_KEY[:4]}...).\"\n",
    "    )  # noqa: T201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcca856",
   "metadata": {
    "papermill": {
     "duration": 0.011349,
     "end_time": "2025-04-21T00:51:48.199290",
     "exception": false,
     "start_time": "2025-04-21T00:51:48.187941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.3. Helper Functions (Retry Logic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb31ee",
   "metadata": {
    "papermill": {
     "duration": 0.010982,
     "end_time": "2025-04-21T00:51:48.221562",
     "exception": false,
     "start_time": "2025-04-21T00:51:48.210580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook interacts with the Google Generative AI API, which might occasionally return errors due to rate limits (too many requests) or temporary server issues. We define a helper function (`is_retriable`) to identify these specific, temporary errors so that we can automatically retry the API calls a few times before failing. This makes the notebook more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de59f4",
   "metadata": {
    "papermill": {
     "duration": 1.43616,
     "end_time": "2025-04-21T00:51:49.668831",
     "exception": false,
     "start_time": "2025-04-21T00:51:48.232671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use original import style based on user feedback and source\n",
    "from google import genai\n",
    "from google.api_core import retry\n",
    "\n",
    "\n",
    "def is_retriable(exception: Exception) -> bool:\n",
    "    \"\"\"Checks if an exception is a retriable Google APIError (429 or 503).\"\"\"\n",
    "    return isinstance(exception, genai.errors.APIError) and exception.code in {429, 503}\n",
    "\n",
    "\n",
    "# This function will be used with the @retry.Retry decorator for API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3bf860",
   "metadata": {
    "papermill": {
     "duration": 0.011781,
     "end_time": "2025-04-21T00:51:49.691529",
     "exception": false,
     "start_time": "2025-04-21T00:51:49.679748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. User Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df8521",
   "metadata": {
    "papermill": {
     "duration": 0.010399,
     "end_time": "2025-04-21T00:51:49.712837",
     "exception": false,
     "start_time": "2025-04-21T00:51:49.702438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Adjust the variables in the following code cell to control the notebook's behavior. These include file paths, language settings, model parameters, and the job description you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61b792",
   "metadata": {
    "papermill": {
     "duration": 0.023849,
     "end_time": "2025-04-21T00:51:49.747395",
     "exception": false,
     "start_time": "2025-04-21T00:51:49.723546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Required import for path handling\n",
    "from pathlib import Path\n",
    "\n",
    "# Import Literal for specific type checking if needed, list/dict hints use built-ins\n",
    "from typing import Literal\n",
    "\n",
    "# ==============================================================================\n",
    "# User Configuration Variables\n",
    "# ==============================================================================\n",
    "# Adjust these settings before running the rest of the notebook.\n",
    "\n",
    "# --- 1. Input Files & Paths ---\n",
    "\n",
    "# Specify the path where the CV PDF file is located or will be downloaded to.\n",
    "CV_PDF_PATH = Path(\"./resume.pdf\")  # Example: file in the root working directory\n",
    "\n",
    "# Set to True to download the CV from Google Drive using the ID below.\n",
    "# Set to False if you are providing the CV via CV_PDF_PATH directly.\n",
    "DOWNLOAD_CV = True  # Default behavior from original notebook\n",
    "CV_GDRIVE_FILE_ID = \"1avK0u9HcyuEYgpyIs_pBuzPxWPrfgW_C\"  # Example File ID\n",
    "\n",
    "# --- 2. Job Description Input ---\n",
    "\n",
    "# Paste the target job description text between the triple quotes.\n",
    "JOB_DESCRIPTION = \"\"\"\n",
    "Job Title: AI/Machine Learning Engineer\n",
    "\n",
    "Company: Innovate Solutions Inc.\n",
    "\n",
    "Location: Remote (US Based)\n",
    "\n",
    "About Us:\n",
    "Innovate Solutions Inc. is at the forefront of applying artificial intelligence to solve real-world business challenges. We foster a collaborative environment where creative thinking and technical excellence drive our success. We are passionate about building intelligent systems that deliver significant value to our clients across various industries. Join our growing team and help shape the future of applied AI.\n",
    "\n",
    "About the Role:\n",
    "We are seeking a talented and motivated AI/Machine Learning Engineer to join our core development team. You will play a key role in the end-to-end lifecycle of machine learning projects, from conceptualization and data exploration to model deployment and monitoring. You'll work closely with data scientists, software engineers, and product managers to build innovative AI-powered features and products.\n",
    "\n",
    "Responsibilities:\n",
    "- Design, develop, train, and deploy machine learning models (including deep learning models) for tasks such as NLP, predictive analytics, anomaly detection, and personalization.\n",
    "- Process, cleanse, and verify the integrity of large datasets used for analysis and model training.\n",
    "- Collaborate with data engineering teams to build and maintain robust data pipelines for ML workflows.\n",
    "- Implement and maintain MLOps best practices for model versioning, testing, deployment, and monitoring.\n",
    "- Stay current with the latest advancements in AI/ML techniques, tools, and platforms.\n",
    "- Analyze experimental results, iterate on models, and communicate findings to technical and non-technical stakeholders.\n",
    "- Contribute to the development of internal AI platforms and tooling.\n",
    "\n",
    "Required Qualifications:\n",
    "- Bachelor's or Master's degree in Computer Science, Data Science, Statistics, or a related quantitative field.\n",
    "- 2+ years of hands-on experience building and deploying machine learning models in a production environment.\n",
    "- Strong programming skills in Python and proficiency with relevant ML libraries (e.g., Scikit-learn, TensorFlow, PyTorch, Keras).\n",
    "- Solid understanding of core machine learning algorithms, statistical modeling, and evaluation metrics.\n",
    "- Experience working with SQL and/or NoSQL databases.\n",
    "- Familiarity with data processing and analysis libraries (e.g., Pandas, NumPy).\n",
    "- Excellent problem-solving skills and attention to detail.\n",
    "- Strong communication and teamwork abilities.\n",
    "\n",
    "Desired Qualifications (Bonus Points):\n",
    "- PhD in a related field.\n",
    "- Experience with cloud platforms (AWS, GCP, or Azure) and their AI/ML services (e.g., SageMaker, Vertex AI, Azure ML).\n",
    "- Experience with MLOps tools and practices (e.g., Docker, Kubernetes, MLflow, Kubeflow).\n",
    "- Experience with Natural Language Processing (NLP) or Computer Vision (CV).\n",
    "- Experience with big data technologies (e.g., Spark, Hadoop).\n",
    "- Publications in relevant AI/ML conferences or journals.\n",
    "\n",
    "What We Offer:\n",
    "- Competitive salary and benefits package.\n",
    "- Opportunity to work on challenging and impactful AI projects.\n",
    "- A dynamic, collaborative, and supportive work environment.\n",
    "- Flexible remote work policy.\n",
    "- Professional development opportunities.\n",
    "\"\"\"\n",
    "\n",
    "# --- 3. Language Settings ---\n",
    "\n",
    "# Set the primary language for prompts (influences LLM responses).\n",
    "LANGUAGE: Literal[\"en\", \"es\"] = \"en\"  # Options: 'en' or 'es'\n",
    "\n",
    "# --- 4. RAG & Embedding Settings ---\n",
    "\n",
    "# Retrieval Parameters for RAG Search (Controls how many results are fetched/processed)\n",
    "MAX_RELEVANT_ITEMS = 3  # Target number of CV items to refine\n",
    "RETRIEVAL_WINDOW = 2  # Extra items retrieved for context during search\n",
    "\n",
    "# Embedding Model Name\n",
    "EMBEDDING_MODEL_NAME = \"models/text-embedding-004\"\n",
    "\n",
    "# Vector Database Configuration (Using Single Collection Strategy)\n",
    "CHROMA_DB_PATH = Path(\"./chroma_db_persistent\")  # Path for persistent DB\n",
    "COLLECTION_NAME = \"cv_embeddings_v1\"  # Name for the single collection\n",
    "\n",
    "# --- 5. LLM Configuration ---\n",
    "\n",
    "# Generative model name for structuring the CV and for the rewriting chat.\n",
    "# Using the specific name mentioned by the user.\n",
    "GENERATIVE_MODEL_NAME = \"gemini-2.0-flash\"\n",
    "\n",
    "# Parameters for the PDF -> JSON *Structuring* LLM call\n",
    "STRUCTURING_LLM_TEMPERATURE = 0.1\n",
    "\n",
    "# Parameters for the CV Item *Rewriting* LLM Chat\n",
    "REWRITING_LLM_TEMPERATURE = 0.8\n",
    "REWRITING_LLM_TOP_P = 0.95\n",
    "REWRITING_LLM_TOP_K = 30\n",
    "\n",
    "# --- 6. CV Processing Settings ---\n",
    "\n",
    "# List the sections (JSON keys) from the structured CV to embed and use for RAG.\n",
    "# Keeping this as a list allows processing/searching multiple sections with the single DB collection.\n",
    "# Example sections based on JSON Resume schema and original recommendations:\n",
    "# \"work\", \"certificates\", \"publications\", \"projects\", \"skills\", \"education\"\n",
    "CV_SECTIONS_TO_FOCUS: list[str] = [\n",
    "    \"work\",\n",
    "    \"certificates\",\n",
    "    \"projects\",\n",
    "]  # Edit this list as needed\n",
    "\n",
    "# Optional: Limit characters fed to the structuring LLM. None = no limit.\n",
    "MAX_CV_TEXT_LENGTH_FOR_STRUCTURING = 12000\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# Directory Setup (Using pathlib)\n",
    "# ==============================================================================\n",
    "\n",
    "# Ensure the directory for ChromaDB exists\n",
    "CHROMA_DB_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure the parent directory for the CV PDF exists\n",
    "CV_PDF_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"User configuration loaded and directories created.\")  # noqa: T201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2041e",
   "metadata": {
    "papermill": {
     "duration": 0.010359,
     "end_time": "2025-04-21T00:51:49.768747",
     "exception": false,
     "start_time": "2025-04-21T00:51:49.758388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Load and Process CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608f056",
   "metadata": {
    "papermill": {
     "duration": 0.010947,
     "end_time": "2025-04-21T00:51:49.853659",
     "exception": false,
     "start_time": "2025-04-21T00:51:49.842712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.1. Load PDF Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01fe7d",
   "metadata": {
    "papermill": {
     "duration": 0.011089,
     "end_time": "2025-04-21T00:51:49.876186",
     "exception": false,
     "start_time": "2025-04-21T00:51:49.865097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This step ensures the CV PDF file specified in the configuration (`CV_PDF_PATH`) is available. If `DOWNLOAD_CV` was set to `True`, it downloads the file from Google Drive using the provided ID. Otherwise, it assumes the file exists at `CV_PDF_PATH`.\n",
    "\n",
    "Finally, it checks if the file exists and displays the PDF inline for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02db192",
   "metadata": {
    "papermill": {
     "duration": 4.013851,
     "end_time": "2025-04-21T00:51:53.900903",
     "exception": false,
     "start_time": "2025-04-21T00:51:49.887052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "from IPython.display import IFrame, display\n",
    "\n",
    "# --- 1. Conditional Download ---\n",
    "if DOWNLOAD_CV:\n",
    "    print(  # noqa: T201\n",
    "        f\"Attempting to download CV from Google Drive (ID: {CV_GDRIVE_FILE_ID}) to\"\n",
    "        f\" '{CV_PDF_PATH}'...\"\n",
    "    )\n",
    "    try:\n",
    "        # Download using gdown, specifying the output path\n",
    "        gdown.download(id=CV_GDRIVE_FILE_ID, output=str(CV_PDF_PATH), quiet=False)\n",
    "        print(\"Download attempt finished.\")  # noqa: T201\n",
    "    except Exception as e:\n",
    "        print(\"\\nERROR: Failed to download file from Google Drive.\")  # noqa: T201\n",
    "        print(  # noqa: T201\n",
    "            f\"Please check the file ID ('{CV_GDRIVE_FILE_ID}') and ensure the file is\"\n",
    "            \" accessible.\"\n",
    "        )\n",
    "        print(f\"Error details: {e}\")  # noqa: T201\n",
    "else:\n",
    "    print(  # noqa: T201\n",
    "        f\"Skipping download. Assuming CV PDF is already present at: '{CV_PDF_PATH}'\"\n",
    "    )  # noqa: T201\n",
    "\n",
    "# --- 2. Verify File Existence and Display ---\n",
    "if CV_PDF_PATH.is_file():\n",
    "    print(  # noqa: T201\n",
    "        f\"CV PDF found at '{CV_PDF_PATH}'. Displaying PDF inline below:\"\n",
    "    )  # noqa: T201\n",
    "    # Display the PDF inline for verification\n",
    "    display(IFrame(src=CV_PDF_PATH, width=\"90%\", height=\"600px\"))\n",
    "else:\n",
    "    print(  # noqa: T201\n",
    "        f\"\\nERROR: CV PDF file was not found at the path: '{CV_PDF_PATH}'\"\n",
    "    )  # noqa: T201\n",
    "    # Provide context based on whether download was attempted\n",
    "    if DOWNLOAD_CV:\n",
    "        print(  # noqa: T201\n",
    "            \"The download may have failed, or the configured path is incorrect.\"\n",
    "        )  # noqa: T201\n",
    "    else:\n",
    "        print(  # noqa: T201\n",
    "            \"Please ensure the file exists at the specified path in the configuration.\"\n",
    "        )\n",
    "    # If the CV is critical for subsequent steps, consider stopping execution:\n",
    "    # raise FileNotFoundError(f\"CV PDF not found at {CV_PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02608736",
   "metadata": {
    "papermill": {
     "duration": 0.011057,
     "end_time": "2025-04-21T00:51:53.923828",
     "exception": false,
     "start_time": "2025-04-21T00:51:53.912771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.2. Extract Text from PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c43274",
   "metadata": {
    "papermill": {
     "duration": 0.010943,
     "end_time": "2025-04-21T00:51:53.945949",
     "exception": false,
     "start_time": "2025-04-21T00:51:53.935006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that the PDF file is available, we will extract its text content. We use the `pdfplumber` library for this task, as it's generally effective for text-based PDFs. The extracted text will be stored in a variable for the next step (structuring with the LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0191ef",
   "metadata": {
    "papermill": {
     "duration": 0.152752,
     "end_time": "2025-04-21T00:51:54.109897",
     "exception": false,
     "start_time": "2025-04-21T00:51:53.957145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "raw_cv_text = \"\"  # Initialize variable\n",
    "\n",
    "print(f\"Extracting text from PDF: '{CV_PDF_PATH}'...\")  # noqa: T201\n",
    "try:\n",
    "    with pdfplumber.open(CV_PDF_PATH) as pdf:\n",
    "        # Extract text page by page, handling potential None values from empty pages\n",
    "        raw_cv_text = \"\".join(\n",
    "            page.extract_text(x_tolerance=1, y_tolerance=3) or \"\" for page in pdf.pages\n",
    "        )\n",
    "\n",
    "    print(  # noqa: T201\n",
    "        f\"Text extraction successful. Total characters: {len(raw_cv_text)}\"\n",
    "    )  # noqa: T201\n",
    "    # Optional: Uncomment the line below to print the first 500 characters\n",
    "    print(f\"--- Snippet ---\\n{raw_cv_text[:500]}\\n---------------\")  # noqa: T201\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(  # noqa: T201\n",
    "        f\"\\nERROR: PDF file not found at '{CV_PDF_PATH}'. Cannot extract text.\"\n",
    "    )  # noqa: T201\n",
    "    # Optional: Uncomment the 'raise' below to stop execution if the file is critical\n",
    "    # raise\n",
    "except Exception as e:\n",
    "    print(  # noqa: T201\n",
    "        f\"\\nERROR: Failed to open or extract text from PDF '{CV_PDF_PATH}'.\"\n",
    "    )  # noqa: T201\n",
    "    print(f\"Error details: {e}\")  # noqa: T201\n",
    "    # Optional: Uncomment the 'raise' below to stop execution on errors\n",
    "    # raise\n",
    "\n",
    "# Ensure raw_cv_text exists as an empty string if extraction failed above\n",
    "if \"raw_cv_text\" not in locals():\n",
    "    raw_cv_text = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb680db8",
   "metadata": {
    "papermill": {
     "duration": 0.011342,
     "end_time": "2025-04-21T00:51:54.133119",
     "exception": false,
     "start_time": "2025-04-21T00:51:54.121777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.3. Structure Text into JSON using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a632f",
   "metadata": {
    "papermill": {
     "duration": 0.011457,
     "end_time": "2025-04-21T00:51:54.155957",
     "exception": false,
     "start_time": "2025-04-21T00:51:54.144500",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this crucial step, we leverage the configured Generative AI model (Gemini) to parse the unstructured `raw_cv_text` extracted from the PDF. We instruct the model to return the information structured according to the [JSON Resume schema](https://jsonresume.org/schema/).\n",
    "\n",
    "This involves:\n",
    "1.  Defining the JSON Resume schema using Python's `TypedDict` for type safety and clarity (this helps the model adhere to the desired output structure).\n",
    "2.  Constructing a prompt that tells the model the task, provides the raw CV text (potentially truncated based on `MAX_CV_TEXT_LENGTH_FOR_STRUCTURING`), and specifies the desired JSON output format.\n",
    "3.  Calling the Gemini API, explicitly requesting JSON output and providing the schema definition.\n",
    "4.  Parsing the LLM's JSON response into a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d7e19d",
   "metadata": {
    "papermill": {
     "duration": 0.02762,
     "end_time": "2025-04-21T00:51:54.194945",
     "exception": false,
     "start_time": "2025-04-21T00:51:54.167325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# Define nested structures first (order matters for definition)\n",
    "class Location(TypedDict, total=False):\n",
    "    address: str\n",
    "    postalCode: str\n",
    "    city: str\n",
    "    countryCode: str\n",
    "    region: str\n",
    "\n",
    "\n",
    "class Profile(TypedDict, total=False):\n",
    "    network: str\n",
    "    username: str\n",
    "    url: str\n",
    "\n",
    "\n",
    "class Basics(TypedDict, total=False):\n",
    "    name: str\n",
    "    label: str\n",
    "    image: str  # URL to image\n",
    "    email: str\n",
    "    phone: str\n",
    "    url: str  # Personal website/portfolio URL\n",
    "    summary: str  # Professional summary\n",
    "    location: Location\n",
    "    profiles: list[Profile]  # Use modern list hint\n",
    "\n",
    "\n",
    "class WorkItem(TypedDict, total=False):\n",
    "    name: str  # Name of the company/organization\n",
    "    position: str  # Job title\n",
    "    url: str  # Company website\n",
    "    startDate: str  # Format YYYY-MM-DD or YYYY-MM or YYYY\n",
    "    endDate: str  # Format YYYY-MM-DD or YYYY-MM or YYYY, or Present\n",
    "    summary: str  # High-level description of role/company\n",
    "    highlights: list[str]  # Specific achievements or responsibilities (bullet points)\n",
    "\n",
    "\n",
    "class VolunteerItem(TypedDict, total=False):\n",
    "    organization: str\n",
    "    position: str\n",
    "    url: str\n",
    "    startDate: str\n",
    "    endDate: str\n",
    "    summary: str\n",
    "    highlights: list[str]\n",
    "\n",
    "\n",
    "class EducationItem(TypedDict, total=False):\n",
    "    institution: str\n",
    "    url: str\n",
    "    area: str  # e.g., Computer Science\n",
    "    studyType: str  # e.g., Bachelor's Degree, Master's\n",
    "    startDate: str\n",
    "    endDate: str\n",
    "    score: str  # e.g., GPA\n",
    "    courses: list[str]  # Relevant coursework\n",
    "\n",
    "\n",
    "class AwardItem(TypedDict, total=False):\n",
    "    title: str\n",
    "    date: str  # Date awarded\n",
    "    awarder: str  # Organization that gave the award\n",
    "    summary: str  # Description of the award\n",
    "\n",
    "\n",
    "class CertificateItem(TypedDict, total=False):\n",
    "    name: str  # Name of the certificate\n",
    "    date: str  # Date issued\n",
    "    issuer: str  # Issuing organization (e.g., Coursera, Google)\n",
    "    url: str  # Link to certificate if available\n",
    "\n",
    "\n",
    "class PublicationItem(TypedDict, total=False):\n",
    "    name: str  # Title of the publication\n",
    "    publisher: str  # e.g., Journal name, Conference\n",
    "    releaseDate: str\n",
    "    url: str  # Link to publication\n",
    "    summary: str  # Abstract or brief description\n",
    "\n",
    "\n",
    "class SkillItem(TypedDict, total=False):\n",
    "    name: str  # Broad skill category (e.g., Web Development, Data Science)\n",
    "    level: str  # Optional proficiency level (e.g., Intermediate, Advanced)\n",
    "    keywords: list[str]  # Specific technologies or tools (e.g., Python, PyTorch, AWS)\n",
    "\n",
    "\n",
    "class LanguageItem(TypedDict, total=False):\n",
    "    language: str  # e.g., English, Spanish\n",
    "    fluency: str  # e.g., Native, Fluent, Conversational\n",
    "\n",
    "\n",
    "class InterestItem(TypedDict, total=False):\n",
    "    name: str  # Category of interest (e.g., Open Source, AI Ethics)\n",
    "    keywords: list[str]  # Specific interests\n",
    "\n",
    "\n",
    "class ReferenceItem(TypedDict, total=False):\n",
    "    name: str  # Name of reference (ensure consent)\n",
    "    reference: str  # Testimonial or contact details (handle privacy appropriately)\n",
    "\n",
    "\n",
    "class ProjectItem(TypedDict, total=False):\n",
    "    name: str  # Project title\n",
    "    startDate: str\n",
    "    endDate: str\n",
    "    description: str  # Overall description of the project\n",
    "    highlights: list[str]  # Key contributions or features\n",
    "    url: str  # Link to project demo or repository\n",
    "\n",
    "\n",
    "# --- Top-Level Curriculum Schema ---\n",
    "class Curriculum(TypedDict, total=False):\n",
    "    \"\"\"Represents the complete JSON Resume structure.\"\"\"\n",
    "\n",
    "    basics: Basics\n",
    "    work: list[WorkItem]\n",
    "    volunteer: list[VolunteerItem]\n",
    "    education: list[EducationItem]\n",
    "    awards: list[AwardItem]\n",
    "    certificates: list[CertificateItem]\n",
    "    publications: list[PublicationItem]\n",
    "    skills: list[SkillItem]\n",
    "    languages: list[LanguageItem]\n",
    "    interests: list[InterestItem]\n",
    "    references: list[ReferenceItem]\n",
    "    projects: list[ProjectItem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ee3d0",
   "metadata": {
    "papermill": {
     "duration": 6.341404,
     "end_time": "2025-04-21T00:52:00.547970",
     "exception": false,
     "start_time": "2025-04-21T00:51:54.206566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from google import genai  # noqa: F811\n",
    "from google.genai import types\n",
    "from google.api_core import retry  # noqa: F811\n",
    "\n",
    "# --- 1. Initialize Model & Validate API Key ---\n",
    "if \"GOOGLE_API_KEY\" not in locals() or not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found or empty. Please check Kaggle Secrets.\")\n",
    "\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2. Prepare Prompt & Configuration for JSON Structuring ---\n",
    "cv_text_for_prompt = raw_cv_text\n",
    "if (\n",
    "    MAX_CV_TEXT_LENGTH_FOR_STRUCTURING\n",
    "    and len(raw_cv_text) > MAX_CV_TEXT_LENGTH_FOR_STRUCTURING\n",
    "):\n",
    "    cv_text_for_prompt = raw_cv_text[:MAX_CV_TEXT_LENGTH_FOR_STRUCTURING]\n",
    "    print(  # noqa: T201\n",
    "        f\"Warning: CV text truncated to {MAX_CV_TEXT_LENGTH_FOR_STRUCTURING} chars for\"\n",
    "        \" structuring prompt.\"\n",
    "    )\n",
    "\n",
    "# System instruction defines the LLM's role and desired output format\n",
    "system_instruction = (\n",
    "    \"You are an expert CV parser. Extract information from the provided CV text and\"\n",
    "    \" format it strictly according to the JSON Resume Schema provided. Return ONLY the\"\n",
    "    \" valid JSON object conforming to the schema - no introductory text, no markdown\"\n",
    "    \" formatting ('```json', '```'), no explanations.\"\n",
    ")\n",
    "# Prompt combines instructions with the actual CV text\n",
    "prompt_message = f\"\"\"\n",
    "Given the following CV text, populate the fields of the JSON Resume Schema as accurately as possible.\n",
    "Use empty strings, arrays, or null values for fields where information is missing in the text.\n",
    "\n",
    "CV Text:\n",
    "---\n",
    "{cv_text_for_prompt}\n",
    "---\n",
    "\"\"\"\n",
    "full_structuring_prompt = system_instruction + \"\\n\\n\" + prompt_message\n",
    "\n",
    "# Configuration forces JSON output matching our TypedDict schema\n",
    "json_generation_config = {\n",
    "    \"temperature\": STRUCTURING_LLM_TEMPERATURE,\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "    \"response_schema\": Curriculum,\n",
    "}\n",
    "\n",
    "\n",
    "# --- 3. Define Function for API Call with Retry Logic ---\n",
    "@retry.Retry(predicate=is_retriable)  # Use helper defined in Setup\n",
    "def generate_structured_cv_json_with_retry(prompt, config):\n",
    "    \"\"\"Calls the Gemini API to generate structured JSON, with retries on specific errors.\"\"\"\n",
    "    print(\"Calling Gemini API to structure CV text into JSON...\")  # noqa: T201\n",
    "    response = client.models.generate_content(\n",
    "        model=GENERATIVE_MODEL_NAME,\n",
    "        contents=prompt,\n",
    "        config=config,\n",
    "    )\n",
    "    print(\"Gemini API call finished.\")  # noqa: T201\n",
    "    return response.text\n",
    "\n",
    "\n",
    "# --- 4. Execute API Call and Parse Response ---\n",
    "structured_cv_data = None  # Initialize result variable\n",
    "\n",
    "try:\n",
    "    json_response_text = generate_structured_cv_json_with_retry(\n",
    "        prompt=full_structuring_prompt,\n",
    "        config=json_generation_config,\n",
    "    )\n",
    "\n",
    "    if json_response_text:\n",
    "        try:\n",
    "            # Parse the validated JSON text from the API response\n",
    "            structured_cv_data = json.loads(json_response_text)\n",
    "            print(  # noqa: T201\n",
    "                \"Successfully parsed LLM response into structured CV data.\"\n",
    "            )  # noqa: T201\n",
    "        except json.JSONDecodeError as e:\n",
    "            # This error *should* be rare given response_mime_type=\"application/json\"\n",
    "            print(\"\\nERROR: Failed to parse the LLM's response as JSON.\")  # noqa: T201\n",
    "            print(f\"JSONDecodeError: {e}\")  # noqa: T201\n",
    "            print(  # noqa: T201\n",
    "                \"\\nLLM Response Text Received:\\n---\\n\", json_response_text, \"\\n---\"\n",
    "            )  # noqa: T201\n",
    "    else:\n",
    "        print(\"\\nERROR: Received an empty response from the LLM API.\")  # noqa: T201\n",
    "\n",
    "except Exception as e:\n",
    "    print(  # noqa: T201\n",
    "        \"\\nERROR: An unexpected error occurred during the LLM structuring call.\"\n",
    "    )  # noqa: T201\n",
    "    print(f\"Error details: {e}\")  # noqa: T201\n",
    "\n",
    "# --- 5. Final Status Update ---\n",
    "if structured_cv_data:\n",
    "    print(\"\\nCV structuring process completed successfully.\")  # noqa: T201\n",
    "    # The variable `structured_cv_data` now holds the Python dictionary.\n",
    "else:\n",
    "    print(\"\\nCV structuring process failed or produced no data.\")  # noqa: T201\n",
    "    # Consider stopping execution if this data is critical\n",
    "    # raise RuntimeError(\"Failed to obtain structured CV data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cfa7c1",
   "metadata": {
    "papermill": {
     "duration": 0.011208,
     "end_time": "2025-04-21T00:52:00.571156",
     "exception": false,
     "start_time": "2025-04-21T00:52:00.559948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4.4. (Optional) Display Structured CV Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e64ced",
   "metadata": {
    "papermill": {
     "duration": 0.011371,
     "end_time": "2025-04-21T00:52:00.594727",
     "exception": false,
     "start_time": "2025-04-21T00:52:00.583356",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You can run the next cell to print the `structured_cv_data` dictionary in a readable YAML format. This helps verify that the LLM correctly parsed and structured the information from your CV text according to the JSON Resume schema. Check if key sections like 'basics', 'work', 'education', 'skills', etc., were populated as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc75b72e",
   "metadata": {
    "papermill": {
     "duration": 0.062397,
     "end_time": "2025-04-21T00:52:00.671007",
     "exception": false,
     "start_time": "2025-04-21T00:52:00.608610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import yaml library for pretty printing dictionaries\n",
    "import yaml\n",
    "\n",
    "# Display the structured data only if it was successfully created and is a dictionary\n",
    "if isinstance(structured_cv_data, dict):\n",
    "    print(\"#\" + \"=\" * 79)  # noqa: T201\n",
    "    print(\"# Structured CV Data (YAML Format):\")  # noqa: T201\n",
    "    print(\"#\" + \"=\" * 79)  # noqa: T201\n",
    "    # Use yaml.dump for a readable, multi-line representation of the dictionary\n",
    "    # allow_unicode ensures correct display of special characters (like accents)\n",
    "    # sort_keys=False maintains the order defined in the schema where possible\n",
    "    print(  # noqa: T201\n",
    "        yaml.dump(\n",
    "            structured_cv_data, allow_unicode=True, sort_keys=False, width=float(\"inf\")\n",
    "        )\n",
    "    )\n",
    "    print(\"#\" + \"=\" * 79)  # noqa: T201\n",
    "else:\n",
    "    # Message if the data isn't available (e.g., previous step failed)\n",
    "    print(  # noqa: T201\n",
    "        \"Structured CV data not available or is not a dictionary. Cannot display YAML.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fb81c",
   "metadata": {
    "papermill": {
     "duration": 0.014399,
     "end_time": "2025-04-21T00:52:00.703230",
     "exception": false,
     "start_time": "2025-04-21T00:52:00.688831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Prepare and Store Embeddings in ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b689c5e1",
   "metadata": {
    "papermill": {
     "duration": 0.014778,
     "end_time": "2025-04-21T00:52:00.733826",
     "exception": false,
     "start_time": "2025-04-21T00:52:00.719048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This section focuses on converting the relevant parts of your structured CV data into vector embeddings and storing them in a searchable vector database (ChromaDB). This allows us to later find the CV items most semantically similar to a job description (the core of RAG).\n",
    "\n",
    "The key steps are:\n",
    "1.  **Prepare Documents:** Extract the specific CV items (like individual work experiences or projects) from the sections listed in `CV_SECTIONS_TO_FOCUS` and format them as text documents suitable for embedding. We'll use the YAML dump method as previously decided.\n",
    "2.  **Define Embedding Function:** Set up a function that uses the configured Google embedding model (`EMBEDDING_MODEL_NAME`) to convert text documents into numerical vectors (embeddings).\n",
    "3.  **Initialize Vector Database:** Connect to or create a persistent ChromaDB database using the configured path (`CHROMA_DB_PATH`) and collection name (`COLLECTION_NAME`). We will use a single collection for all items.\n",
    "4.  **Generate & Store Embeddings:** Iterate through the prepared documents, generate embeddings for them using the embedding function, and add the embeddings, the original documents, and relevant metadata (like the source section) to the ChromaDB collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc7e27",
   "metadata": {
    "papermill": {
     "duration": 0.014624,
     "end_time": "2025-04-21T00:52:00.760536",
     "exception": false,
     "start_time": "2025-04-21T00:52:00.745912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.1. Prepare Text Documents from Structured CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e33960",
   "metadata": {
    "papermill": {
     "duration": 0.047622,
     "end_time": "2025-04-21T00:52:00.823460",
     "exception": false,
     "start_time": "2025-04-21T00:52:00.775838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import re\n",
    "\n",
    "# --- 1. Helper Functions for ID Generation ---\n",
    "\n",
    "# Maps section keys to functions creating a base ID string from item content\n",
    "BASE_ID_GENERATORS = {\n",
    "    \"work\": lambda item: (\n",
    "        f\"{item.get('name', 'NoCompany')}.{item.get('position', 'NoPosition')}\"\n",
    "    ),\n",
    "    \"certificates\": lambda item: (\n",
    "        f\"{item.get('issuer', 'NoIssuer')}.{item.get('name', 'NoCert')}\"\n",
    "    ),\n",
    "    \"publications\": lambda item: (\n",
    "        f\"{item.get('publisher', 'NoPublisher')}.{item.get('name', 'NoPub')}\"\n",
    "    ),\n",
    "    \"projects\": lambda item: item.get(\"name\", \"NoProject\"),\n",
    "    \"volunteer\": lambda item: (\n",
    "        f\"{item.get('organization', 'NoOrg')}.{item.get('position', 'NoVolunteerPos')}\"\n",
    "    ),\n",
    "    \"education\": lambda item: (\n",
    "        f\"{item.get('institution', 'NoInstitution')}.{item.get('area', 'NoArea')}.{item.get('studyType', '')}\"\n",
    "    ),\n",
    "    \"basics\": lambda item: item.get(\"name\", \"NoPerson\"),\n",
    "    \"awards\": lambda item: (\n",
    "        f\"{item.get('awarder', 'NoAwarder')}.{item.get('title', 'NoAward')}\"\n",
    "    ),\n",
    "    \"skills\": lambda item: item.get(\"name\", \"NoSkill\"),\n",
    "    \"languages\": lambda item: item.get(\"language\", \"NoLang\"),\n",
    "    \"interests\": lambda item: item.get(\"name\", \"NoInterest\"),\n",
    "    \"references\": lambda item: item.get(\"name\", \"NoReference\"),\n",
    "}\n",
    "\n",
    "\n",
    "def sanitize_id(text_id: str) -> str:\n",
    "    \"\"\"Cleans and formats a string into a valid ChromaDB ID.\"\"\"\n",
    "    text_id = text_id.lower()\n",
    "    # Basic accent removal using a lambda for compactness\n",
    "    text_id = re.sub(\n",
    "        r\"[áäâàãåéëêèíïîìóöôòõøúüûùñç]\",\n",
    "        lambda m: {\n",
    "            \"á\": \"a\",\n",
    "            \"ä\": \"a\",\n",
    "            \"â\": \"a\",\n",
    "            \"à\": \"a\",\n",
    "            \"ã\": \"a\",\n",
    "            \"å\": \"a\",\n",
    "            \"é\": \"e\",\n",
    "            \"ë\": \"e\",\n",
    "            \"ê\": \"e\",\n",
    "            \"è\": \"e\",\n",
    "            \"í\": \"i\",\n",
    "            \"ï\": \"i\",\n",
    "            \"î\": \"i\",\n",
    "            \"ì\": \"i\",\n",
    "            \"ó\": \"o\",\n",
    "            \"ö\": \"o\",\n",
    "            \"ô\": \"o\",\n",
    "            \"ò\": \"o\",\n",
    "            \"õ\": \"o\",\n",
    "            \"ø\": \"o\",\n",
    "            \"ú\": \"u\",\n",
    "            \"ü\": \"u\",\n",
    "            \"û\": \"u\",\n",
    "            \"ù\": \"u\",\n",
    "            \"ñ\": \"n\",\n",
    "            \"ç\": \"c\",\n",
    "        }.get(m.group(0)),\n",
    "        text_id,\n",
    "    )\n",
    "    text_id = re.sub(r\"[\\s_:-]+\", \".\", text_id)  # Replace separators with dot\n",
    "    text_id = re.sub(r\"[^a-z0-9.]\", \"\", text_id)  # Keep only alphanumeric and dot\n",
    "    text_id = re.sub(r\"\\.+\", \".\", text_id)  # Consolidate consecutive dots\n",
    "    text_id = text_id.strip(\".\")  # Remove leading/trailing dots\n",
    "    if len(text_id) < 3:  # noqa: PLR2004\n",
    "        text_id = f\"{text_id}.id\"  # Ensure min length\n",
    "    return text_id[:63]  # Truncate to 63 characters (ChromaDB limit)\n",
    "\n",
    "\n",
    "def generate_unique_item_id(section_key: str, item: dict, item_index: int) -> str:\n",
    "    \"\"\"Generates a unique, sanitized ID for a CV item using dot separator.\"\"\"\n",
    "    id_generator = BASE_ID_GENERATORS.get(\n",
    "        section_key, lambda i: f\"item.{item_index}\"\n",
    "    )  # Fallback\n",
    "    base_id = id_generator(item)\n",
    "    full_id_base = f\"{section_key}.{base_id}.{item_index}\"  # Use dot separator\n",
    "    return sanitize_id(full_id_base)\n",
    "\n",
    "\n",
    "# --- 2. Function to Prepare Data for Embedding ---\n",
    "\n",
    "\n",
    "def prepare_embedding_data(\n",
    "    cv_data: dict, sections_to_include: list[str]\n",
    ") -> tuple[list, list, list]:\n",
    "    \"\"\"\n",
    "    Extracts items, formats documents (YAML strings), generates IDs, creates metadata.\n",
    "    Returns tuple: (documents, ids, metadatas).\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    all_ids = []\n",
    "    all_metadatas = []\n",
    "\n",
    "    if not isinstance(cv_data, dict):\n",
    "        print(\"Warning: structured_cv_data is not a dictionary.\")  # noqa: T201\n",
    "        return [], [], []\n",
    "\n",
    "    print(  # noqa: T201\n",
    "        f\"Preparing embedding data for sections: {sections_to_include}...\"\n",
    "    )  # noqa: T201\n",
    "    processed_count = 0\n",
    "    for section_key in sections_to_include:\n",
    "        section_items = cv_data.get(section_key)\n",
    "        if isinstance(section_items, list):\n",
    "            for index, item in enumerate(section_items):\n",
    "                if isinstance(item, dict):\n",
    "                    item_id = generate_unique_item_id(section_key, item, index)\n",
    "                    try:\n",
    "                        item_doc = yaml.dump(\n",
    "                            item,\n",
    "                            allow_unicode=True,\n",
    "                            sort_keys=False,\n",
    "                            width=float(\"inf\"),\n",
    "                            default_flow_style=None,\n",
    "                        )\n",
    "                    except yaml.YAMLError:\n",
    "                        item_doc = str(item)  # Fallback\n",
    "                        print(  # noqa: T201\n",
    "                            f\"Warning: YAML dump failed for item {index} in\"\n",
    "                            f\" {section_key}.\"\n",
    "                        )\n",
    "\n",
    "                    metadata = {\"section\": section_key, \"item_index\": index}\n",
    "                    # Add potentially useful fields from item to metadata if they exist\n",
    "                    for key in [\n",
    "                        \"name\",\n",
    "                        \"position\",\n",
    "                        \"issuer\",\n",
    "                        \"institution\",\n",
    "                        \"organization\",\n",
    "                    ]:\n",
    "                        if value := item.get(\n",
    "                            key\n",
    "                        ):  # Walrus operator requires Python 3.8+\n",
    "                            metadata[f\"id_{key}\"] = value\n",
    "\n",
    "                    all_documents.append(item_doc)\n",
    "                    all_ids.append(item_id)\n",
    "                    all_metadatas.append(metadata)\n",
    "                    processed_count += 1\n",
    "\n",
    "    print(f\"Prepared {processed_count} documents for embedding.\")  # noqa: T201\n",
    "    return all_documents, all_ids, all_metadatas\n",
    "\n",
    "\n",
    "# --- 3. Execute Preparation ---\n",
    "embedding_documents = []\n",
    "embedding_ids = []\n",
    "embedding_metadatas = []\n",
    "\n",
    "if isinstance(structured_cv_data, dict):\n",
    "    embedding_documents, embedding_ids, embedding_metadatas = prepare_embedding_data(\n",
    "        cv_data=structured_cv_data, sections_to_include=CV_SECTIONS_TO_FOCUS\n",
    "    )\n",
    "\n",
    "    if embedding_documents:\n",
    "        print(\"\\n--- Sample Prepared Data (First Item) ---\")  # noqa: T201\n",
    "        print(f\"ID        : {embedding_ids[0]}\")  # noqa: T201\n",
    "        print(f\"Metadata  : {embedding_metadatas[0]}\")  # noqa: T201\n",
    "        print(f\"Doc Snippet:\\n---\\n{embedding_documents[0][:200]}...\")  # noqa: T201\n",
    "        print(\"---\")  # noqa: T201\n",
    "else:\n",
    "    print(  # noqa: T201\n",
    "        \"\\nERROR: `structured_cv_data` not available. Cannot prepare embedding data.\"\n",
    "    )  # noqa: T201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877792c9",
   "metadata": {
    "papermill": {
     "duration": 0.011798,
     "end_time": "2025-04-21T00:52:00.847888",
     "exception": false,
     "start_time": "2025-04-21T00:52:00.836090",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.2. Define Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e50d18",
   "metadata": {
    "papermill": {
     "duration": 2.344372,
     "end_time": "2025-04-21T00:52:03.204298",
     "exception": false,
     "start_time": "2025-04-21T00:52:00.859926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "from google.api_core import retry\n",
    "import google.generativeai as genai\n",
    "from google.genai import types  # noqa: F811\n",
    "\n",
    "\n",
    "# Use the original class structure provided by the user\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    \"\"\"Custom ChromaDB embedding function using the original implementation structure.\"\"\"\n",
    "\n",
    "    def __init__(self, document_mode: bool = True) -> None:\n",
    "        \"\"\"Initializes based on document_mode, uses model from config.\"\"\"\n",
    "        self.embedding_task: str = (\n",
    "            \"retrieval_document\" if document_mode else \"retrieval_query\"\n",
    "        )\n",
    "        self.model: str = EMBEDDING_MODEL_NAME\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable)\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        \"\"\"Generates embeddings using client.models.embed_content.\"\"\"\n",
    "        response = client.models.embed_content(\n",
    "            model=self.model,\n",
    "            contents=input,\n",
    "            config=types.EmbedContentConfig(task_type=self.embedding_task),\n",
    "        )\n",
    "        return [e.values for e in response.embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d50a3",
   "metadata": {
    "papermill": {
     "duration": 0.01143,
     "end_time": "2025-04-21T00:52:03.227758",
     "exception": false,
     "start_time": "2025-04-21T00:52:03.216328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.3. Initialize Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8578c1",
   "metadata": {
    "papermill": {
     "duration": 0.369655,
     "end_time": "2025-04-21T00:52:03.609253",
     "exception": false,
     "start_time": "2025-04-21T00:52:03.239598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Assumes GeminiEmbeddingFunction is defined in Cell 36\n",
    "# Assumes CHROMA_DB_PATH, COLLECTION_NAME defined in Cell 16\n",
    "\n",
    "cv_collection = None  # Initialize variable\n",
    "\n",
    "try:\n",
    "    # Initialize persistent client\n",
    "    print(f\"Initializing ChromaDB client at path: {CHROMA_DB_PATH}\")  # noqa: T201\n",
    "    chroma_client = chromadb.PersistentClient(path=str(CHROMA_DB_PATH))\n",
    "\n",
    "    # Instantiate embedding function (uses default document_mode=True from its __init__)\n",
    "    gemini_embedder = GeminiEmbeddingFunction(document_mode=True)\n",
    "\n",
    "    # Get or create the single collection using name from config and the embedder\n",
    "    print(f\"Accessing collection: '{COLLECTION_NAME}'...\")  # noqa: T201\n",
    "    cv_collection = chroma_client.get_or_create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=gemini_embedder,\n",
    "        # Optional: Add metadata={\"hnsw:space\": \"cosine\"} here if needed\n",
    "    )\n",
    "\n",
    "    # Confirm collection is ready and show current count\n",
    "    print(  # noqa: T201\n",
    "        f\"Collection '{cv_collection.name}' ready. Item count: {cv_collection.count()}\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nERROR: Failed to initialize ChromaDB client or collection.\")  # noqa: T201\n",
    "    print(f\"Check path ('{CHROMA_DB_PATH}') and ChromaDB setup.\")  # noqa: T201\n",
    "    print(f\"Error details: {e}\")  # noqa: T201\n",
    "    # Consider uncommenting 'raise' if ChromaDB is essential for subsequent steps\n",
    "    # raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d84ef",
   "metadata": {
    "papermill": {
     "duration": 0.011673,
     "end_time": "2025-04-21T00:52:03.633048",
     "exception": false,
     "start_time": "2025-04-21T00:52:03.621375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5.4. Generate and Add Embeddings to Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc473f",
   "metadata": {
    "papermill": {
     "duration": 0.34581,
     "end_time": "2025-04-21T00:52:03.990835",
     "exception": false,
     "start_time": "2025-04-21T00:52:03.645025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the prepared documents, IDs, and metadata to the ChromaDB collection.\n",
    "# ChromaDB uses the 'GeminiEmbeddingFunction' provided during collection creation\n",
    "# to automatically generate embeddings for the documents list.\n",
    "\n",
    "# Ensure we have a valid collection object and data to add\n",
    "if (\n",
    "    \"cv_collection\" in locals()\n",
    "    and cv_collection\n",
    "    and \"embedding_documents\" in locals()\n",
    "    and embedding_documents\n",
    "):\n",
    "\n",
    "    print(  # noqa: T201\n",
    "        f\"Adding/updating {len(embedding_documents)} documents in ChromaDB collection\"\n",
    "        f\" '{cv_collection.name}'...\"\n",
    "    )\n",
    "    # Note: This step involves API calls via the embedding function and may take time.\n",
    "    try:\n",
    "        # Use upsert=True to add new items and update existing ones if IDs match.\n",
    "        # This makes the process idempotent (safe to re-run).\n",
    "        cv_collection.upsert(\n",
    "            ids=embedding_ids,\n",
    "            metadatas=embedding_metadatas,\n",
    "            documents=embedding_documents,\n",
    "        )\n",
    "\n",
    "        print(\"\\nDocuments successfully added/updated in the collection.\")  # noqa: T201\n",
    "        # Verify final count\n",
    "        final_count = cv_collection.count()\n",
    "        print(  # noqa: T201\n",
    "            f\"Collection '{cv_collection.name}' now contains {final_count} items.\"\n",
    "        )  # noqa: T201\n",
    "        if final_count < len(embedding_ids):\n",
    "            print(  # noqa: T201\n",
    "                \"Warning: Final item count is less than prepared documents count.\"\n",
    "            )  # noqa: T201\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nERROR adding/updating documents in ChromaDB:\")  # noqa: T201\n",
    "        print(f\"Error details: {e}\")  # noqa: T201\n",
    "        # raise # Optional: Stop execution\n",
    "\n",
    "else:\n",
    "    print(  # noqa: T201\n",
    "        \"Skipping document addition to ChromaDB: Prerequisite data or collection\"\n",
    "        \" missing.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada7e2be",
   "metadata": {
    "papermill": {
     "duration": 0.022016,
     "end_time": "2025-04-21T00:52:04.025536",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.003520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2a89cd",
   "metadata": {
    "papermill": {
     "duration": 0.012113,
     "end_time": "2025-04-21T00:52:04.049928",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.037815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Retrieve Relevant CV Items (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f972f9",
   "metadata": {
    "papermill": {
     "duration": 0.222556,
     "end_time": "2025-04-21T00:52:04.284752",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.062196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform RAG query using the job description\n",
    "\n",
    "retrieved_ids = []\n",
    "retrieved_documents = []\n",
    "retrieved_metadatas = []\n",
    "retrieved_distances = []\n",
    "\n",
    "try:\n",
    "    # Basic validation of inputs from previous steps\n",
    "    if \"cv_collection\" not in locals() or not cv_collection:\n",
    "        raise ValueError(\"ChromaDB collection object 'cv_collection' not found.\")\n",
    "    if not JOB_DESCRIPTION:\n",
    "        raise ValueError(\"JOB_DESCRIPTION variable is empty.\")\n",
    "    if \"GeminiEmbeddingFunction\" not in locals():\n",
    "        raise NameError(\"GeminiEmbeddingFunction class definition not found.\")\n",
    "\n",
    "    print(f\"Performing RAG query on collection '{cv_collection.name}'...\")  # noqa: T201\n",
    "\n",
    "    # Instantiate embedder for the 'retrieval_query' task type\n",
    "    query_embedder = GeminiEmbeddingFunction(document_mode=False)\n",
    "\n",
    "    print(\"Embedding job description (query)...\")  # noqa: T201\n",
    "    # Embed query (API call happens here, raises exception on failure via embedder)\n",
    "    query_embedding = query_embedder([JOB_DESCRIPTION])[0]\n",
    "\n",
    "    # Removed the ambiguous check: if not query_embedding:\n",
    "    # If the line above succeeded, query_embedding contains the vector.\n",
    "    # If it failed, the exception would be caught by the main except block below.\n",
    "\n",
    "    num_results_to_fetch = MAX_RELEVANT_ITEMS + RETRIEVAL_WINDOW\n",
    "    print(f\"Querying for {num_results_to_fetch} items...\")  # noqa: T201\n",
    "\n",
    "    results = cv_collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=num_results_to_fetch,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    # Safely extract results\n",
    "    retrieved_ids = results.get(\"ids\", [[]])[0]\n",
    "    retrieved_documents = results.get(\"documents\", [[]])[0]\n",
    "    retrieved_metadatas = results.get(\"metadatas\", [[]])[0]\n",
    "    retrieved_distances = results.get(\"distances\", [[]])[0]\n",
    "\n",
    "    print(f\"RAG retrieval complete. Found {len(retrieved_ids)} items.\")  # noqa: T201\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR during RAG retrieval: {type(e).__name__} - {e}\")  # noqa: T201\n",
    "    # Ensure lists are reset on error\n",
    "    retrieved_ids, retrieved_documents, retrieved_metadatas, retrieved_distances = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "\n",
    "if not retrieved_ids:\n",
    "    print(\"\\nWarning: No relevant items were retrieved.\")  # noqa: T201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72600ccd",
   "metadata": {
    "papermill": {
     "duration": 0.012277,
     "end_time": "2025-04-21T00:52:04.310098",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.297821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Rewrite CV Items using LLM Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce240ad7",
   "metadata": {
    "papermill": {
     "duration": 0.011964,
     "end_time": "2025-04-21T00:52:04.334498",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.322534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7.1. Define Prompts for Rewriting (EN & ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b3bad",
   "metadata": {
    "papermill": {
     "duration": 0.019272,
     "end_time": "2025-04-21T00:52:04.366019",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.346747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Structure to hold multi-language prompt components\n",
    "content = {\"en\": {}, \"es\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d487eddd",
   "metadata": {
    "papermill": {
     "duration": 0.01971,
     "end_time": "2025-04-21T00:52:04.398729",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.379019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "content[\"en\"][\"quit_msg\"] = \"To exit, enter 'q' or 'quit'\"\n",
    "content[\"es\"][\"quit_msg\"] = \"Para salir, ingresa 'q' o 'salir'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2cdbf",
   "metadata": {
    "papermill": {
     "duration": 0.019359,
     "end_time": "2025-04-21T00:52:04.430484",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.411125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example format for the LLM to follow when presenting a modified experience\n",
    "content[\"en\"][\"example\"] = \"\"\"\n",
    "## Position Name / Title: [JOB TITLE]\n",
    "- Company Name / Business Name: [COMPANY/ORG NAME]\n",
    "- Industry type: [INDUSTRY]\n",
    "- Job Field: [FIELD]\n",
    "- Sub-Area of Work: [SUB-AREA]\n",
    "\n",
    "### Original Description\n",
    "\n",
    "[ORIGINAL DESCRIPTION TEXT]\n",
    "\n",
    "### Modified description\n",
    "\n",
    "[SHORT SUMMARY/LEAD-IN]\n",
    "- [ATS-Optimized achievement/responsibility 1 incorporating keywords]\n",
    "- [ATS-Optimized achievement/responsibility 2 incorporating keywords]\n",
    "- ...\n",
    "\n",
    "### Changes made\n",
    "\n",
    "- Keywords used: [KEYWORD 1], [KEYWORD 2], ...\n",
    "- Explanation of changes: [Brief summary of additions/removals/focus shifts].\n",
    "\n",
    "\n",
    "Shall we continue [Y/n]?\n",
    "\"\"\"\n",
    "\n",
    "content[\"es\"][\"example\"] = \"\"\"\n",
    "## Nombre del puesto / Título: [TÍTULO PUESTO]\n",
    "- Nombre de empresa / Negocio: [NOMBRE EMPRESA/ORG]\n",
    "- Tipo de industria: [INDUSTRIA]\n",
    "- Área de trabajo: [ÁREA]\n",
    "- Subárea de trabajo: [SUB-ÁREA]\n",
    "\n",
    "### Descripción original\n",
    "\n",
    "[TEXTO DESCRIPCIÓN ORIGINAL]\n",
    "\n",
    "### Descripción modificada\n",
    "\n",
    "[RESUMEN CORTO/INTRODUCCIÓN]\n",
    "- [Logro/responsabilidad optimizado para ATS 1 incorporando palabras clave]\n",
    "- [Logro/responsabilidad optimizado para ATS 2 incorporando palabras clave]\n",
    "- ...\n",
    "\n",
    "### Changes made\n",
    "\n",
    "- Palabras clave utilizadas: [PALABRA CLAVE 1], [PALABRA CLAVE 2], ...\n",
    "- Explicación de los cambios: [Resumen breve de adiciones/eliminaciones/reenfoques].\n",
    "\n",
    "\n",
    "¿Continuamos? [Y/n]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b545f0",
   "metadata": {
    "papermill": {
     "duration": 0.022492,
     "end_time": "2025-04-21T00:52:04.465413",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.442921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main prompt defining the LLM's role and the interactive workflow.\n",
    "# Placeholders {n_max_exp}, {example}, {description}, {experiences} will be formatted later.\n",
    "\n",
    "content[\"en\"][\"prompt\"] = r\"\"\"\n",
    "You are an expert CV writer specialized in optimizing resumes for Applicant Tracking Systems (ATS) and identifying keywords from job descriptions.\n",
    "Analyze the provided job description and the list of retrieved CV experiences below. Your goal is to refine the experiences to align strongly with the job description keywords, making them ATS-compatible.\n",
    "\n",
    "**Output Requirements:**\n",
    "- ATS-friendly language.\n",
    "- Concise and impactful wording.\n",
    "- Focus on achievements and quantifiable results where possible.\n",
    "- Use active voice (e.g., Developed, Managed, Led).\n",
    "- Adhere strictly to the requested formats. Avoid extra conversational text.\n",
    "\n",
    "**Initial Analysis Task:**\n",
    "1. Identify and list the most critical keywords from the **JOB DESCRIPTION**.\n",
    "2. List the top {n_max_exp} most relevant **HIGHLIGHTED EXPERIENCES** retrieved, ordered by relevance (most relevant first). Include start/end dates (if available), title/position, and company/organization name.\n",
    "3. List any remaining **NON-SELECTED EXPERIENCES** retrieved similarly.\n",
    "\n",
    "**Initial Analysis Output Format:**\n",
    "**KEYWORDS:** [KEYWORD 1], [KEYWORD 2], ...\n",
    "**HIGHLIGHTED EXPERIENCES:**\n",
    "- ([START DATE] - [END DATE]) [JOB TITLE/PROJECT NAME 1], [COMPANY/ORG 1]\n",
    "- ... (up to {n_max_exp} items) ...\n",
    "**NON-SELECTED EXPERIENCES:**\n",
    "- ([START DATE] - [END DATE]) [JOB TITLE/PROJECT NAME X], [COMPANY/ORG X]\n",
    "- ... (remaining items) ...\n",
    "\n",
    "**Interaction Flow:**\n",
    "After presenting the initial analysis, ASK THE FOLLOWING QUESTION EXACTLY:\n",
    "\"Do you possess skills relevant to the keywords? Should any specific HIGHLIGHTED experience be modified?\"\n",
    "\n",
    "WAIT for my response. I will tell you which keywords are most relevant and which specific HIGHLIGHTED experience (by its title/position) I want to modify first.\n",
    "\n",
    "**Modification Task (Perform ONLY AFTER I select an experience):**\n",
    "When I ask you to modify a specific experience:\n",
    "1. Focus ONLY on the single experience I selected.\n",
    "2. Rewrite its description/highlights to incorporate the relevant **KEYWORDS** identified earlier.\n",
    "3. Make the description achievement-oriented and ATS-friendly.\n",
    "4. Present the modified experience using the **EXACT** format shown in the example below. Include the original description text for comparison. Mention the keywords used and explain the changes made.\n",
    "\n",
    "**Modification Output Format Example:** {example}\n",
    "\n",
    "**IMPORTANT:** Modify and present ONLY ONE experience at a time, based on my selection. After presenting a modified experience and asking \"Shall we continue [Y/n]?\", WAIT for my confirmation ('Y' or 'y') before proceeding to ask which *next* highlighted experience I want to modify, OR wait for my feedback/request for further changes on the *current* one. If I enter 'n' or 'q', stop the process.\n",
    "\n",
    "**Context:**\n",
    "JOB DESCRIPTION: \"{description}\"\n",
    "RETRIEVED CV EXPERIENCES:\n",
    "{experiences}\n",
    "\n",
    "Start by performing the **Initial Analysis Task**.\n",
    "\"\"\"\n",
    "\n",
    "content[\"es\"][\"prompt\"] = r\"\"\"\n",
    "Eres un experto redactor de CVs, especialista en optimizar para Applicant Tracking Systems (ATS) e identificar palabras clave en descripciones de trabajo.\n",
    "Analiza la descripción del puesto y la lista de experiencias recuperadas del CV que se proporcionan a continuación. Tu objetivo es refinar las experiencias para alinearlas fuertemente con las palabras clave de la descripción del puesto, haciéndolas compatibles con ATS.\n",
    "\n",
    "**Requisitos de Salida:**\n",
    "- Lenguaje amigable para ATS.\n",
    "- Redacción concisa e impactante.\n",
    "- Enfoque en logros y resultados cuantificables siempre que sea posible.\n",
    "- Usar voz activa (ej., Desarrollé, Gestioné, Lideré).\n",
    "- Adherirse estrictamente a los formatos solicitados. Evitar texto conversacional extra.\n",
    "\n",
    "**Tarea de Análisis Inicial:**\n",
    "1. Identifica y lista las palabras clave más críticas de la **DESCRIPCIÓN DE LA OFERTA**.\n",
    "2. Lista las {n_max_exp} **EXPERIENCIAS DESTACADAS** más relevantes recuperadas, ordenadas por relevance (la más relevante primero). Incluye fechas de inicio/fin (si están disponibles), título/puesto y nombre de la empresa/organización.\n",
    "3. Lista cualquier **EXPERIENCIA NO SELECCIONADA** restante recuperada de manera similar.\n",
    "\n",
    "**Formato de Salida del Análisis Inicial:**\n",
    "**PALABRAS CLAVES:** [PALABRA CLAVE 1], [PALABRA CLAVE 2], ...\n",
    "**EXPERIENCIAS DESTACADAS:**\n",
    "- ([FECHA INICIO] - [FECHA FIN]) [TÍTULO PUESTO/PROYECTO 1], [EMPRESA/ORG 1]\n",
    "- ... (hasta {n_max_exp} ítems) ...\n",
    "**EXPERIENCIAS NO SELECCIONADAS:**\n",
    "- ([FECHA INICIO] - [FECHA FIN]) [TÍTULO PUESTO/PROYECTO X], [EMPRESA/ORG X]\n",
    "- ... (ítems restantes) ...\n",
    "\n",
    "**Flujo de Interacción:**\n",
    "Después de presentar el análisis inicial, HAZ LA SIGUIENTE PREGUNTA EXACTAMENTE:\n",
    "\"¿Posees habilidades relevantes para las palabras claves? ¿Debería modificarse alguna EXPERIENCIA DESTACADA específica?\"\n",
    "\n",
    "ESPERA mi respuesta. Te indicaré qué palabras clave son más relevantes y qué EXPERIENCIA DESTACADA específica (por su título/puesto) quiero modificar primero.\n",
    "\n",
    "**Tarea de Modificación (Realizar SÓLO DESPUÉS de que yo seleccione una experiencia):**\n",
    "Cuando te pida modificar una experiencia específica:\n",
    "1. Enfócate SÓLO en la única experiencia que seleccioné.\n",
    "2. Reescribe su descripción/logros para incorporar las **PALABRAS CLAVES** relevantes identificadas anteriormente.\n",
    "3. Haz la descripción orientada a logros y amigable para ATS.\n",
    "4. Presenta la experiencia modificada usando el formato **EXACTO** mostrado en el ejemplo a continuación. Incluye el texto de la descripción original para comparación. Menciona las palabras clave usadas y explica los cambios realizados.\n",
    "\n",
    "**Ejemplo de Formato de Salida de Modificación:** {example}\n",
    "\n",
    "**IMPORTANTE:** Modifica y presenta SÓLO UNA experiencia a la vez, según mi selección. Después de presentar una experiencia modificada y preguntar \"¿Continuamos? [Y/n]?\", ESPERA mi confirmación ('Y', 'y' o Enter) antes de preguntar qué *siguiente* experiencia destacada quiero modificar, O espera mi feedback/solicitud de más cambios en la *actual*. Si ingreso 'n' o 'q', detén el proceso.\n",
    "\n",
    "**Contexto:**\n",
    "DESCRIPCIÓN DE LA OFERTA: \"{description}\"\n",
    "EXPERIENCIAS DEL CV RECUPERADAS:\n",
    "{experiences}\n",
    "\n",
    "Comienza realizando la **Tarea de Análisis Inicial**.\n",
    "\"\"\"\n",
    "\n",
    "# Note: The placeholders {n_max_exp}, {example}, {description}, {experiences}\n",
    "# will be filled using .format() before sending the prompt to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b214b",
   "metadata": {
    "papermill": {
     "duration": 0.01191,
     "end_time": "2025-04-21T00:52:04.489745",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.477835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7.2. Initiate Chat and Generate Initial Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21704d8c",
   "metadata": {
    "papermill": {
     "duration": 1.547416,
     "end_time": "2025-04-21T00:52:06.049288",
     "exception": false,
     "start_time": "2025-04-21T00:52:04.501872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "# Assumes 'genai' and 'types' are available from previous cell executions\n",
    "\n",
    "# --- 1. Format Retrieved Experiences for Prompt ---\n",
    "formatted_experiences = []\n",
    "if \"retrieved_documents\" in locals() and retrieved_documents:\n",
    "    for i, doc in enumerate(retrieved_documents):\n",
    "        meta = retrieved_metadatas[i] if i < len(retrieved_metadatas) else {}\n",
    "        section = meta.get(\"section\", \"Unknown Section\")\n",
    "        name = meta.get(\"id_name\", \"\")\n",
    "        position = meta.get(\"id_position\", \"\")\n",
    "        header = f\"{name}{(' - ' + position) if position else ''} ({section})\"\n",
    "        formatted_experiences.append(f\"--- Experience {i+1}: {header} ---\\n{doc}\")\n",
    "    experiences_context_string = \"\\n\\n\".join(formatted_experiences)\n",
    "else:\n",
    "    experiences_context_string = (\n",
    "        \"No relevant experiences were retrieved from the database.\"\n",
    "    )\n",
    "    print(\"Warning: No retrieved experiences to include in the prompt.\")  # noqa: T201\n",
    "\n",
    "# --- 2. Select Language and Format Final Prompt ---\n",
    "content_lang = content[LANGUAGE]  # Select EN or ES content dict\n",
    "initial_prompt = content_lang[\"prompt\"].format(\n",
    "    n_max_exp=MAX_RELEVANT_ITEMS,\n",
    "    example=content_lang[\"example\"],\n",
    "    description=JOB_DESCRIPTION,\n",
    "    experiences=experiences_context_string,\n",
    ")\n",
    "\n",
    "# --- 3. Initialize LLM and Chat for Rewriting ---\n",
    "chat = None  # Initialize chat variable\n",
    "try:\n",
    "    # Initialize the model instance for chat/rewriting\n",
    "    # Removed safety_settings argument as it wasn't defined previously\n",
    "    rewriting_model = GENERATIVE_MODEL_NAME\n",
    "\n",
    "    # Define generation config using parameters from Cell 16\n",
    "    # Assumes 'types' is available from a previous import\n",
    "    rewriting_generation_config = {\n",
    "        \"temperature\": REWRITING_LLM_TEMPERATURE,  # From Cell 16\n",
    "        \"top_p\": REWRITING_LLM_TOP_P,  # From Cell 16\n",
    "        \"top_k\": REWRITING_LLM_TOP_K,  # From Cell 16\n",
    "    }\n",
    "\n",
    "    # Start a new chat session (history is empty)\n",
    "    chat = client.chats.create(\n",
    "        model=GENERATIVE_MODEL_NAME, history=[], config=rewriting_generation_config\n",
    "    )\n",
    "    print(  # noqa: T201\n",
    "        f\"Chat session initialized with model '{GENERATIVE_MODEL_NAME}'.\"\n",
    "    )  # noqa: T201\n",
    "\n",
    "    # --- 4. Send Initial Prompt and Get Response ---\n",
    "    print(\"Sending initial prompt to LLM for analysis...\")  # noqa: T201\n",
    "    # Send the formatted prompt, applying the specific generation config\n",
    "    initial_response = chat.send_message(initial_prompt)\n",
    "\n",
    "\n",
    "except NameError as e:\n",
    "    # Catch errors if genai or types wasn't imported/available\n",
    "    print(  # noqa: T201\n",
    "        \"ERROR: Required object not defined (e.g., 'genai', 'types'). Check cell\"\n",
    "        \" execution order.\"\n",
    "    )\n",
    "    print(f\"Error: {e}\")  # noqa: T201\n",
    "    chat = None\n",
    "except Exception as e:\n",
    "    print(\"\\nERROR initializing chat or sending initial message:\")  # noqa: T201\n",
    "    print(f\"Error Type: {type(e).__name__}\")  # noqa: T201\n",
    "    print(f\"Error details: {e}\")  # noqa: T201\n",
    "    chat = None\n",
    "\n",
    "# Ensure 'chat' variable exists for the next step\n",
    "if \"chat\" not in locals():\n",
    "    chat = None\n",
    "\n",
    "# Ensure variables exist for the next step\n",
    "if \"chat\" not in locals():\n",
    "    chat = None\n",
    "if \"initial_response\" not in locals():\n",
    "    initial_response = None\n",
    "\n",
    "# Verification print moved to focus only on successful execution of this cell's task\n",
    "if chat and initial_response:\n",
    "    print(\"Initial LLM response obtained successfully.\")  # noqa: T201\n",
    "else:\n",
    "    print(\"Failed to obtain initial LLM response or initialize chat.\")  # noqa: T201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e4586",
   "metadata": {
    "papermill": {
     "duration": 0.014474,
     "end_time": "2025-04-21T00:52:06.083467",
     "exception": false,
     "start_time": "2025-04-21T00:52:06.068993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7.3. Interactive Refinement Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5295a2",
   "metadata": {
    "papermill": {
     "duration": 0.01203,
     "end_time": "2025-04-21T00:52:06.107748",
     "exception": false,
     "start_time": "2025-04-21T00:52:06.095718",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The following cell starts the interactive loop. First, it displays the initial analysis received from the LLM in the previous step. Then, it repeatedly prompts you for input.\n",
    "\n",
    "Based on the LLM's analysis and questions, provide your feedback or select an experience to modify. Your input will be sent back to the LLM, and its response will be displayed.\n",
    "\n",
    "Enter 'q' or 'quit' (or the Spanish equivalent if configured) to exit the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369a960",
   "metadata": {
    "papermill": {
     "duration": 0.027388,
     "end_time": "2025-04-21T00:52:06.148976",
     "exception": false,
     "start_time": "2025-04-21T00:52:06.121588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display  # noqa: F811\n",
    "\n",
    "# Assumes 'chat', 'initial_response', 'content', 'LANGUAGE',\n",
    "# 'rewriting_generation_config' exist from previous cells.\n",
    "\n",
    "# --- 1. Display Initial LLM Analysis ---\n",
    "# Display the response received from the initial prompt (Cell 50)\n",
    "if \"initial_response\" in locals() and initial_response:\n",
    "    print(\"\\n\" + \"=\" * 80)  # noqa: T201\n",
    "    print(\"LLM Initial Analysis Response:\")  # noqa: T201\n",
    "    print(\"=\" * 80)  # noqa: T201\n",
    "    display(Markdown(initial_response.text))\n",
    "    print(\"=\" * 80)  # noqa: T201\n",
    "else:\n",
    "    print(\"\\nWarning: Initial LLM response not available to display.\")  # noqa: T201\n",
    "\n",
    "# --- 2. Interactive Refinement Loop ---\n",
    "# Proceed only if the chat object was successfully created\n",
    "if \"chat\" in locals() and chat:\n",
    "    print(\"\\n--- Starting Interactive Refinement ---\")  # noqa: T201\n",
    "    # Get the appropriate quit message for the configured language\n",
    "    quit_msg = content[LANGUAGE].get(\"quit_msg\", \"Enter 'q' or 'quit' to exit\")\n",
    "    # Define various inputs that signal exiting the loop (incl. 'n' for No)\n",
    "    quit_options = {\"q\", \"quit\", \"exit\", \"salir\", \"n\"}\n",
    "\n",
    "    while True:\n",
    "        # Display quit instructions clearly\n",
    "        print(\"\\n\" + f\" {quit_msg} \".center(80, \"-\"))  # noqa: T201\n",
    "        try:\n",
    "            # Prompt user for input\n",
    "            user_input = input(\"> \")\n",
    "\n",
    "            # Check if the user wants to exit\n",
    "            if user_input.lower() in quit_options:\n",
    "                print(\"\\nExiting interactive refinement loop.\")  # noqa: T201\n",
    "                break\n",
    "\n",
    "            # Assume 'Yes' (continue/approve) if user just presses Enter\n",
    "            if not user_input:\n",
    "                user_input = \"Y\"\n",
    "                print(  # noqa: T201\n",
    "                    \"(Interpreting empty input as 'Y' to continue/confirm)\"\n",
    "                )  # noqa: T201\n",
    "\n",
    "            # Send the user's message to the LLM via the chat session\n",
    "            print(\"\\nSending message to LLM...\")  # noqa: T201\n",
    "            llm_response = chat.send_message(user_input)\n",
    "            print(\"LLM response received:\")  # noqa: T201\n",
    "\n",
    "            # Display the LLM's response using Markdown rendering\n",
    "            display(Markdown(llm_response.text))\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nLoop interrupted by user (Ctrl+C). Exiting.\")  # noqa: T201\n",
    "            break\n",
    "        except Exception as e:\n",
    "            # Catch potential errors during chat interaction\n",
    "            print(\"\\nERROR during interactive chat:\")  # noqa: T201\n",
    "            print(f\"Error Type: {type(e).__name__}\")  # noqa: T201\n",
    "            print(f\"Error details: {e}\")  # noqa: T201\n",
    "            print(\"Exiting interactive loop due to error.\")  # noqa: T201\n",
    "            break  # Exit loop on error\n",
    "\n",
    "    print(\"--- End of Interactive Refinement ---\")  # noqa: T201\n",
    "\n",
    "else:\n",
    "    # Message if chat object wasn't created successfully earlier\n",
    "    print(  # noqa: T201\n",
    "        \"\\nSkipping interactive loop: Chat object ('chat') is not available.\"\n",
    "    )  # noqa: T201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85134df0",
   "metadata": {
    "papermill": {
     "duration": 0.013142,
     "end_time": "2025-04-21T00:52:06.176952",
     "exception": false,
     "start_time": "2025-04-21T00:52:06.163810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Conclusion & Take-aways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a74085",
   "metadata": {
    "papermill": {
     "duration": 0.013431,
     "end_time": "2025-04-21T00:52:06.203837",
     "exception": false,
     "start_time": "2025-04-21T00:52:06.190406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook demonstrated a complete workflow for optimizing CV sections against a job description using RAG and Generative AI:\n",
    "\n",
    "* **CV Structuring:** Successfully parsed a PDF CV into a structured JSON format using an LLM (Gemini).\n",
    "* **Vector Search (RAG):** Created embeddings for CV items and utilized ChromaDB to retrieve the sections most semantically relevant to a specific job description.\n",
    "* **LLM-Powered Rewriting:** Employed a Gemini chat model in an interactive loop to refine the retrieved CV sections, incorporating keywords and optimizing for ATS.\n",
    "* **Self-Contained Workflow:** The entire process, from PDF loading to interactive rewriting, is contained within this single Kaggle notebook.\n",
    "\n",
    "This approach allows for targeted CV customization, potentially improving application success rates by better aligning the CV with job requirements and ATS filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820da19b",
   "metadata": {
    "papermill": {
     "duration": 0.015175,
     "end_time": "2025-04-21T00:52:06.232303",
     "exception": false,
     "start_time": "2025-04-21T00:52:06.217128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 9. Final Remarks / Contact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621f5649",
   "metadata": {
    "papermill": {
     "duration": 0.012673,
     "end_time": "2025-04-21T00:52:06.257754",
     "exception": false,
     "start_time": "2025-04-21T00:52:06.245081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "🌟 CONGRATULATIONS 🌟\n",
    "\n",
    "If you’ve made it all the way to the end of this notebook (whether you’re an evaluator, a fellow data‑enthusiast, or just curious about the process), you definitely deserve a prize!\n",
    "\n",
    "Our project is evolving every week, and your feedback keeps it alive.\n",
    "👉 Show some love by giving the repo a star on GitHub:\n",
    "https://github.com/framunoz/cv-analyser-with-rag/\n",
    "\n",
    "A single ⭐ helps the framework grow, motivates new features, and lets us know this work is valuable to the community.\n",
    "\n",
    "Thanks for reading, building, and experimenting alongside us—see you in the next commit!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 97258,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 70.308237,
   "end_time": "2025-04-21T00:52:07.595838",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-21T00:50:57.287601",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
