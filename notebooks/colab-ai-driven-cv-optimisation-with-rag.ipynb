{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/framunoz/cv-analyser-with-rag/blob/feature%2Fkaggle-colab-notebooks/notebooks/colab-ai-driven-cv-optimisation-with-rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87cbd2aa",
      "metadata": {
        "papermill": {
          "duration": 0.010245,
          "end_time": "2025-04-21T00:51:01.848333",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.838088",
          "status": "completed"
        },
        "tags": [],
        "id": "87cbd2aa"
      },
      "source": [
        "# 📝 Optimize Your CV with AI to Beat the ATS Filters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad24d49",
      "metadata": {
        "papermill": {
          "duration": 0.00828,
          "end_time": "2025-04-21T00:51:01.865453",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.857173",
          "status": "completed"
        },
        "tags": [],
        "id": "3ad24d49"
      },
      "source": [
        "## 1. Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fe8e3b",
      "metadata": {
        "papermill": {
          "duration": 0.008816,
          "end_time": "2025-04-21T00:51:01.883005",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.874189",
          "status": "completed"
        },
        "tags": [],
        "id": "61fe8e3b"
      },
      "source": [
        "**What does this notebook do?**\n",
        "\n",
        "Applying for jobs often means getting past automated screening software (called ATS - Applicant Tracking Systems) *before* a human reads your CV. This notebook acts as your **personal AI assistant** to help tailor your CV for specific job openings, increasing your chances of getting noticed.\n",
        "\n",
        "Here's the plan:\n",
        "1.  You provide your CV and the description of a job you want.\n",
        "2.  AI reads and understands your CV's content.\n",
        "3.  It compares your CV to the job description to find your **most relevant** experiences or skills for that role.\n",
        "4.  Then, the AI helps you **rewrite** those key parts of your CV, adding important keywords from the job description to make it **ATS-friendly** and more impactful for recruiters.\n",
        "\n",
        "**Who is this for?**\n",
        "\n",
        "Anyone applying for jobs who wants a smarter way to customize their CV for each application. **No AI or programming knowledge is needed!**\n",
        "\n",
        "**How it works (in simple terms):**\n",
        "\n",
        "* **Understanding Text:** The AI reads the text from your CV.\n",
        "* **Making Connections (Embeddings & RAG):** It represents the meaning of your CV sections and the job description in a special way (like giving ideas coordinates). This allows it to mathematically find which parts of your CV are the closest match to the job requirements (this smart searching is sometimes called RAG).\n",
        "* **AI Writing Assistant (LLM):** It uses a powerful AI language model (Google's Gemini) to help rephrase your relevant experiences, weaving in keywords naturally.\n",
        "* **You're in Control:** You'll interact with the AI in a chat to guide the rewriting process, ensuring the final text sounds like you.\n",
        "\n",
        "**Steps We'll Follow:**\n",
        "\n",
        "1.  **Setup:** Prepare the notebook (install tools, add your secure Google AI API key).\n",
        "2.  **Your Input:** Provide your CV and the job description using simple forms/buttons.\n",
        "3.  **CV Processing:** AI reads and structures your CV information.\n",
        "4.  **Create CV \"Memory\":** AI creates a searchable representation of your CV content (using embeddings and a vector database called ChromaDB).\n",
        "5.  **Find Best Matches:** AI searches its \"memory\" to find the parts of your CV most relevant to the job.\n",
        "6.  **Rewrite & Refine:** You chat with the AI to optimize the suggested CV sections.\n",
        "7.  **Review:** Check the final optimized text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc57789",
      "metadata": {
        "papermill": {
          "duration": 0.008258,
          "end_time": "2025-04-21T00:51:01.899782",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.891524",
          "status": "completed"
        },
        "tags": [],
        "id": "2dc57789"
      },
      "source": [
        "## 2. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3945b6c5",
      "metadata": {
        "papermill": {
          "duration": 0.008222,
          "end_time": "2025-04-21T00:51:01.916465",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.908243",
          "status": "completed"
        },
        "tags": [],
        "id": "3945b6c5"
      },
      "source": [
        "This section handles the initial preparation needed to run the notebook in Google Colab. We'll install necessary software libraries and guide you on securely setting up your Google AI API key."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252b77ab",
      "metadata": {
        "papermill": {
          "duration": 0.00801,
          "end_time": "2025-04-21T00:51:01.933143",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.925133",
          "status": "completed"
        },
        "tags": [],
        "id": "252b77ab"
      },
      "source": [
        "### 2.1. Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to install the necessary Python libraries. These are software packages that give our notebook the tools needed to work with PDFs, connect to the Google AI services, manage the special \"CV memory\" database (ChromaDB), and handle file uploads. This cell might take a minute or two to run."
      ],
      "metadata": {
        "id": "kQoFevzEs-5u"
      },
      "id": "kQoFevzEs-5u"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a9240366",
      "metadata": {
        "papermill": {
          "duration": 46.074818,
          "end_time": "2025-04-21T00:51:48.016501",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.941683",
          "status": "completed"
        },
        "tags": [],
        "id": "a9240366",
        "outputId": "fcef8e64-0188-4f11-873d-a521d88b221d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "✅ Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "#@title Install Required Libraries (click the ► button to run)\n",
        "# This command installs the necessary Python packages for the notebook.\n",
        "# It might take a minute or two to complete.\n",
        "# The '-q' flag makes the output less noisy (quieter).\n",
        "\n",
        "# Uninstall conflicting library (optional step from original notebook, may help in Colab)\n",
        "!pip uninstall -qqy jupyterlab\n",
        "\n",
        "# Install Google AI, PDF reader, Vector DB, GDrive downloader (if used), YAML handler\n",
        "# We install a specific version of google-genai used in the original development\n",
        "# and typing_extensions for compatibility.\n",
        "!pip install -q google-genai pdfplumber chromadb gdown PyYAML typing_extensions\n",
        "\n",
        "print(\"✅ Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64076413",
      "metadata": {
        "papermill": {
          "duration": 0.010587,
          "end_time": "2025-04-21T00:51:48.038561",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.027974",
          "status": "completed"
        },
        "tags": [],
        "id": "64076413"
      },
      "source": [
        "### 2.2. API Key Configuration (Colab Secrets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a172b04",
      "metadata": {
        "papermill": {
          "duration": 0.010462,
          "end_time": "2025-04-21T00:51:48.059579",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.049117",
          "status": "completed"
        },
        "tags": [],
        "id": "4a172b04"
      },
      "source": [
        "To use the Google AI services (like Gemini), this notebook needs your API key. Colab offers convenient and secure ways to add it using the **Secrets** manager (click the **🔑 key icon** in the left sidebar).\n",
        "\n",
        "**➡️ Providing Your API Key (Choose ONE method):**\n",
        "\n",
        "* **Method 1: Import from Google AI Studio (Recommended)**\n",
        "    1.  Click the **🔑 (key) icon** in the left sidebar to open Secrets.\n",
        "    2.  Find the **\"Gemini API keys\"** dropdown menu.\n",
        "    3.  Select **\"Import key from Google AI Studio\"**.\n",
        "    4.  Follow the on-screen prompts to authenticate and import your existing key. Colab should store it securely, likely naming it `GOOGLE_API_KEY` automatically.\n",
        "\n",
        "* **Method 2: Add Manually**\n",
        "    1.  Click the **🔑 (key) icon** in the left sidebar.\n",
        "    2.  Click **\"+ Add new secret\"**.\n",
        "    3.  Enter the **Name** exactly as `GOOGLE_API_KEY`.\n",
        "    4.  Paste your Google AI API key (obtainable from [Google AI Studio](https://aistudio.google.com/app/apikey) or Google Cloud) into the **Value** field.\n",
        "    5.  **Enable** the toggle switch ☑️ \"Notebook access\".\n",
        "    6.  Close the Secrets panel.\n",
        "\n",
        "The next code cell will attempt to read the secret named `GOOGLE_API_KEY`. **Ensure the key is available via one of these methods before running the next cell.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1af7af19",
      "metadata": {
        "papermill": {
          "duration": 0.106451,
          "end_time": "2025-04-21T00:51:48.176706",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.070255",
          "status": "completed"
        },
        "tags": [],
        "id": "1af7af19",
        "outputId": "71664a21-da8c-4e9d-ef63-c1c546c74e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GOOGLE_API_KEY loaded successfully from Colab Secrets.\n"
          ]
        }
      ],
      "source": [
        "#@title Load Google AI API Key from Colab Secrets (click the ► button to run)\n",
        "from google.colab import userdata\n",
        "\n",
        "# This dictionary provides helpful messages based on errors\n",
        "_API_KEY_ERROR_MESSAGES = {\n",
        "    \"NOT_FOUND\": \"\"\"\n",
        "❌ **Secret 'GOOGLE_API_KEY' not found.**\n",
        "   Please ensure you have added the secret correctly using one of the methods\n",
        "   described above (using the 🔑 icon in the left sidebar) and that\n",
        "   'Notebook access' is enabled for it.\n",
        "    \"\"\",\n",
        "    \"EMPTY\": \"\"\"\n",
        "❌ **Secret 'GOOGLE_API_KEY' was found but is empty.**\n",
        "   Please check the secret value you added using the 🔑 icon in the left sidebar.\n",
        "    \"\"\",\n",
        "    \"IMPORT_ERROR\": \"\"\"\n",
        "❌ **Could not access Colab Secrets.**\n",
        "   This notebook is designed to run in Google Colab. If you are in Colab,\n",
        "   try restarting the runtime (Runtime -> Restart runtime).\n",
        "    \"\"\",\n",
        "    \"UNKNOWN\": \"\"\"\n",
        "❌ **An unexpected error occurred while loading the API key.**\n",
        "   Details: {error}\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Attempt to retrieve the secret named 'GOOGLE_API_KEY'\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    if GOOGLE_API_KEY is None:\n",
        "        # Secret not found or access not enabled\n",
        "        print(_API_KEY_ERROR_MESSAGES[\"NOT_FOUND\"])\n",
        "        raise ValueError(\"API Key not found.\") # Stop execution\n",
        "    elif GOOGLE_API_KEY == \"\":\n",
        "        # Secret found but has no value\n",
        "        print(_API_KEY_ERROR_MESSAGES[\"EMPTY\"])\n",
        "        raise ValueError(\"API Key is empty.\") # Stop execution\n",
        "    else:\n",
        "        # Key loaded successfully\n",
        "        print(\"✅ GOOGLE_API_KEY loaded successfully from Colab Secrets.\")\n",
        "        # Optional: Uncomment below to verify the first few chars (for debugging)\n",
        "        # print(f\"(Key starts with: {GOOGLE_API_KEY[:4]}...)\")\n",
        "\n",
        "except ImportError:\n",
        "    # Handle case where userdata is not available (running outside Colab)\n",
        "    print(_API_KEY_ERROR_MESSAGES[\"IMPORT_ERROR\"])\n",
        "    GOOGLE_API_KEY = None\n",
        "    raise # Stop execution\n",
        "except Exception as e:\n",
        "    # Handle any other unexpected errors during secret retrieval\n",
        "    print(_API_KEY_ERROR_MESSAGES[\"UNKNOWN\"].format(error=e))\n",
        "    GOOGLE_API_KEY = None\n",
        "    raise # Stop execution\n",
        "\n",
        "# If the code reaches here without raising an error, the key is loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bcca856",
      "metadata": {
        "papermill": {
          "duration": 0.011349,
          "end_time": "2025-04-21T00:51:48.199290",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.187941",
          "status": "completed"
        },
        "tags": [],
        "id": "1bcca856"
      },
      "source": [
        "### 2.3. Helper Functions (Retry Logic)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72cb31ee",
      "metadata": {
        "papermill": {
          "duration": 0.010982,
          "end_time": "2025-04-21T00:51:48.221562",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.210580",
          "status": "completed"
        },
        "tags": [],
        "id": "72cb31ee"
      },
      "source": [
        "Sometimes, when contacting the Google AI service, it might be temporarily busy or unavailable (like getting a busy signal on a phone line). To handle this smoothly, the next code cell defines a small helper function.\n",
        "\n",
        "This helper allows the notebook to automatically try contacting the service again a few times if it encounters these specific temporary issues. This makes the whole process more reliable without you having to manually re-run things for minor glitches. You don't need to worry about the details of the code itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b0de59f4",
      "metadata": {
        "papermill": {
          "duration": 1.43616,
          "end_time": "2025-04-21T00:51:49.668831",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.232671",
          "status": "completed"
        },
        "tags": [],
        "id": "b0de59f4"
      },
      "outputs": [],
      "source": [
        "#@title Define Helper Function for API Retries (click to expand)\n",
        "from google import genai\n",
        "from google.api_core import retry\n",
        "\n",
        "def is_retriable(exception: Exception) -> bool:\n",
        "    \"\"\"Checks if an exception is a known temporary/retriable Google API Error.\"\"\"\n",
        "    return isinstance(exception, genai.errors.APIError) and exception.code in {429, 503}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba3bf860",
      "metadata": {
        "papermill": {
          "duration": 0.011781,
          "end_time": "2025-04-21T00:51:49.691529",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.679748",
          "status": "completed"
        },
        "tags": [],
        "id": "ba3bf860"
      },
      "source": [
        "## 3. User Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4df8521",
      "metadata": {
        "papermill": {
          "duration": 0.010399,
          "end_time": "2025-04-21T00:51:49.712837",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.702438",
          "status": "completed"
        },
        "tags": [],
        "id": "f4df8521"
      },
      "source": [
        "Adjust the variables in the following code cell to control the notebook's behavior. These include file paths, language settings, model parameters, and the job description you want to analyze."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2a61b792",
      "metadata": {
        "papermill": {
          "duration": 0.023849,
          "end_time": "2025-04-21T00:51:49.747395",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.723546",
          "status": "completed"
        },
        "tags": [],
        "id": "2a61b792"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Literal\n",
        "\n",
        "# ==============================================================================\n",
        "# User Configuration Variables\n",
        "# ==============================================================================\n",
        "# Adjust these settings before running the rest of the notebook.\n",
        "\n",
        "# --- 1. Input Files & Paths ---\n",
        "\n",
        "# Specify the path where the CV PDF file is located or will be downloaded to.\n",
        "CV_PDF_PATH = Path(\"./resume.pdf\")  # Example: file in the root working directory\n",
        "\n",
        "# Set to True to download the CV from Google Drive using the ID below.\n",
        "# Set to False if you are providing the CV via CV_PDF_PATH directly.\n",
        "DOWNLOAD_CV = True  # Default behavior from original notebook\n",
        "CV_GDRIVE_FILE_ID = \"1avK0u9HcyuEYgpyIs_pBuzPxWPrfgW_C\"  # Example File ID\n",
        "\n",
        "# --- 2. Job Description Input ---\n",
        "\n",
        "# Paste the target job description text between the triple quotes.\n",
        "JOB_DESCRIPTION = \"\"\"\n",
        "Job Title: AI/Machine Learning Engineer\n",
        "\n",
        "Company: Innovate Solutions Inc.\n",
        "\n",
        "Location: Remote (US Based)\n",
        "\n",
        "About Us:\n",
        "Innovate Solutions Inc. is at the forefront of applying artificial intelligence to solve real-world business challenges. We foster a collaborative environment where creative thinking and technical excellence drive our success. We are passionate about building intelligent systems that deliver significant value to our clients across various industries. Join our growing team and help shape the future of applied AI.\n",
        "\n",
        "About the Role:\n",
        "We are seeking a talented and motivated AI/Machine Learning Engineer to join our core development team. You will play a key role in the end-to-end lifecycle of machine learning projects, from conceptualization and data exploration to model deployment and monitoring. You'll work closely with data scientists, software engineers, and product managers to build innovative AI-powered features and products.\n",
        "\n",
        "Responsibilities:\n",
        "- Design, develop, train, and deploy machine learning models (including deep learning models) for tasks such as NLP, predictive analytics, anomaly detection, and personalization.\n",
        "- Process, cleanse, and verify the integrity of large datasets used for analysis and model training.\n",
        "- Collaborate with data engineering teams to build and maintain robust data pipelines for ML workflows.\n",
        "- Implement and maintain MLOps best practices for model versioning, testing, deployment, and monitoring.\n",
        "- Stay current with the latest advancements in AI/ML techniques, tools, and platforms.\n",
        "- Analyze experimental results, iterate on models, and communicate findings to technical and non-technical stakeholders.\n",
        "- Contribute to the development of internal AI platforms and tooling.\n",
        "\n",
        "Required Qualifications:\n",
        "- Bachelor's or Master's degree in Computer Science, Data Science, Statistics, or a related quantitative field.\n",
        "- 2+ years of hands-on experience building and deploying machine learning models in a production environment.\n",
        "- Strong programming skills in Python and proficiency with relevant ML libraries (e.g., Scikit-learn, TensorFlow, PyTorch, Keras).\n",
        "- Solid understanding of core machine learning algorithms, statistical modeling, and evaluation metrics.\n",
        "- Experience working with SQL and/or NoSQL databases.\n",
        "- Familiarity with data processing and analysis libraries (e.g., Pandas, NumPy).\n",
        "- Excellent problem-solving skills and attention to detail.\n",
        "- Strong communication and teamwork abilities.\n",
        "\n",
        "Desired Qualifications (Bonus Points):\n",
        "- PhD in a related field.\n",
        "- Experience with cloud platforms (AWS, GCP, or Azure) and their AI/ML services (e.g., SageMaker, Vertex AI, Azure ML).\n",
        "- Experience with MLOps tools and practices (e.g., Docker, Kubernetes, MLflow, Kubeflow).\n",
        "- Experience with Natural Language Processing (NLP) or Computer Vision (CV).\n",
        "- Experience with big data technologies (e.g., Spark, Hadoop).\n",
        "- Publications in relevant AI/ML conferences or journals.\n",
        "\n",
        "What We Offer:\n",
        "- Competitive salary and benefits package.\n",
        "- Opportunity to work on challenging and impactful AI projects.\n",
        "- A dynamic, collaborative, and supportive work environment.\n",
        "- Flexible remote work policy.\n",
        "- Professional development opportunities.\n",
        "\"\"\"\n",
        "\n",
        "# --- 3. Language Settings ---\n",
        "\n",
        "# Set the primary language for prompts (influences LLM responses).\n",
        "LANGUAGE: Literal[\"en\", \"es\"] = \"en\"  # Options: 'en' or 'es'\n",
        "\n",
        "# --- 4. RAG & Embedding Settings ---\n",
        "\n",
        "# Retrieval Parameters for RAG Search (Controls how many results are fetched/processed)\n",
        "MAX_RELEVANT_ITEMS = 3  # Target number of CV items to refine\n",
        "RETRIEVAL_WINDOW = 2  # Extra items retrieved for context during search\n",
        "\n",
        "# Embedding Model Name\n",
        "EMBEDDING_MODEL_NAME = \"models/text-embedding-004\"\n",
        "\n",
        "# Vector Database Configuration (Using Single Collection Strategy)\n",
        "CHROMA_DB_PATH = Path(\"./chroma_db_persistent\")  # Path for persistent DB\n",
        "COLLECTION_NAME = \"cv_embeddings_v1\"  # Name for the single collection\n",
        "\n",
        "# --- 5. LLM Configuration ---\n",
        "\n",
        "# Generative model name for structuring the CV and for the rewriting chat.\n",
        "# Using the specific name mentioned by the user.\n",
        "GENERATIVE_MODEL_NAME = \"gemini-2.0-flash\"\n",
        "\n",
        "# Parameters for the PDF -> JSON *Structuring* LLM call\n",
        "STRUCTURING_LLM_TEMPERATURE = 0.1\n",
        "\n",
        "# Parameters for the CV Item *Rewriting* LLM Chat\n",
        "REWRITING_LLM_TEMPERATURE = 0.8\n",
        "REWRITING_LLM_TOP_P = 0.95\n",
        "REWRITING_LLM_TOP_K = 30\n",
        "\n",
        "# --- 6. CV Processing Settings ---\n",
        "\n",
        "# List the sections (JSON keys) from the structured CV to embed and use for RAG.\n",
        "# Keeping this as a list allows processing/searching multiple sections with the single DB collection.\n",
        "# Example sections based on JSON Resume schema and original recommendations:\n",
        "# \"work\", \"certificates\", \"publications\", \"projects\", \"skills\", \"education\"\n",
        "CV_SECTIONS_TO_FOCUS: list[str] = [\n",
        "    \"work\",\n",
        "    \"certificates\",\n",
        "    \"projects\",\n",
        "]  # Edit this list as needed\n",
        "\n",
        "# Optional: Limit characters fed to the structuring LLM. None = no limit.\n",
        "MAX_CV_TEXT_LENGTH_FOR_STRUCTURING = 12000\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Directory Setup (Using pathlib)\n",
        "# ==============================================================================\n",
        "\n",
        "# Ensure the directory for ChromaDB exists\n",
        "CHROMA_DB_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Ensure the parent directory for the CV PDF exists\n",
        "CV_PDF_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "print(\"User configuration loaded and directories created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f2041e",
      "metadata": {
        "papermill": {
          "duration": 0.010359,
          "end_time": "2025-04-21T00:51:49.768747",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.758388",
          "status": "completed"
        },
        "tags": [],
        "id": "c3f2041e"
      },
      "source": [
        "## 4. Load and Process CV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b608f056",
      "metadata": {
        "papermill": {
          "duration": 0.010947,
          "end_time": "2025-04-21T00:51:49.853659",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.842712",
          "status": "completed"
        },
        "tags": [],
        "id": "b608f056"
      },
      "source": [
        "### 4.1. Load PDF Document"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b01fe7d",
      "metadata": {
        "papermill": {
          "duration": 0.011089,
          "end_time": "2025-04-21T00:51:49.876186",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.865097",
          "status": "completed"
        },
        "tags": [],
        "id": "9b01fe7d"
      },
      "source": [
        "This step ensures the CV PDF file specified in the configuration (`CV_PDF_PATH`) is available. If `DOWNLOAD_CV` was set to `True`, it downloads the file from Google Drive using the provided ID. Otherwise, it assumes the file exists at `CV_PDF_PATH`.\n",
        "\n",
        "Finally, it checks if the file exists and displays the PDF inline for verification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a02db192",
      "metadata": {
        "papermill": {
          "duration": 4.013851,
          "end_time": "2025-04-21T00:51:53.900903",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.887052",
          "status": "completed"
        },
        "tags": [],
        "id": "a02db192",
        "outputId": "825a7d57-b8af-466f-93b0-5003cee8e547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "CV Schema ('Curriculum') not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5953af41942f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'GOOGLE_API_KEY'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mGOOGLE_API_KEY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY not set.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Check other necessary variables defined earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;34m'Curriculum'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CV Schema ('Curriculum') not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'is_retriable'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Retry helper ('is_retriable') not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'GENERATIVE_MODEL_NAME'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GENERATIVE_MODEL_NAME not set.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: CV Schema ('Curriculum') not found."
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "from IPython.display import IFrame, display\n",
        "\n",
        "# --- 1. Conditional Download ---\n",
        "if DOWNLOAD_CV:\n",
        "    print(\n",
        "        f\"Attempting to download CV from Google Drive (ID: {CV_GDRIVE_FILE_ID}) to\"\n",
        "        f\" '{CV_PDF_PATH}'...\"\n",
        "    )\n",
        "    try:\n",
        "        # Download using gdown, specifying the output path\n",
        "        gdown.download(id=CV_GDRIVE_FILE_ID, output=str(CV_PDF_PATH), quiet=False)\n",
        "        print(\"Download attempt finished.\")\n",
        "    except Exception as e:\n",
        "        print(\"\\nERROR: Failed to download file from Google Drive.\")\n",
        "        print(\n",
        "            f\"Please check the file ID ('{CV_GDRIVE_FILE_ID}') and ensure the file is\"\n",
        "            \" accessible.\"\n",
        "        )\n",
        "        print(f\"Error details: {e}\")\n",
        "else:\n",
        "    print(f\"Skipping download. Assuming CV PDF is already present at: '{CV_PDF_PATH}'\")\n",
        "\n",
        "# --- 2. Verify File Existence and Display ---\n",
        "if CV_PDF_PATH.is_file():\n",
        "    print(f\"CV PDF found at '{CV_PDF_PATH}'. Displaying PDF inline below:\")\n",
        "    # Display the PDF inline for verification\n",
        "    display(IFrame(src=CV_PDF_PATH, width=\"90%\", height=\"600px\"))\n",
        "else:\n",
        "    print(f\"\\nERROR: CV PDF file was not found at the path: '{CV_PDF_PATH}'\")\n",
        "    # Provide context based on whether download was attempted\n",
        "    if DOWNLOAD_CV:\n",
        "        print(\"The download may have failed, or the configured path is incorrect.\")\n",
        "    else:\n",
        "        print(\n",
        "            \"Please ensure the file exists at the specified path in the configuration.\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02608736",
      "metadata": {
        "papermill": {
          "duration": 0.011057,
          "end_time": "2025-04-21T00:51:53.923828",
          "exception": false,
          "start_time": "2025-04-21T00:51:53.912771",
          "status": "completed"
        },
        "tags": [],
        "id": "02608736"
      },
      "source": [
        "### 4.2. Extract Text from PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c43274",
      "metadata": {
        "papermill": {
          "duration": 0.010943,
          "end_time": "2025-04-21T00:51:53.945949",
          "exception": false,
          "start_time": "2025-04-21T00:51:53.935006",
          "status": "completed"
        },
        "tags": [],
        "id": "18c43274"
      },
      "source": [
        "Now that the PDF file is available, we will extract its text content. We use the `pdfplumber` library for this task, as it's generally effective for text-based PDFs. The extracted text will be stored in a variable for the next step (structuring with the LLM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e0191ef",
      "metadata": {
        "papermill": {
          "duration": 0.152752,
          "end_time": "2025-04-21T00:51:54.109897",
          "exception": false,
          "start_time": "2025-04-21T00:51:53.957145",
          "status": "completed"
        },
        "tags": [],
        "id": "3e0191ef"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "\n",
        "raw_cv_text = \"\"  # Initialize variable\n",
        "\n",
        "print(f\"Extracting text from PDF: '{CV_PDF_PATH}'...\")\n",
        "try:\n",
        "    with pdfplumber.open(CV_PDF_PATH) as pdf:\n",
        "        # Extract text page by page, handling potential None values from empty pages\n",
        "        raw_cv_text = \"\".join(\n",
        "            page.extract_text(x_tolerance=1, y_tolerance=3) or \"\" for page in pdf.pages\n",
        "        )\n",
        "\n",
        "    print(f\"Text extraction successful. Total characters: {len(raw_cv_text)}\")\n",
        "    # Optional: Uncomment the line below to print the first 500 characters\n",
        "    print(f\"--- Snippet ---\\n{raw_cv_text[:500]}\\n---------------\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nERROR: PDF file not found at '{CV_PDF_PATH}'. Cannot extract text.\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR: Failed to open or extract text from PDF '{CV_PDF_PATH}'.\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb680db8",
      "metadata": {
        "papermill": {
          "duration": 0.011342,
          "end_time": "2025-04-21T00:51:54.133119",
          "exception": false,
          "start_time": "2025-04-21T00:51:54.121777",
          "status": "completed"
        },
        "tags": [],
        "id": "cb680db8"
      },
      "source": [
        "### 4.3. Structure Text into JSON using LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "272a632f",
      "metadata": {
        "papermill": {
          "duration": 0.011457,
          "end_time": "2025-04-21T00:51:54.155957",
          "exception": false,
          "start_time": "2025-04-21T00:51:54.144500",
          "status": "completed"
        },
        "tags": [],
        "id": "272a632f"
      },
      "source": [
        "In this crucial step, we leverage the configured Generative AI model (Gemini) to parse the unstructured `raw_cv_text` extracted from the PDF. We instruct the model to return the information structured according to the [JSON Resume schema](https://jsonresume.org/schema/).\n",
        "\n",
        "This involves:\n",
        "1.  Defining the JSON Resume schema using Python's `TypedDict` for type safety and clarity (this helps the model adhere to the desired output structure).\n",
        "2.  Constructing a prompt that tells the model the task, provides the raw CV text (potentially truncated based on `MAX_CV_TEXT_LENGTH_FOR_STRUCTURING`), and specifies the desired JSON output format.\n",
        "3.  Calling the Gemini API, explicitly requesting JSON output and providing the schema definition.\n",
        "4.  Parsing the LLM's JSON response into a Python dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9d7e19d",
      "metadata": {
        "papermill": {
          "duration": 0.02762,
          "end_time": "2025-04-21T00:51:54.194945",
          "exception": false,
          "start_time": "2025-04-21T00:51:54.167325",
          "status": "completed"
        },
        "tags": [],
        "id": "b9d7e19d"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "# Define nested structures first (order matters for definition)\n",
        "class Location(TypedDict, total=False):\n",
        "    address: str\n",
        "    postalCode: str\n",
        "    city: str\n",
        "    countryCode: str\n",
        "    region: str\n",
        "\n",
        "\n",
        "class Profile(TypedDict, total=False):\n",
        "    network: str\n",
        "    username: str\n",
        "    url: str\n",
        "\n",
        "\n",
        "class Basics(TypedDict, total=False):\n",
        "    name: str\n",
        "    label: str\n",
        "    image: str  # URL to image\n",
        "    email: str\n",
        "    phone: str\n",
        "    url: str  # Personal website/portfolio URL\n",
        "    summary: str  # Professional summary\n",
        "    location: Location\n",
        "    profiles: list[Profile]  # Use modern list hint\n",
        "\n",
        "\n",
        "class WorkItem(TypedDict, total=False):\n",
        "    name: str  # Name of the company/organization\n",
        "    position: str  # Job title\n",
        "    url: str  # Company website\n",
        "    startDate: str  # Format YYYY-MM-DD or YYYY-MM or YYYY\n",
        "    endDate: str  # Format YYYY-MM-DD or YYYY-MM or YYYY, or Present\n",
        "    summary: str  # High-level description of role/company\n",
        "    highlights: list[str]  # Specific achievements or responsibilities (bullet points)\n",
        "\n",
        "\n",
        "class VolunteerItem(TypedDict, total=False):\n",
        "    organization: str\n",
        "    position: str\n",
        "    url: str\n",
        "    startDate: str\n",
        "    endDate: str\n",
        "    summary: str\n",
        "    highlights: list[str]\n",
        "\n",
        "\n",
        "class EducationItem(TypedDict, total=False):\n",
        "    institution: str\n",
        "    url: str\n",
        "    area: str  # e.g., Computer Science\n",
        "    studyType: str  # e.g., Bachelor's Degree, Master's\n",
        "    startDate: str\n",
        "    endDate: str\n",
        "    score: str  # e.g., GPA\n",
        "    courses: list[str]  # Relevant coursework\n",
        "\n",
        "\n",
        "class AwardItem(TypedDict, total=False):\n",
        "    title: str\n",
        "    date: str  # Date awarded\n",
        "    awarder: str  # Organization that gave the award\n",
        "    summary: str  # Description of the award\n",
        "\n",
        "\n",
        "class CertificateItem(TypedDict, total=False):\n",
        "    name: str  # Name of the certificate\n",
        "    date: str  # Date issued\n",
        "    issuer: str  # Issuing organization (e.g., Coursera, Google)\n",
        "    url: str  # Link to certificate if available\n",
        "\n",
        "\n",
        "class PublicationItem(TypedDict, total=False):\n",
        "    name: str  # Title of the publication\n",
        "    publisher: str  # e.g., Journal name, Conference\n",
        "    releaseDate: str\n",
        "    url: str  # Link to publication\n",
        "    summary: str  # Abstract or brief description\n",
        "\n",
        "\n",
        "class SkillItem(TypedDict, total=False):\n",
        "    name: str  # Broad skill category (e.g., Web Development, Data Science)\n",
        "    level: str  # Optional proficiency level (e.g., Intermediate, Advanced)\n",
        "    keywords: list[str]  # Specific technologies or tools (e.g., Python, PyTorch, AWS)\n",
        "\n",
        "\n",
        "class LanguageItem(TypedDict, total=False):\n",
        "    language: str  # e.g., English, Spanish\n",
        "    fluency: str  # e.g., Native, Fluent, Conversational\n",
        "\n",
        "\n",
        "class InterestItem(TypedDict, total=False):\n",
        "    name: str  # Category of interest (e.g., Open Source, AI Ethics)\n",
        "    keywords: list[str]  # Specific interests\n",
        "\n",
        "\n",
        "class ReferenceItem(TypedDict, total=False):\n",
        "    name: str  # Name of reference (ensure consent)\n",
        "    reference: str  # Testimonial or contact details (handle privacy appropriately)\n",
        "\n",
        "\n",
        "class ProjectItem(TypedDict, total=False):\n",
        "    name: str  # Project title\n",
        "    startDate: str\n",
        "    endDate: str\n",
        "    description: str  # Overall description of the project\n",
        "    highlights: list[str]  # Key contributions or features\n",
        "    url: str  # Link to project demo or repository\n",
        "\n",
        "\n",
        "# --- Top-Level Curriculum Schema ---\n",
        "class Curriculum(TypedDict, total=False):\n",
        "    \"\"\"Represents the complete JSON Resume structure.\"\"\"\n",
        "\n",
        "    basics: Basics\n",
        "    work: list[WorkItem]\n",
        "    volunteer: list[VolunteerItem]\n",
        "    education: list[EducationItem]\n",
        "    awards: list[AwardItem]\n",
        "    certificates: list[CertificateItem]\n",
        "    publications: list[PublicationItem]\n",
        "    skills: list[SkillItem]\n",
        "    languages: list[LanguageItem]\n",
        "    interests: list[InterestItem]\n",
        "    references: list[ReferenceItem]\n",
        "    projects: list[ProjectItem]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315ee3d0",
      "metadata": {
        "papermill": {
          "duration": 6.341404,
          "end_time": "2025-04-21T00:52:00.547970",
          "exception": false,
          "start_time": "2025-04-21T00:51:54.206566",
          "status": "completed"
        },
        "tags": [],
        "id": "315ee3d0"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.api_core import retry\n",
        "\n",
        "# --- 1. Initialize Model & Validate API Key ---\n",
        "if \"GOOGLE_API_KEY\" not in locals() or not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"GOOGLE_API_KEY not found or empty. Please check Kaggle Secrets.\")\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# --- 2. Prepare Prompt & Configuration for JSON Structuring ---\n",
        "cv_text_for_prompt = raw_cv_text\n",
        "if (\n",
        "    MAX_CV_TEXT_LENGTH_FOR_STRUCTURING\n",
        "    and len(raw_cv_text) > MAX_CV_TEXT_LENGTH_FOR_STRUCTURING\n",
        "):\n",
        "    cv_text_for_prompt = raw_cv_text[:MAX_CV_TEXT_LENGTH_FOR_STRUCTURING]\n",
        "    print(\n",
        "        f\"Warning: CV text truncated to {MAX_CV_TEXT_LENGTH_FOR_STRUCTURING} chars for\"\n",
        "        \" structuring prompt.\"\n",
        "    )\n",
        "\n",
        "# System instruction defines the LLM's role and desired output format\n",
        "system_instruction = (\n",
        "    \"You are an expert CV parser. Extract information from the provided CV text and\"\n",
        "    \" format it strictly according to the JSON Resume Schema provided. Return ONLY the\"\n",
        "    \" valid JSON object conforming to the schema - no introductory text, no markdown\"\n",
        "    \" formatting ('```json', '```'), no explanations.\"\n",
        ")\n",
        "# Prompt combines instructions with the actual CV text\n",
        "prompt_message = f\"\"\"\n",
        "Given the following CV text, populate the fields of the JSON Resume Schema as accurately as possible.\n",
        "Use empty strings, arrays, or null values for fields where information is missing in the text.\n",
        "\n",
        "CV Text:\n",
        "---\n",
        "{cv_text_for_prompt}\n",
        "---\n",
        "\"\"\"\n",
        "full_structuring_prompt = system_instruction + \"\\n\\n\" + prompt_message\n",
        "\n",
        "# Configuration forces JSON output matching our TypedDict schema\n",
        "json_generation_config = {\n",
        "    \"temperature\": STRUCTURING_LLM_TEMPERATURE,\n",
        "    \"response_mime_type\": \"application/json\",\n",
        "    \"response_schema\": Curriculum,\n",
        "}\n",
        "\n",
        "\n",
        "# --- 3. Define Function for API Call with Retry Logic ---\n",
        "@retry.Retry(predicate=is_retriable)  # Use helper defined in Setup\n",
        "def generate_structured_cv_json_with_retry(prompt, config):\n",
        "    \"\"\"Calls the Gemini API to generate structured JSON, with retries on specific errors.\"\"\"\n",
        "    print(\"Calling Gemini API to structure CV text into JSON...\")\n",
        "    response = client.models.generate_content(\n",
        "        model=GENERATIVE_MODEL_NAME,\n",
        "        contents=prompt,\n",
        "        config=config,\n",
        "    )\n",
        "    print(\"Gemini API call finished.\")\n",
        "    return response.text\n",
        "\n",
        "\n",
        "# --- 4. Execute API Call and Parse Response ---\n",
        "structured_cv_data = None  # Initialize result variable\n",
        "\n",
        "try:\n",
        "    json_response_text = generate_structured_cv_json_with_retry(\n",
        "        prompt=full_structuring_prompt,\n",
        "        config=json_generation_config,\n",
        "    )\n",
        "\n",
        "    if json_response_text:\n",
        "        try:\n",
        "            # Parse the validated JSON text from the API response\n",
        "            structured_cv_data = json.loads(json_response_text)\n",
        "            print(\"Successfully parsed LLM response into structured CV data.\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            # This error *should* be rare given response_mime_type=\"application/json\"\n",
        "            print(\"\\nERROR: Failed to parse the LLM's response as JSON.\")\n",
        "            print(f\"JSONDecodeError: {e}\")\n",
        "            print(\"\\nLLM Response Text Received:\\n---\\n\", json_response_text, \"\\n---\")\n",
        "    else:\n",
        "        print(\"\\nERROR: Received an empty response from the LLM API.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\nERROR: An unexpected error occurred during the LLM structuring call.\")\n",
        "    print(f\"Error details: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cfa7c1",
      "metadata": {
        "papermill": {
          "duration": 0.011208,
          "end_time": "2025-04-21T00:52:00.571156",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.559948",
          "status": "completed"
        },
        "tags": [],
        "id": "f6cfa7c1"
      },
      "source": [
        "### 4.4. (Optional) Display Structured CV Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3e64ced",
      "metadata": {
        "papermill": {
          "duration": 0.011371,
          "end_time": "2025-04-21T00:52:00.594727",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.583356",
          "status": "completed"
        },
        "tags": [],
        "id": "e3e64ced"
      },
      "source": [
        "You can run the next cell to print the `structured_cv_data` dictionary in a readable YAML format. This helps verify that the LLM correctly parsed and structured the information from your CV text according to the JSON Resume schema. Check if key sections like 'basics', 'work', 'education', 'skills', etc., were populated as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc75b72e",
      "metadata": {
        "papermill": {
          "duration": 0.062397,
          "end_time": "2025-04-21T00:52:00.671007",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.608610",
          "status": "completed"
        },
        "tags": [],
        "id": "dc75b72e"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "print(\"#\" + \"=\" * 79)\n",
        "print(\"# Structured CV Data (YAML Format):\")\n",
        "print(\"#\" + \"=\" * 79)\n",
        "print(\n",
        "    yaml.dump(\n",
        "        structured_cv_data, allow_unicode=True, sort_keys=False, width=float(\"inf\")\n",
        "    )\n",
        ")\n",
        "print(\"#\" + \"=\" * 79)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7fb81c",
      "metadata": {
        "papermill": {
          "duration": 0.014399,
          "end_time": "2025-04-21T00:52:00.703230",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.688831",
          "status": "completed"
        },
        "tags": [],
        "id": "ba7fb81c"
      },
      "source": [
        "## 5. Prepare and Store Embeddings in ChromaDB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b689c5e1",
      "metadata": {
        "papermill": {
          "duration": 0.014778,
          "end_time": "2025-04-21T00:52:00.733826",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.719048",
          "status": "completed"
        },
        "tags": [],
        "id": "b689c5e1"
      },
      "source": [
        "This section focuses on converting the relevant parts of your structured CV data into vector embeddings and storing them in a searchable vector database (ChromaDB). This allows us to later find the CV items most semantically similar to a job description (the core of RAG).\n",
        "\n",
        "The key steps are:\n",
        "1.  **Prepare Documents:** Extract the specific CV items (like individual work experiences or projects) from the sections listed in `CV_SECTIONS_TO_FOCUS` and format them as text documents suitable for embedding. We'll use the YAML dump method as previously decided.\n",
        "2.  **Define Embedding Function:** Set up a function that uses the configured Google embedding model (`EMBEDDING_MODEL_NAME`) to convert text documents into numerical vectors (embeddings).\n",
        "3.  **Initialize Vector Database:** Connect to or create a persistent ChromaDB database using the configured path (`CHROMA_DB_PATH`) and collection name (`COLLECTION_NAME`). We will use a single collection for all items.\n",
        "4.  **Generate & Store Embeddings:** Iterate through the prepared documents, generate embeddings for them using the embedding function, and add the embeddings, the original documents, and relevant metadata (like the source section) to the ChromaDB collection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3fc7e27",
      "metadata": {
        "papermill": {
          "duration": 0.014624,
          "end_time": "2025-04-21T00:52:00.760536",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.745912",
          "status": "completed"
        },
        "tags": [],
        "id": "f3fc7e27"
      },
      "source": [
        "### 5.1. Prepare Text Documents from Structured CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92e33960",
      "metadata": {
        "papermill": {
          "duration": 0.047622,
          "end_time": "2025-04-21T00:52:00.823460",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.775838",
          "status": "completed"
        },
        "tags": [],
        "id": "92e33960"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import re\n",
        "\n",
        "# --- 1. Helper Functions for ID Generation ---\n",
        "\n",
        "# Maps section keys to functions creating a base ID string from item content\n",
        "BASE_ID_GENERATORS = {\n",
        "    \"work\": lambda item: (\n",
        "        f\"{item.get('name', 'NoCompany')}.{item.get('position', 'NoPosition')}\"\n",
        "    ),\n",
        "    \"certificates\": lambda item: (\n",
        "        f\"{item.get('issuer', 'NoIssuer')}.{item.get('name', 'NoCert')}\"\n",
        "    ),\n",
        "    \"publications\": lambda item: (\n",
        "        f\"{item.get('publisher', 'NoPublisher')}.{item.get('name', 'NoPub')}\"\n",
        "    ),\n",
        "    \"projects\": lambda item: item.get(\"name\", \"NoProject\"),\n",
        "    \"volunteer\": lambda item: (\n",
        "        f\"{item.get('organization', 'NoOrg')}.{item.get('position', 'NoVolunteerPos')}\"\n",
        "    ),\n",
        "    \"education\": lambda item: (\n",
        "        f\"{item.get('institution', 'NoInstitution')}.{item.get('area', 'NoArea')}.{item.get('studyType', '')}\"\n",
        "    ),\n",
        "    \"basics\": lambda item: item.get(\"name\", \"NoPerson\"),\n",
        "    \"awards\": lambda item: (\n",
        "        f\"{item.get('awarder', 'NoAwarder')}.{item.get('title', 'NoAward')}\"\n",
        "    ),\n",
        "    \"skills\": lambda item: item.get(\"name\", \"NoSkill\"),\n",
        "    \"languages\": lambda item: item.get(\"language\", \"NoLang\"),\n",
        "    \"interests\": lambda item: item.get(\"name\", \"NoInterest\"),\n",
        "    \"references\": lambda item: item.get(\"name\", \"NoReference\"),\n",
        "}\n",
        "\n",
        "\n",
        "def sanitize_id(text_id: str) -> str:\n",
        "    \"\"\"Cleans and formats a string into a valid ChromaDB ID.\"\"\"\n",
        "    text_id = text_id.lower()\n",
        "    # Basic accent removal using a lambda for compactness\n",
        "    # fmt: off\n",
        "    accent_map = {\n",
        "        'á':'a', 'ä':'a', 'â':'a', 'à':'a', 'ã':'a', 'å':'a', 'é':'e', 'ë':'e',\n",
        "        'ê':'e', 'è':'e', 'í':'i', 'ï':'i', 'î':'i', 'ì':'i', 'ó':'o', 'ö':'o',\n",
        "        'ô':'o', 'ò':'o', 'õ':'o', 'ø':'o', 'ú':'u', 'ü':'u', 'û':'u', 'ù':'u',\n",
        "        'ñ':'n', 'ç':'c',\n",
        "    }\n",
        "    # fmt: on\n",
        "    text_id = re.sub(\n",
        "        r\"[áäâàãåéëêèíïîìóöôòõøúüûùñç]\", lambda m: accent_map.get(m.group(0)), text_id\n",
        "    )\n",
        "    text_id = re.sub(r\"[\\s_:-]+\", \".\", text_id)  # Replace separators with dot\n",
        "    text_id = re.sub(r\"[^a-z0-9.]\", \"\", text_id)  # Keep only alphanumeric and dot\n",
        "    text_id = re.sub(r\"\\.+\", \".\", text_id)  # Consolidate consecutive dots\n",
        "    text_id = text_id.strip(\".\")  # Remove leading/trailing dots\n",
        "    if len(text_id) < 3:  # noqa: PLR2004\n",
        "        text_id = f\"{text_id}.id\"  # Ensure min length\n",
        "    return text_id[:63]  # Truncate to 63 characters (ChromaDB limit)\n",
        "\n",
        "\n",
        "def generate_unique_item_id(section_key: str, item: dict, item_index: int) -> str:\n",
        "    \"\"\"Generates a unique, sanitized ID for a CV item using dot separator.\"\"\"\n",
        "    id_generator = BASE_ID_GENERATORS.get(\n",
        "        section_key, lambda i: f\"item.{item_index}\"\n",
        "    )  # Fallback\n",
        "    base_id = id_generator(item)\n",
        "    full_id_base = f\"{section_key}.{base_id}.{item_index}\"  # Use dot separator\n",
        "    return sanitize_id(full_id_base)\n",
        "\n",
        "\n",
        "# --- 2. Function to Prepare Data for Embedding ---\n",
        "\n",
        "\n",
        "def prepare_embedding_data(\n",
        "    cv_data: dict, sections_to_include: list[str]\n",
        ") -> tuple[list, list, list]:\n",
        "    \"\"\"\n",
        "    Extracts items, formats documents (YAML strings), generates IDs, creates metadata.\n",
        "    Returns tuple: (documents, ids, metadatas).\n",
        "    \"\"\"\n",
        "    all_documents = []\n",
        "    all_ids = []\n",
        "    all_metadatas = []\n",
        "\n",
        "    if not isinstance(cv_data, dict):\n",
        "        print(\"Warning: structured_cv_data is not a dictionary.\")\n",
        "        return [], [], []\n",
        "\n",
        "    print(f\"Preparing embedding data for sections: {sections_to_include}...\")\n",
        "    processed_count = 0\n",
        "    for section_key in sections_to_include:\n",
        "        section_items = cv_data.get(section_key)\n",
        "\n",
        "        if not isinstance(section_items, list):\n",
        "            print(\"Warning: Expected a list for section items.\")\n",
        "            continue\n",
        "\n",
        "        for index, item in enumerate(section_items):\n",
        "            if not isinstance(item, dict):\n",
        "                print(\n",
        "                    f\"Warning: Expected a dictionary for item {index} in {section_key}.\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            item_id = generate_unique_item_id(section_key, item, index)\n",
        "            try:\n",
        "                item_doc = yaml.dump(\n",
        "                    item,\n",
        "                    allow_unicode=True,\n",
        "                    sort_keys=False,\n",
        "                    width=float(\"inf\"),\n",
        "                    default_flow_style=None,\n",
        "                )\n",
        "            except yaml.YAMLError:\n",
        "                item_doc = str(item)  # Fallback\n",
        "                print(f\"Warning: YAML dump failed for item {index} in {section_key}.\")\n",
        "\n",
        "            metadata = {\"section\": section_key, \"item_index\": index}\n",
        "            # Add potentially useful fields from item to metadata if they exist\n",
        "            for key in [\"name\", \"position\", \"issuer\", \"institution\", \"organization\"]:\n",
        "                if value := item.get(key):\n",
        "                    metadata[f\"id_{key}\"] = value\n",
        "\n",
        "            all_documents.append(item_doc)\n",
        "            all_ids.append(item_id)\n",
        "            all_metadatas.append(metadata)\n",
        "            processed_count += 1\n",
        "\n",
        "    print(f\"Prepared {processed_count} documents for embedding.\")\n",
        "    return all_documents, all_ids, all_metadatas\n",
        "\n",
        "\n",
        "# --- 3. Execute Preparation ---\n",
        "embedding_documents = []\n",
        "embedding_ids = []\n",
        "embedding_metadatas = []\n",
        "\n",
        "embedding_documents, embedding_ids, embedding_metadatas = prepare_embedding_data(\n",
        "    cv_data=structured_cv_data, sections_to_include=CV_SECTIONS_TO_FOCUS\n",
        ")\n",
        "\n",
        "if embedding_documents:\n",
        "    print(\"\\n--- Sample Prepared Data (First Item) ---\")\n",
        "    print(f\"ID        : {embedding_ids[0]}\")\n",
        "    print(f\"Metadata  : {embedding_metadatas[0]}\")\n",
        "    print(f\"Doc Snippet:\\n---\\n{embedding_documents[0][:200]}...\")\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "877792c9",
      "metadata": {
        "papermill": {
          "duration": 0.011798,
          "end_time": "2025-04-21T00:52:00.847888",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.836090",
          "status": "completed"
        },
        "tags": [],
        "id": "877792c9"
      },
      "source": [
        "### 5.2. Define Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97e50d18",
      "metadata": {
        "papermill": {
          "duration": 2.344372,
          "end_time": "2025-04-21T00:52:03.204298",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.859926",
          "status": "completed"
        },
        "tags": [],
        "id": "97e50d18"
      },
      "outputs": [],
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from google.api_core import retry\n",
        "import google.generativeai as genai\n",
        "from google.genai import types\n",
        "\n",
        "\n",
        "# Use the original class structure provided by the user\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    \"\"\"Custom ChromaDB embedding function using the original implementation structure.\"\"\"\n",
        "\n",
        "    def __init__(self, document_mode: bool = True) -> None:\n",
        "        \"\"\"Initializes based on document_mode, uses model from config.\"\"\"\n",
        "        self.embedding_task: str = (\n",
        "            \"retrieval_document\" if document_mode else \"retrieval_query\"\n",
        "        )\n",
        "        self.model: str = EMBEDDING_MODEL_NAME\n",
        "\n",
        "    @retry.Retry(predicate=is_retriable)\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        \"\"\"Generates embeddings using client.models.embed_content.\"\"\"\n",
        "        response = client.models.embed_content(\n",
        "            model=self.model,\n",
        "            contents=input,\n",
        "            config=types.EmbedContentConfig(task_type=self.embedding_task),\n",
        "        )\n",
        "        return [e.values for e in response.embeddings]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c2d50a3",
      "metadata": {
        "papermill": {
          "duration": 0.01143,
          "end_time": "2025-04-21T00:52:03.227758",
          "exception": false,
          "start_time": "2025-04-21T00:52:03.216328",
          "status": "completed"
        },
        "tags": [],
        "id": "7c2d50a3"
      },
      "source": [
        "### 5.3. Initialize Vector Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca8578c1",
      "metadata": {
        "papermill": {
          "duration": 0.369655,
          "end_time": "2025-04-21T00:52:03.609253",
          "exception": false,
          "start_time": "2025-04-21T00:52:03.239598",
          "status": "completed"
        },
        "tags": [],
        "id": "ca8578c1"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "cv_collection = None  # Initialize variable\n",
        "\n",
        "try:\n",
        "    # Initialize persistent client\n",
        "    print(f\"Initializing ChromaDB client at path: {CHROMA_DB_PATH}\")\n",
        "    chroma_client = chromadb.PersistentClient(path=str(CHROMA_DB_PATH))\n",
        "\n",
        "    gemini_embedder = GeminiEmbeddingFunction(document_mode=True)\n",
        "\n",
        "    print(f\"Accessing collection: '{COLLECTION_NAME}'...\")\n",
        "    cv_collection = chroma_client.get_or_create_collection(\n",
        "        name=COLLECTION_NAME, embedding_function=gemini_embedder\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"Collection '{cv_collection.name}' ready. Item count: {cv_collection.count()}\"\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\nERROR: Failed to initialize ChromaDB client or collection.\")\n",
        "    print(f\"Check path ('{CHROMA_DB_PATH}') and ChromaDB setup.\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991d84ef",
      "metadata": {
        "papermill": {
          "duration": 0.011673,
          "end_time": "2025-04-21T00:52:03.633048",
          "exception": false,
          "start_time": "2025-04-21T00:52:03.621375",
          "status": "completed"
        },
        "tags": [],
        "id": "991d84ef"
      },
      "source": [
        "### 5.4. Generate and Add Embeddings to Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6bc473f",
      "metadata": {
        "papermill": {
          "duration": 0.34581,
          "end_time": "2025-04-21T00:52:03.990835",
          "exception": false,
          "start_time": "2025-04-21T00:52:03.645025",
          "status": "completed"
        },
        "tags": [],
        "id": "a6bc473f"
      },
      "outputs": [],
      "source": [
        "# Add the prepared documents, IDs, and metadata to the ChromaDB collection.\n",
        "# ChromaDB uses the 'GeminiEmbeddingFunction' provided during collection creation\n",
        "# to automatically generate embeddings for the documents list.\n",
        "\n",
        "print(\n",
        "    f\"Adding/updating {len(embedding_documents)} documents in ChromaDB collection\"\n",
        "    f\" '{cv_collection.name}'...\"\n",
        ")\n",
        "try:\n",
        "    cv_collection.upsert(\n",
        "        ids=embedding_ids, metadatas=embedding_metadatas, documents=embedding_documents\n",
        "    )\n",
        "\n",
        "    print(\"\\nDocuments successfully added/updated in the collection.\")\n",
        "    final_count = cv_collection.count()\n",
        "    print(f\"Collection '{cv_collection.name}' now contains {final_count} items.\")\n",
        "    if final_count < len(embedding_ids):\n",
        "        print(\"Warning: Final item count is less than prepared documents count.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\nERROR adding/updating documents in ChromaDB:\")\n",
        "    print(f\"Error details: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2a89cd",
      "metadata": {
        "papermill": {
          "duration": 0.012113,
          "end_time": "2025-04-21T00:52:04.049928",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.037815",
          "status": "completed"
        },
        "tags": [],
        "id": "aa2a89cd"
      },
      "source": [
        "## 6. Retrieve Relevant CV Items (RAG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f972f9",
      "metadata": {
        "papermill": {
          "duration": 0.222556,
          "end_time": "2025-04-21T00:52:04.284752",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.062196",
          "status": "completed"
        },
        "tags": [],
        "id": "d6f972f9"
      },
      "outputs": [],
      "source": [
        "# Perform RAG query using the job description\n",
        "\n",
        "retrieved_ids = []\n",
        "retrieved_documents = []\n",
        "retrieved_metadatas = []\n",
        "retrieved_distances = []\n",
        "\n",
        "try:\n",
        "    # Basic validation of inputs from previous steps\n",
        "    if not JOB_DESCRIPTION:\n",
        "        raise ValueError(\"JOB_DESCRIPTION variable is empty.\")\n",
        "\n",
        "    print(f\"Performing RAG query on collection '{cv_collection.name}'...\")\n",
        "\n",
        "    # Instantiate embedder for the 'retrieval_query' task type\n",
        "    query_embedder = GeminiEmbeddingFunction(document_mode=False)\n",
        "\n",
        "    print(\"Embedding job description (query)...\")\n",
        "    query_embedding = query_embedder([JOB_DESCRIPTION])[0]\n",
        "\n",
        "    num_results_to_fetch = MAX_RELEVANT_ITEMS + RETRIEVAL_WINDOW\n",
        "    print(f\"Querying for {num_results_to_fetch} items...\")\n",
        "\n",
        "    results = cv_collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=num_results_to_fetch,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
        "    )\n",
        "\n",
        "    # Safely extract results\n",
        "    retrieved_ids = results.get(\"ids\", [[]])[0]\n",
        "    retrieved_documents = results.get(\"documents\", [[]])[0]\n",
        "    retrieved_metadatas = results.get(\"metadatas\", [[]])[0]\n",
        "    retrieved_distances = results.get(\"distances\", [[]])[0]\n",
        "\n",
        "    print(f\"RAG retrieval complete. Found {len(retrieved_ids)} items.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR during RAG retrieval: {type(e).__name__} - {e}\")\n",
        "    # Ensure lists are reset on error\n",
        "    retrieved_ids, retrieved_documents, retrieved_metadatas, retrieved_distances = (\n",
        "        [],\n",
        "        [],\n",
        "        [],\n",
        "        [],\n",
        "    )\n",
        "\n",
        "if not retrieved_ids:\n",
        "    print(\"\\nWarning: No relevant items were retrieved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72600ccd",
      "metadata": {
        "papermill": {
          "duration": 0.012277,
          "end_time": "2025-04-21T00:52:04.310098",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.297821",
          "status": "completed"
        },
        "tags": [],
        "id": "72600ccd"
      },
      "source": [
        "## 7. Rewrite CV Items using LLM Chat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce240ad7",
      "metadata": {
        "papermill": {
          "duration": 0.011964,
          "end_time": "2025-04-21T00:52:04.334498",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.322534",
          "status": "completed"
        },
        "tags": [],
        "id": "ce240ad7"
      },
      "source": [
        "### 7.1. Define Prompts for Rewriting (EN & ES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e9b3bad",
      "metadata": {
        "papermill": {
          "duration": 0.019272,
          "end_time": "2025-04-21T00:52:04.366019",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.346747",
          "status": "completed"
        },
        "tags": [],
        "id": "1e9b3bad"
      },
      "outputs": [],
      "source": [
        "# Structure to hold multi-language prompt components\n",
        "content = {\"en\": {}, \"es\": {}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d2cdbf",
      "metadata": {
        "papermill": {
          "duration": 0.019359,
          "end_time": "2025-04-21T00:52:04.430484",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.411125",
          "status": "completed"
        },
        "tags": [],
        "id": "14d2cdbf"
      },
      "outputs": [],
      "source": [
        "# Example format for the LLM to follow when presenting a modified experience\n",
        "content[\"en\"][\"example\"] = \"\"\"\n",
        "## Position Name / Title: [JOB TITLE]\n",
        "- Company Name / Business Name: [COMPANY/ORG NAME]\n",
        "- Industry type: [INDUSTRY]\n",
        "- Job Field: [FIELD]\n",
        "- Sub-Area of Work: [SUB-AREA]\n",
        "\n",
        "### Original Description\n",
        "\n",
        "[ORIGINAL DESCRIPTION TEXT]\n",
        "\n",
        "### Modified description\n",
        "\n",
        "[SHORT SUMMARY/LEAD-IN]\n",
        "- [ATS-Optimized achievement/responsibility 1 incorporating keywords]\n",
        "- [ATS-Optimized achievement/responsibility 2 incorporating keywords]\n",
        "- ...\n",
        "\n",
        "### Changes made\n",
        "\n",
        "- Keywords used: [KEYWORD 1], [KEYWORD 2], ...\n",
        "- Explanation of changes: [Brief summary of additions/removals/focus shifts].\n",
        "\n",
        "\n",
        "Shall we continue [Y/n]?\n",
        "\"\"\"\n",
        "\n",
        "content[\"es\"][\"example\"] = \"\"\"\n",
        "## Nombre del puesto / Título: [TÍTULO PUESTO]\n",
        "- Nombre de empresa / Negocio: [NOMBRE EMPRESA/ORG]\n",
        "- Tipo de industria: [INDUSTRIA]\n",
        "- Área de trabajo: [ÁREA]\n",
        "- Subárea de trabajo: [SUB-ÁREA]\n",
        "\n",
        "### Descripción original\n",
        "\n",
        "[TEXTO DESCRIPCIÓN ORIGINAL]\n",
        "\n",
        "### Descripción modificada\n",
        "\n",
        "[RESUMEN CORTO/INTRODUCCIÓN]\n",
        "- [Logro/responsabilidad optimizado para ATS 1 incorporando palabras clave]\n",
        "- [Logro/responsabilidad optimizado para ATS 2 incorporando palabras clave]\n",
        "- ...\n",
        "\n",
        "### Changes made\n",
        "\n",
        "- Palabras clave utilizadas: [PALABRA CLAVE 1], [PALABRA CLAVE 2], ...\n",
        "- Explicación de los cambios: [Resumen breve de adiciones/eliminaciones/reenfoques].\n",
        "\n",
        "\n",
        "¿Continuamos? [Y/n]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61b545f0",
      "metadata": {
        "papermill": {
          "duration": 0.022492,
          "end_time": "2025-04-21T00:52:04.465413",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.442921",
          "status": "completed"
        },
        "tags": [],
        "id": "61b545f0"
      },
      "outputs": [],
      "source": [
        "# Main prompt defining the LLM's role and the interactive workflow.\n",
        "# Placeholders {n_max_exp}, {example}, {description}, {experiences} will be formatted later.\n",
        "\n",
        "content[\"en\"][\"prompt\"] = r\"\"\"\n",
        "You are an expert CV writer specialized in optimizing resumes for Applicant Tracking Systems (ATS) and identifying keywords from job descriptions.\n",
        "Analyze the provided job description and the list of retrieved CV experiences below. Your goal is to refine the experiences to align strongly with the job description keywords, making them ATS-compatible.\n",
        "\n",
        "**Output Requirements:**\n",
        "- ATS-friendly language.\n",
        "- Concise and impactful wording.\n",
        "- Focus on achievements and quantifiable results where possible.\n",
        "- Use active voice (e.g., Developed, Managed, Led).\n",
        "- Adhere strictly to the requested formats. Avoid extra conversational text.\n",
        "\n",
        "**Initial Analysis Task:**\n",
        "1. Identify and list the most critical keywords from the **JOB DESCRIPTION**.\n",
        "2. List the top {n_max_exp} most relevant **HIGHLIGHTED EXPERIENCES** retrieved, ordered by relevance (most relevant first). Include start/end dates (if available), title/position, and company/organization name.\n",
        "3. List any remaining **NON-SELECTED EXPERIENCES** retrieved similarly.\n",
        "\n",
        "**Initial Analysis Output Format:**\n",
        "**KEYWORDS:** [KEYWORD 1], [KEYWORD 2], ...\n",
        "**HIGHLIGHTED EXPERIENCES:**\n",
        "- ([START DATE] - [END DATE]) [JOB TITLE/PROJECT NAME 1], [COMPANY/ORG 1]\n",
        "- ... (up to {n_max_exp} items) ...\n",
        "**NON-SELECTED EXPERIENCES:**\n",
        "- ([START DATE] - [END DATE]) [JOB TITLE/PROJECT NAME X], [COMPANY/ORG X]\n",
        "- ... (remaining items) ...\n",
        "\n",
        "**Interaction Flow:**\n",
        "After presenting the initial analysis, ASK THE FOLLOWING QUESTION EXACTLY:\n",
        "\"Do you possess skills relevant to the keywords? Should any specific HIGHLIGHTED experience be modified?\"\n",
        "\n",
        "WAIT for my response. I will tell you which keywords are most relevant and which specific HIGHLIGHTED experience (by its title/position) I want to modify first.\n",
        "\n",
        "**Modification Task (Perform ONLY AFTER I select an experience):**\n",
        "When I ask you to modify a specific experience:\n",
        "1. Focus ONLY on the single experience I selected.\n",
        "2. Rewrite its description/highlights to incorporate the relevant **KEYWORDS** identified earlier.\n",
        "3. Make the description achievement-oriented and ATS-friendly.\n",
        "4. Present the modified experience using the **EXACT** format shown in the example below. Include the original description text for comparison. Mention the keywords used and explain the changes made.\n",
        "\n",
        "**Modification Output Format Example:** {example}\n",
        "\n",
        "**IMPORTANT:** Modify and present ONLY ONE experience at a time, based on my selection. After presenting a modified experience and asking \"Shall we continue [Y/n]?\", WAIT for my confirmation ('Y' or 'y') before proceeding to ask which *next* highlighted experience I want to modify, OR wait for my feedback/request for further changes on the *current* one. If I enter 'n' or 'q', stop the process.\n",
        "\n",
        "**Context:**\n",
        "JOB DESCRIPTION: \"{description}\"\n",
        "RETRIEVED CV EXPERIENCES:\n",
        "{experiences}\n",
        "\n",
        "Start by performing the **Initial Analysis Task**.\n",
        "\"\"\"\n",
        "\n",
        "content[\"es\"][\"prompt\"] = r\"\"\"\n",
        "Eres un experto redactor de CVs, especialista en optimizar para Applicant Tracking Systems (ATS) e identificar palabras clave en descripciones de trabajo.\n",
        "Analiza la descripción del puesto y la lista de experiencias recuperadas del CV que se proporcionan a continuación. Tu objetivo es refinar las experiencias para alinearlas fuertemente con las palabras clave de la descripción del puesto, haciéndolas compatibles con ATS.\n",
        "\n",
        "**Requisitos de Salida:**\n",
        "- Lenguaje amigable para ATS.\n",
        "- Redacción concisa e impactante.\n",
        "- Enfoque en logros y resultados cuantificables siempre que sea posible.\n",
        "- Usar voz activa (ej., Desarrollé, Gestioné, Lideré).\n",
        "- Adherirse estrictamente a los formatos solicitados. Evitar texto conversacional extra.\n",
        "\n",
        "**Tarea de Análisis Inicial:**\n",
        "1. Identifica y lista las palabras clave más críticas de la **DESCRIPCIÓN DE LA OFERTA**.\n",
        "2. Lista las {n_max_exp} **EXPERIENCIAS DESTACADAS** más relevantes recuperadas, ordenadas por relevance (la más relevante primero). Incluye fechas de inicio/fin (si están disponibles), título/puesto y nombre de la empresa/organización.\n",
        "3. Lista cualquier **EXPERIENCIA NO SELECCIONADA** restante recuperada de manera similar.\n",
        "\n",
        "**Formato de Salida del Análisis Inicial:**\n",
        "**PALABRAS CLAVES:** [PALABRA CLAVE 1], [PALABRA CLAVE 2], ...\n",
        "**EXPERIENCIAS DESTACADAS:**\n",
        "- ([FECHA INICIO] - [FECHA FIN]) [TÍTULO PUESTO/PROYECTO 1], [EMPRESA/ORG 1]\n",
        "- ... (hasta {n_max_exp} ítems) ...\n",
        "**EXPERIENCIAS NO SELECCIONADAS:**\n",
        "- ([FECHA INICIO] - [FECHA FIN]) [TÍTULO PUESTO/PROYECTO X], [EMPRESA/ORG X]\n",
        "- ... (ítems restantes) ...\n",
        "\n",
        "**Flujo de Interacción:**\n",
        "Después de presentar el análisis inicial, HAZ LA SIGUIENTE PREGUNTA EXACTAMENTE:\n",
        "\"¿Posees habilidades relevantes para las palabras claves? ¿Debería modificarse alguna EXPERIENCIA DESTACADA específica?\"\n",
        "\n",
        "ESPERA mi respuesta. Te indicaré qué palabras clave son más relevantes y qué EXPERIENCIA DESTACADA específica (por su título/puesto) quiero modificar primero.\n",
        "\n",
        "**Tarea de Modificación (Realizar SÓLO DESPUÉS de que yo seleccione una experiencia):**\n",
        "Cuando te pida modificar una experiencia específica:\n",
        "1. Enfócate SÓLO en la única experiencia que seleccioné.\n",
        "2. Reescribe su descripción/logros para incorporar las **PALABRAS CLAVES** relevantes identificadas anteriormente.\n",
        "3. Haz la descripción orientada a logros y amigable para ATS.\n",
        "4. Presenta la experiencia modificada usando el formato **EXACTO** mostrado en el ejemplo a continuación. Incluye el texto de la descripción original para comparación. Menciona las palabras clave usadas y explica los cambios realizados.\n",
        "\n",
        "**Ejemplo de Formato de Salida de Modificación:** {example}\n",
        "\n",
        "**IMPORTANTE:** Modifica y presenta SÓLO UNA experiencia a la vez, según mi selección. Después de presentar una experiencia modificada y preguntar \"¿Continuamos? [Y/n]?\", ESPERA mi confirmación ('Y', 'y' o Enter) antes de preguntar qué *siguiente* experiencia destacada quiero modificar, O espera mi feedback/solicitud de más cambios en la *actual*. Si ingreso 'n' o 'q', detén el proceso.\n",
        "\n",
        "**Contexto:**\n",
        "DESCRIPCIÓN DE LA OFERTA: \"{description}\"\n",
        "EXPERIENCIAS DEL CV RECUPERADAS:\n",
        "{experiences}\n",
        "\n",
        "Comienza realizando la **Tarea de Análisis Inicial**.\n",
        "\"\"\"\n",
        "\n",
        "# Note: The placeholders {n_max_exp}, {example}, {description}, {experiences}\n",
        "# will be filled using .format() before sending the prompt to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d487eddd",
      "metadata": {
        "papermill": {
          "duration": 0.01971,
          "end_time": "2025-04-21T00:52:04.398729",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.379019",
          "status": "completed"
        },
        "tags": [],
        "id": "d487eddd"
      },
      "outputs": [],
      "source": [
        "# Messages for user interaction\n",
        "\n",
        "# --- quit message ---\n",
        "content[\"en\"][\"quit_msg\"] = \"To exit, enter 'q' or 'quit'\"\n",
        "content[\"es\"][\"quit_msg\"] = \"Para salir, ingresa 'q' o 'salir'\"\n",
        "\n",
        "# --- initial response ---\n",
        "content[\"en\"][\"initial_response\"] = \"LLM Initial Analysis Response:\"\n",
        "content[\"es\"][\"initial_response\"] = \"Respuesta de Análisis Inicial del LLM:\"\n",
        "\n",
        "# --- exit message ---\n",
        "content[\"en\"][\"exit_msg\"] = \"\\nExiting interactive loop.\"\n",
        "content[\"es\"][\"exit_msg\"] = \"\\nSaliendo del bucle interactivo.\"\n",
        "\n",
        "# --- y interpretation ---\n",
        "content[\"en\"][\n",
        "    \"y_interpretation\"\n",
        "] = \"(Interpreting empty input as 'Y' to continue/confirm)\"\n",
        "content[\"es\"][\n",
        "    \"y_interpretation\"\n",
        "] = \"(Interpretando entrada vacía como 'Y' para continuar/confirmar)\"\n",
        "\n",
        "# --- send message ---\n",
        "content[\"en\"][\"send_message\"] = \"\\nSending message to LLM...\"\n",
        "content[\"es\"][\"send_message\"] = \"\\nEnviando mensaje al LLM...\"\n",
        "\n",
        "# --- received message ---\n",
        "content[\"en\"][\"received_message\"] = \"LLM response received:\"\n",
        "content[\"es\"][\"received_message\"] = \"Respuesta del LLM recibida:\"\n",
        "\n",
        "# --- Keyboard Interrupt ---\n",
        "content[\"en\"][\"keyboard_interrupt\"] = \"\\nLoop interrupted by user (Ctrl+C). Exiting.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569b214b",
      "metadata": {
        "papermill": {
          "duration": 0.01191,
          "end_time": "2025-04-21T00:52:04.489745",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.477835",
          "status": "completed"
        },
        "tags": [],
        "id": "569b214b"
      },
      "source": [
        "### 7.2. Initiate Chat and Generate Initial Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21704d8c",
      "metadata": {
        "papermill": {
          "duration": 1.547416,
          "end_time": "2025-04-21T00:52:06.049288",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.501872",
          "status": "completed"
        },
        "tags": [],
        "id": "21704d8c"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "# --- 1. Format Retrieved Experiences for Prompt ---\n",
        "formatted_experiences = []\n",
        "if \"retrieved_documents\" in locals() and retrieved_documents:\n",
        "    for i, doc in enumerate(retrieved_documents):\n",
        "        meta = retrieved_metadatas[i] if i < len(retrieved_metadatas) else {}\n",
        "        section = meta.get(\"section\", \"Unknown Section\")\n",
        "        name = meta.get(\"id_name\", \"\")\n",
        "        position = meta.get(\"id_position\", \"\")\n",
        "        header = f\"{name}{(' - ' + position) if position else ''} ({section})\"\n",
        "        formatted_experiences.append(f\"--- Experience {i+1}: {header} ---\\n{doc}\")\n",
        "    experiences_context_string = \"\\n\\n\".join(formatted_experiences)\n",
        "else:\n",
        "    experiences_context_string = (\n",
        "        \"No relevant experiences were retrieved from the database.\"\n",
        "    )\n",
        "    print(\"Warning: No retrieved experiences to include in the prompt.\")\n",
        "\n",
        "# --- 2. Select Language and Format Final Prompt ---\n",
        "content_lang = content[LANGUAGE]  # Select EN or ES content dict\n",
        "initial_prompt = content_lang[\"prompt\"].format(\n",
        "    n_max_exp=MAX_RELEVANT_ITEMS,\n",
        "    example=content_lang[\"example\"],\n",
        "    description=JOB_DESCRIPTION,\n",
        "    experiences=experiences_context_string,\n",
        ")\n",
        "\n",
        "# --- 3. Initialize LLM and Chat for Rewriting ---\n",
        "chat = None  # Initialize chat variable\n",
        "try:\n",
        "    rewriting_model = GENERATIVE_MODEL_NAME\n",
        "\n",
        "    rewriting_generation_config = {\n",
        "        \"temperature\": REWRITING_LLM_TEMPERATURE,\n",
        "        \"top_p\": REWRITING_LLM_TOP_P,\n",
        "        \"top_k\": REWRITING_LLM_TOP_K,\n",
        "    }\n",
        "\n",
        "    # Start a new chat session (history is empty)\n",
        "    chat = client.chats.create(\n",
        "        model=GENERATIVE_MODEL_NAME, history=[], config=rewriting_generation_config\n",
        "    )\n",
        "    print(f\"Chat session initialized with model '{GENERATIVE_MODEL_NAME}'.\")\n",
        "\n",
        "    # --- 4. Send Initial Prompt and Get Response ---\n",
        "    print(\"Sending initial prompt to LLM for analysis...\")\n",
        "    # Send the formatted prompt, applying the specific generation config\n",
        "    initial_response = chat.send_message(initial_prompt)\n",
        "\n",
        "\n",
        "except NameError as e:\n",
        "    # Catch errors if genai or types wasn't imported/available\n",
        "    print(\n",
        "        \"ERROR: Required object not defined (e.g., 'genai', 'types'). Check cell\"\n",
        "        \" execution order.\"\n",
        "    )\n",
        "    print(f\"Error: {e}\")\n",
        "    chat = None\n",
        "except Exception as e:\n",
        "    print(\"\\nERROR initializing chat or sending initial message:\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    chat = None\n",
        "\n",
        "# Ensure variables exist for the next step\n",
        "if \"initial_response\" not in locals():\n",
        "    initial_response = None\n",
        "\n",
        "# Verification print moved to focus only on successful execution of this cell's task\n",
        "if chat and initial_response:\n",
        "    print(\"Initial LLM response obtained successfully.\")\n",
        "else:\n",
        "    print(\"Failed to obtain initial LLM response or initialize chat.\")\n",
        "    raise RuntimeError(\"Chat session or initial response is not valid.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "007e4586",
      "metadata": {
        "papermill": {
          "duration": 0.014474,
          "end_time": "2025-04-21T00:52:06.083467",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.068993",
          "status": "completed"
        },
        "tags": [],
        "id": "007e4586"
      },
      "source": [
        "### 7.3. Interactive Refinement Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5295a2",
      "metadata": {
        "papermill": {
          "duration": 0.01203,
          "end_time": "2025-04-21T00:52:06.107748",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.095718",
          "status": "completed"
        },
        "tags": [],
        "id": "0f5295a2"
      },
      "source": [
        "The following cell starts the interactive loop. First, it displays the initial analysis received from the LLM in the previous step. Then, it repeatedly prompts you for input.\n",
        "\n",
        "Based on the LLM's analysis and questions, provide your feedback or select an experience to modify. Your input will be sent back to the LLM, and its response will be displayed.\n",
        "\n",
        "Enter 'q' or 'quit' (or the Spanish equivalent if configured) to exit the loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2369a960",
      "metadata": {
        "papermill": {
          "duration": 0.027388,
          "end_time": "2025-04-21T00:52:06.148976",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.121588",
          "status": "completed"
        },
        "tags": [],
        "id": "2369a960"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "content_lang = content[LANGUAGE]  # Select EN or ES content dict\n",
        "\n",
        "# --- 1. Display Initial LLM Analysis ---\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(content_lang[\"initial_response\"])\n",
        "print(\"=\" * 80)\n",
        "display(Markdown(initial_response.text))\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# --- 2. Interactive Refinement Loop ---\n",
        "quit_msg = content_lang.get(\"quit_msg\", \"Enter 'q' or 'quit' to exit\")\n",
        "# Define various inputs that signal exiting the loop (incl. 'n' for No)\n",
        "quit_options = {\"q\", \"quit\", \"exit\", \"salir\", \"n\"}\n",
        "\n",
        "while True:\n",
        "    # Display quit instructions clearly\n",
        "    print(\"\\n\" + f\" {quit_msg} \".center(80, \"-\"))\n",
        "    try:\n",
        "        # Prompt user for input\n",
        "        user_input = input(\"> \")\n",
        "\n",
        "        # Check if the user wants to exit\n",
        "        if user_input.lower() in quit_options:\n",
        "            print(content_lang[\"exit_msg\"])\n",
        "            break\n",
        "\n",
        "        # Assume 'Yes' (continue/approve) if user just presses Enter\n",
        "        if not user_input:\n",
        "            user_input = \"Y\"\n",
        "            print(content_lang[\"y_interpretation\"])\n",
        "\n",
        "        # Send the user's message to the LLM via the chat session\n",
        "        print(content_lang[\"send_message\"])\n",
        "        llm_response = chat.send_message(user_input)\n",
        "        print(content_lang[\"received_message\"])\n",
        "\n",
        "        # Display the LLM's response using Markdown rendering\n",
        "        display(Markdown(llm_response.text))\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(content_lang[\"keyboard_interrupt\"])\n",
        "        break\n",
        "    except Exception as e:\n",
        "        # Catch potential errors during chat interaction\n",
        "        print(\"\\nERROR during interactive chat:\")\n",
        "        print(f\"Error Type: {type(e).__name__}\")\n",
        "        print(f\"Error details: {e}\")\n",
        "        print(\"Exiting interactive loop due to error.\")\n",
        "        break  # Exit loop on error"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85134df0",
      "metadata": {
        "papermill": {
          "duration": 0.013142,
          "end_time": "2025-04-21T00:52:06.176952",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.163810",
          "status": "completed"
        },
        "tags": [],
        "id": "85134df0"
      },
      "source": [
        "## 8. Conclusion & Take-aways"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a74085",
      "metadata": {
        "papermill": {
          "duration": 0.013431,
          "end_time": "2025-04-21T00:52:06.203837",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.190406",
          "status": "completed"
        },
        "tags": [],
        "id": "c0a74085"
      },
      "source": [
        "This notebook demonstrated a complete workflow for optimizing CV sections against a job description using RAG and Generative AI:\n",
        "\n",
        "* **CV Structuring:** Successfully parsed a PDF CV into a structured JSON format using an LLM (Gemini).\n",
        "* **Vector Search (RAG):** Created embeddings for CV items and utilized ChromaDB to retrieve the sections most semantically relevant to a specific job description.\n",
        "* **LLM-Powered Rewriting:** Employed a Gemini chat model in an interactive loop to refine the retrieved CV sections, incorporating keywords and optimizing for ATS.\n",
        "* **Self-Contained Workflow:** The entire process, from PDF loading to interactive rewriting, is contained within this single Kaggle notebook.\n",
        "\n",
        "This approach allows for targeted CV customization, potentially improving application success rates by better aligning the CV with job requirements and ATS filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "820da19b",
      "metadata": {
        "papermill": {
          "duration": 0.015175,
          "end_time": "2025-04-21T00:52:06.232303",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.217128",
          "status": "completed"
        },
        "tags": [],
        "id": "820da19b"
      },
      "source": [
        "## 9. Final Remarks / Contact"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "621f5649",
      "metadata": {
        "papermill": {
          "duration": 0.012673,
          "end_time": "2025-04-21T00:52:06.257754",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.245081",
          "status": "completed"
        },
        "tags": [],
        "id": "621f5649"
      },
      "source": [
        "🌟 CONGRATULATIONS 🌟\n",
        "\n",
        "If you’ve made it all the way to the end of this notebook (whether you’re an evaluator, a fellow data‑enthusiast, or just curious about the process), you definitely deserve a prize!\n",
        "\n",
        "Our project is evolving every week, and your feedback keeps it alive.\n",
        "👉 Show some love by giving the repo a star on GitHub:\n",
        "https://github.com/framunoz/cv-analyser-with-rag/\n",
        "\n",
        "A single ⭐ helps the framework grow, motivates new features, and lets us know this work is valuable to the community.\n",
        "\n",
        "Thanks for reading, building, and experimenting alongside us—see you in the next commit!"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 97258,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 70.308237,
      "end_time": "2025-04-21T00:52:07.595838",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-04-21T00:50:57.287601",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}