{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/framunoz/cv-analyser-with-rag/blob/feature%2Fkaggle-colab-notebooks/notebooks/colab-ai-driven-cv-optimisation-with-rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87cbd2aa",
      "metadata": {
        "papermill": {
          "duration": 0.010245,
          "end_time": "2025-04-21T00:51:01.848333",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.838088",
          "status": "completed"
        },
        "tags": [],
        "id": "87cbd2aa"
      },
      "source": [
        "# üìù Optimize Your CV with AI to Beat the ATS Filters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad24d49",
      "metadata": {
        "papermill": {
          "duration": 0.00828,
          "end_time": "2025-04-21T00:51:01.865453",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.857173",
          "status": "completed"
        },
        "tags": [],
        "id": "3ad24d49"
      },
      "source": [
        "## 1. Welcome! What Does This Notebook Do?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fe8e3b",
      "metadata": {
        "papermill": {
          "duration": 0.008816,
          "end_time": "2025-04-21T00:51:01.883005",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.874189",
          "status": "completed"
        },
        "tags": [],
        "id": "61fe8e3b"
      },
      "source": [
        "**Applying for jobs often feels like sending your CV into a black hole, right?** Many companies use Applicant Tracking Systems (ATS) ‚Äì automated software that scans your CV for keywords and relevant experience *before* a human ever sees it. If your CV doesn't match what the ATS is looking for, it might get filtered out, even if you're a great fit for the role!\n",
        "\n",
        "**This notebook is your personal AI assistant to help you get noticed.** It guides you through tailoring parts of your CV to better match a specific job description, increasing your chances of passing the ATS scan and impressing the hiring manager.\n",
        "\n",
        "**Here's how it works in simple terms:**\n",
        "\n",
        "1.  **You Provide:** You'll give the notebook your CV (as a PDF or JSON file in Google Drive) and the text of the job description you're interested in.\n",
        "2.  **AI Reads & Understands:** The AI reads your CV to understand your skills and experience. It creates a special \"smart index\" that captures the *meaning* of your experience, not just the words.\n",
        "3.  **AI Finds Matches:** Using the job description, the AI searches its \"smart index\" to find the parts of your CV (like specific jobs or projects) that are the **most relevant** matches for *that specific role*.\n",
        "4.  **AI Helps Rewrite:** You'll chat with the AI assistant. It will suggest ways to rewrite the descriptions of your most relevant experiences, helping you incorporate important keywords from the job description naturally. This makes your CV more ATS-friendly and impactful.\n",
        "5.  **You Review & Use:** You review the AI's suggestions and copy the improved text snippets into your actual CV document.\n",
        "\n",
        "**Who is this for?**\n",
        "\n",
        "Anyone applying for jobs who wants a smarter, faster way to customize their CV for each application. **No programming or AI knowledge is needed!** Just follow the steps.\n",
        "\n",
        "**Ready to get started?** We'll guide you through each step below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dc57789",
      "metadata": {
        "papermill": {
          "duration": 0.008258,
          "end_time": "2025-04-21T00:51:01.899782",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.891524",
          "status": "completed"
        },
        "tags": [],
        "id": "2dc57789"
      },
      "source": [
        "## 2. Setup: Getting Ready"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3945b6c5",
      "metadata": {
        "papermill": {
          "duration": 0.008222,
          "end_time": "2025-04-21T00:51:01.916465",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.908243",
          "status": "completed"
        },
        "tags": [],
        "id": "3945b6c5"
      },
      "source": [
        "This section gets the notebook ready to work. We'll install some necessary software tools and then guide you on how to securely connect the notebook to Google's AI services using your own unique key (called an API key)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "252b77ab",
      "metadata": {
        "papermill": {
          "duration": 0.00801,
          "end_time": "2025-04-21T00:51:01.933143",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.925133",
          "status": "completed"
        },
        "tags": [],
        "id": "252b77ab"
      },
      "source": [
        "### 2.1. Install Necessary Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next code cell installs some extra software packages that this notebook needs to do its job. These tools help it read PDF files, connect to Google's AI services, manage the \"smart index\" of your CV, and handle files.\n",
        "\n",
        "**‚û°Ô∏è Just click the Play button (‚ñ∫) on the cell below to run it.** It might take a minute or two to finish installing everything. You'll see some messages scroll by, and it's done when you see a \"‚úÖ Libraries installed successfully!\" message."
      ],
      "metadata": {
        "id": "kQoFevzEs-5u"
      },
      "id": "kQoFevzEs-5u"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "a9240366",
      "metadata": {
        "papermill": {
          "duration": 46.074818,
          "end_time": "2025-04-21T00:51:48.016501",
          "exception": false,
          "start_time": "2025-04-21T00:51:01.941683",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "a9240366",
        "outputId": "1d8bd0f7-244c-449d-e6d4-1f111f1c937c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping jupyterlab as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m‚úÖ Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "#@title Install Required Libraries (Click the ‚ñ∫ button to run)\n",
        "# This command installs the necessary Python packages for the notebook.\n",
        "# It might take a minute or two to complete.\n",
        "# The '-q' flag makes the output less noisy (quieter).\n",
        "# ruff: noqa: T201, F811\n",
        "\n",
        "# Uninstall conflicting library (optional step from original notebook, may help in Colab)\n",
        "!pip uninstall -qqy jupyterlab\n",
        "\n",
        "# Install Google AI, PDF reader, Vector DB, GDrive downloader (if used), YAML handler\n",
        "# We install needed versions of google-genai and typing_extensions for compatibility.\n",
        "!pip install -q google-genai pdfplumber chromadb gdown PyYAML typing-extensions==4.12.0\n",
        "\n",
        "print(\"‚úÖ Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64076413",
      "metadata": {
        "papermill": {
          "duration": 0.010587,
          "end_time": "2025-04-21T00:51:48.038561",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.027974",
          "status": "completed"
        },
        "tags": [],
        "id": "64076413"
      },
      "source": [
        "### 2.2. Add Your Secure API Key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a172b04",
      "metadata": {
        "papermill": {
          "duration": 0.010462,
          "end_time": "2025-04-21T00:51:48.059579",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.049117",
          "status": "completed"
        },
        "tags": [],
        "id": "4a172b04"
      },
      "source": [
        "To use Google's powerful AI (like the Gemini model that powers this notebook), you need a personal \"API Key\". Think of it like a secure password that gives this notebook permission to use the AI service on your behalf. It's important to keep this key private.\n",
        "\n",
        "Google Colab provides a secure way to store this key using the **Secrets Manager**.\n",
        "\n",
        "**‚û°Ô∏è How to Add Your Key (Choose ONE method):**\n",
        "\n",
        "Look for the **üîë (key) icon** in the left sidebar of Colab. Click it to open the Secrets panel.\n",
        "\n",
        "* **Method 1: Import from Google AI Studio (Recommended if you already have a key there)**\n",
        "    1.  In the Secrets panel (üîë), find the dropdown menu labeled **\"Gemini API keys\"**.\n",
        "    2.  Select **\"Import key from Google AI Studio\"**.\n",
        "    3.  Follow the on-screen prompts to sign in and import your key. Colab should automatically name it `GOOGLE_API_KEY`. Make sure the **\"Notebook access\" toggle is turned ON** (‚òëÔ∏è) for this secret.\n",
        "\n",
        "* **Method 2: Add Manually**\n",
        "    1.  If you don't have a key in AI Studio, you can get one from [Google AI Studio](https://aistudio.google.com/app/apikey) (or Google Cloud Console if you use that). Click \"Create API key\".\n",
        "    2.  In Colab's Secrets panel (üîë), click **\"+ Add new secret\"**.\n",
        "    3.  For the **Name**, enter **exactly** `GOOGLE_API_KEY`.\n",
        "    4.  Paste your generated API key into the **Value** field.\n",
        "    5.  **IMPORTANT:** Make sure the **\"Notebook access\" toggle is turned ON** (‚òëÔ∏è) for this secret.\n",
        "    6.  Close the Secrets panel.\n",
        "\n",
        "**After adding the secret using one of these methods, run the next code cell (click ‚ñ∫) to load it.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "1af7af19",
      "metadata": {
        "papermill": {
          "duration": 0.106451,
          "end_time": "2025-04-21T00:51:48.176706",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.070255",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "1af7af19",
        "outputId": "5b7c6f0b-b0d9-4648-a4a8-64e2acac2fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GOOGLE_API_KEY loaded successfully from Colab Secrets.\n"
          ]
        }
      ],
      "source": [
        "#@title Load Google AI API Key from Colab Secrets (Click ‚ñ∫ to run)\n",
        "from google.colab import userdata\n",
        "import sys # Needed to potentially stop execution\n",
        "\n",
        "# Simplified error messages for easier understanding\n",
        "_API_KEY_ERROR_MESSAGES = {\n",
        "    \"NOT_FOUND\": \"\"\"\n",
        "‚ùå **Secret 'GOOGLE_API_KEY' not found.**\n",
        "   Did you add the secret using the üîë icon in the left sidebar?\n",
        "   Make sure the name is exactly 'GOOGLE_API_KEY' and that 'Notebook access' is turned ON (‚òëÔ∏è).\n",
        "    \"\"\",\n",
        "    \"EMPTY\": \"\"\"\n",
        "‚ùå **Secret 'GOOGLE_API_KEY' was found but is empty.**\n",
        "   Please check the value you pasted into the secret using the üîë icon. It should not be blank.\n",
        "    \"\"\",\n",
        "    \"IMPORT_ERROR\": \"\"\"\n",
        "‚ùå **Could not access Colab Secrets.**\n",
        "   This notebook is designed for Google Colab. If you are in Colab, try restarting the runtime (Runtime -> Restart runtime) and run the cells again.\n",
        "    \"\"\",\n",
        "    \"UNKNOWN\": \"\"\"\n",
        "‚ùå **An unexpected error occurred while loading the API key.**\n",
        "   Details: {error}\n",
        "   You might need to check your Colab Secrets setup again.\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Attempt to retrieve the secret named 'GOOGLE_API_KEY'\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    if GOOGLE_API_KEY is None:\n",
        "        # Secret not found or access not enabled\n",
        "        print(_API_KEY_ERROR_MESSAGES[\"NOT_FOUND\"])\n",
        "        # Stop execution if key not found\n",
        "        sys.exit(\"Stopping: API Key is required.\")\n",
        "    elif not GOOGLE_API_KEY: # Checks for empty string\n",
        "        # Secret found but has no value\n",
        "        print(_API_KEY_ERROR_MESSAGES[\"EMPTY\"])\n",
        "        # Stop execution if key is empty\n",
        "        sys.exit(\"Stopping: API Key cannot be empty.\")\n",
        "    else:\n",
        "        # Key loaded successfully\n",
        "        print(\"‚úÖ GOOGLE_API_KEY loaded successfully from Colab Secrets.\")\n",
        "        # Optional: Uncomment below to verify the first few chars (for debugging)\n",
        "        # print(f\"(Key starts with: {GOOGLE_API_KEY[:4]}...)\")\n",
        "\n",
        "except ImportError:\n",
        "    # Handle case where userdata is not available (running outside Colab)\n",
        "    print(_API_KEY_ERROR_MESSAGES[\"IMPORT_ERROR\"])\n",
        "    GOOGLE_API_KEY = None\n",
        "    sys.exit(\"Stopping: Notebook must run in Colab to use Secrets.\")\n",
        "except Exception as e:\n",
        "    # Handle any other unexpected errors during secret retrieval\n",
        "    print(_API_KEY_ERROR_MESSAGES[\"UNKNOWN\"].format(error=e))\n",
        "    GOOGLE_API_KEY = None\n",
        "    sys.exit(f\"Stopping due to error loading API Key: {e}\")\n",
        "\n",
        "# If the code reaches here without exiting, the key is loaded and valid (not None or empty)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bcca856",
      "metadata": {
        "papermill": {
          "duration": 0.011349,
          "end_time": "2025-04-21T00:51:48.199290",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.187941",
          "status": "completed"
        },
        "tags": [],
        "id": "1bcca856"
      },
      "source": [
        "### 2.3. Helper Functions (Retry Logic)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72cb31ee",
      "metadata": {
        "papermill": {
          "duration": 0.010982,
          "end_time": "2025-04-21T00:51:48.221562",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.210580",
          "status": "completed"
        },
        "tags": [],
        "id": "72cb31ee"
      },
      "source": [
        "Sometimes, when contacting the Google AI service, it might be temporarily busy or unavailable (like getting a busy signal on a phone line). To handle this smoothly, the next code cell defines a small helper function.\n",
        "\n",
        "This helper allows the notebook to automatically try contacting the service again a few times if it encounters these specific temporary issues. This makes the whole process more reliable without you having to manually re-run things for minor glitches. You don't need to worry about the details of the code itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "b0de59f4",
      "metadata": {
        "papermill": {
          "duration": 1.43616,
          "end_time": "2025-04-21T00:51:49.668831",
          "exception": false,
          "start_time": "2025-04-21T00:51:48.232671",
          "status": "completed"
        },
        "tags": [],
        "cellView": "form",
        "id": "b0de59f4"
      },
      "outputs": [],
      "source": [
        "#@title Define Helper Function for API Retries (click to expand)\n",
        "from google import genai\n",
        "from google.api_core import retry\n",
        "\n",
        "def is_retriable(exception: Exception) -> bool:\n",
        "    \"\"\"Checks if an exception is a known temporary/retriable Google API Error.\"\"\"\n",
        "    return isinstance(exception, genai.errors.APIError) and exception.code in {429, 503}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba3bf860",
      "metadata": {
        "papermill": {
          "duration": 0.011781,
          "end_time": "2025-04-21T00:51:49.691529",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.679748",
          "status": "completed"
        },
        "tags": [],
        "id": "ba3bf860"
      },
      "source": [
        "## 3. Your Information & Job Details"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 3.1: Connect Google Drive & Specify Folder (Click ‚ñ∫)\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "import sys # To stop execution on error\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 1. Connect to Google Drive\n",
        "#@markdown Run this cell. It will ask for permission to connect to your Google Drive. Click \"Connect to Google Drive\", choose your account, and click \"Allow\".\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True) # force_remount helps if connection is stale\n",
        "    print(\"\\n‚úÖ Google Drive connected successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error mounting Google Drive: {e}\")\n",
        "    print(\"   Please ensure you allow access when prompted.\")\n",
        "    sys.exit(\"Stopping: Google Drive connection failed.\") # Stop execution\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### 2. Enter Folder Path & Options\n",
        "#@markdown Now, enter the full path to the folder **inside your Google Drive** where your CV file (`resume.pdf` or `resume.json`) is located.\n",
        "#@markdown **Example:** `/content/drive/MyDrive/Job Applications/My CV Folder`\n",
        "DRIVE_FOLDER_PATH = \"/content/drive/MyDrive/ia-driven-cv-opt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Check the box below ONLY if you want the notebook to re-process your `resume.pdf`, even if a `resume.json` file already exists in the folder (useful if you updated the PDF recently).\n",
        "FORCE_PDF_REPARSE = False #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- Path Validation ---\n",
        "if not DRIVE_FOLDER_PATH:\n",
        "    print(\"\\n‚ùå Error: Please enter the path to your Google Drive folder in the form above and run this cell again.\")\n",
        "    sys.exit(\"Stopping: Drive folder path is required.\")\n",
        "\n",
        "# Use pathlib for easier path handling\n",
        "_base_path = Path(DRIVE_FOLDER_PATH)\n",
        "\n",
        "if not _base_path.exists():\n",
        "    print(f\"\\n‚ùå Error: The specified folder path does not seem to exist: '{DRIVE_FOLDER_PATH}'\")\n",
        "    print(\"   Please double-check the path you entered. Make sure it starts with '/content/drive/' and that the folder exists in your Google Drive.\")\n",
        "    sys.exit(\"Stopping: Invalid Drive folder path.\")\n",
        "elif not _base_path.is_dir():\n",
        "     print(f\"\\n‚ùå Error: The specified path is not a folder: '{DRIVE_FOLDER_PATH}'\")\n",
        "     print(\"   Please ensure you provide the path to a folder, not directly to a file.\")\n",
        "     sys.exit(\"Stopping: Path is not a folder.\")\n",
        "else:\n",
        "    print(f\"\\n‚úÖ Folder path confirmed: '{_base_path}'\")\n",
        "    # Define derived paths for later use\n",
        "    CV_JSON_PATH = _base_path / \"resume.json\"\n",
        "    CV_PDF_PATH = _base_path / \"resume.pdf\"\n",
        "    # Define ChromaDB path within the user's specified folder\n",
        "    CHROMA_DB_PATH = _base_path / \"cv_chroma_db\"\n",
        "    print(f\"   Will look for CV JSON at: '{CV_JSON_PATH}'\")\n",
        "    print(f\"   Will look for CV PDF at: '{CV_PDF_PATH}'\")\n",
        "    print(f\"   Vector database will be stored in: '{CHROMA_DB_PATH}'\")\n",
        "\n",
        "print(\"\\nProcessing options:\")\n",
        "if FORCE_PDF_REPARSE:\n",
        "    print(\"   - Will attempt to process 'resume.pdf' (ignoring any existing 'resume.json').\")\n",
        "else:\n",
        "    print(\"   - Will first look for 'resume.json'. If not found, will look for 'resume.pdf'.\")\n",
        "\n",
        "# Ensure the directory for ChromaDB exists\n",
        "try:\n",
        "    CHROMA_DB_PATH.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"\\n‚úÖ Directory for vector database ensured at: '{CHROMA_DB_PATH}'\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error creating directory for vector database: {e}\")\n",
        "    sys.exit(\"Stopping: Could not create database directory.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qa4-zhGmVC6V",
        "outputId": "5948fd88-b1d1-4b0a-e981-2ecdf6bd39ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "qa4-zhGmVC6V",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "‚úÖ Google Drive connected successfully!\n",
            "\n",
            "‚úÖ Folder path confirmed: '/content/drive/MyDrive/ia-driven-cv-opt'\n",
            "   Will look for CV JSON at: '/content/drive/MyDrive/ia-driven-cv-opt/resume.json'\n",
            "   Will look for CV PDF at: '/content/drive/MyDrive/ia-driven-cv-opt/resume.pdf'\n",
            "   Vector database will be stored in: '/content/drive/MyDrive/ia-driven-cv-opt/cv_chroma_db'\n",
            "\n",
            "Processing options:\n",
            "   - Will first look for 'resume.json'. If not found, will look for 'resume.pdf'.\n",
            "\n",
            "‚úÖ Directory for vector database ensured at: '/content/drive/MyDrive/ia-driven-cv-opt/cv_chroma_db'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.2: Provide the Job Description"
      ],
      "metadata": {
        "id": "Po1ZA6KeWyua"
      },
      "id": "Po1ZA6KeWyua"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the text box in the **next cell**, please **delete the example text** and **paste the full job description** you are applying for.\n",
        "\n",
        "**‚û°Ô∏è IMPORTANT:** After you paste or edit the job description in the box below, **you MUST run that cell (click its ‚ñ∫ button)**. This saves the description so the notebook can use it for the analysis."
      ],
      "metadata": {
        "id": "i5kHY18lWsPm"
      },
      "id": "i5kHY18lWsPm"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Enter Job Description Here (Then Click ‚ñ∫)\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import sys # To potentially stop execution\n",
        "\n",
        "# Default Job Description Text (from original notebook [Source: 76-87])\n",
        "# This helps the user see the expected format and provides a starting point.\n",
        "default_job_desc = \"\"\"Job Title: AI/Machine Learning Engineer\n",
        "\n",
        "Company: Innovate Solutions Inc.\n",
        "\n",
        "Location: Remote (US Based)\n",
        "\n",
        "About Us:\n",
        "Innovate Solutions Inc. is at the forefront of applying artificial intelligence to solve real-world business challenges. We foster a collaborative environment where creative thinking and technical excellence drive our success. We are passionate about building intelligent systems that deliver significant value to our clients across various industries. Join our growing team and help shape the future of applied AI.\n",
        "\n",
        "About the Role:\n",
        "We are seeking a talented and motivated AI/Machine Learning Engineer to join our core development team. You will play a key role in the end-to-end lifecycle of machine learning projects, from conceptualization and data exploration to model deployment and monitoring. You'll work closely with data scientists, software engineers, and product managers to build innovative AI-powered features and products.\n",
        "\n",
        "Responsibilities:\n",
        "- Design, develop, train, and deploy machine learning models (including deep learning models) for tasks such as NLP, predictive analytics, anomaly detection, and personalization.\n",
        "- Process, cleanse, and verify the integrity of large datasets used for analysis and model training.\n",
        "- Collaborate with data engineering teams to build and maintain robust data pipelines for ML workflows.\n",
        "- Implement and maintain MLOps best practices for model versioning, testing, deployment, and monitoring.\n",
        "- Stay current with the latest advancements in AI/ML techniques, tools, and platforms.\n",
        "- Analyze experimental results, iterate on models, and communicate findings to technical and non-technical stakeholders.\n",
        "- Contribute to the development of internal AI platforms and tooling.\n",
        "\n",
        "Required Qualifications:\n",
        "- Bachelor's or Master's degree in Computer Science, Data Science, Statistics, or a related quantitative field.\n",
        "- 2+ years of hands-on experience building and deploying machine learning models in a production environment.\n",
        "- Strong programming skills in Python and proficiency with relevant ML libraries (e.g., Scikit-learn, TensorFlow, PyTorch, Keras).\n",
        "- Solid understanding of core machine learning algorithms, statistical modeling, and evaluation metrics.\n",
        "- Experience working with SQL and/or NoSQL databases.\n",
        "- Familiarity with data processing and analysis libraries (e.g., Pandas, NumPy).\n",
        "- Excellent problem-solving skills and attention to detail.\n",
        "- Strong communication and teamwork abilities.\n",
        "\n",
        "Desired Qualifications (Bonus Points):\n",
        "- PhD in a related field.\n",
        "- Experience with cloud platforms (AWS, GCP, or Azure) and their AI/ML services (e.g., SageMaker, Vertex AI, Azure ML).\n",
        "- Experience with MLOps tools and practices (e.g., Docker, Kubernetes, MLflow, Kubeflow).\n",
        "- Experience with Natural Language Processing (NLP) or Computer Vision (CV).\n",
        "- Experience with big data technologies (e.g., Spark, Hadoop).\n",
        "- Publications in relevant AI/ML conferences or journals.\n",
        "\n",
        "What We Offer:\n",
        "- Competitive salary and benefits package.\n",
        "- Opportunity to work on challenging and impactful AI projects.\n",
        "- A dynamic, collaborative, and supportive work environment.\n",
        "- Flexible remote work policy.\n",
        "- Professional development opportunities.\n",
        "\"\"\"\n",
        "\n",
        "# --- Create and display the Textarea widget ---\n",
        "print(\"‚¨áÔ∏è Edit the text area below with your target job description.\")\n",
        "print(\"   IMPORTANT: After editing, run this cell again (click ‚ñ∫) to save the text.\")\n",
        "job_desc_widget = widgets.Textarea(\n",
        "    value=default_job_desc,\n",
        "    placeholder='Paste the full job description here...',\n",
        "    description='',\n",
        "    disabled=False,\n",
        "    layout=widgets.Layout(height='400px', width='95%') # Adjust size as needed\n",
        ")\n",
        "display(job_desc_widget)\n",
        "\n",
        "# --- Capture value after execution ---\n",
        "# This line captures the *current* value from the widget when the cell runs.\n",
        "# The user MUST run the cell *after* they finish editing the text area.\n",
        "JOB_DESCRIPTION = job_desc_widget.value\n",
        "\n",
        "# --- Validation after capture ---\n",
        "# Check if the captured text is still the default or empty.\n",
        "if not JOB_DESCRIPTION:\n",
        "     print(\"\\n‚ùå Error: Job description is empty. Please paste the job description and run this cell again.\")\n",
        "     # Optionally stop execution if required: sys.exit(\"Stopping: Job description required.\")\n",
        "elif JOB_DESCRIPTION == default_job_desc:\n",
        "     print(\"\\n‚ö†Ô∏è Warning: The text box still contains the example job description.\")\n",
        "     print(\"   Please replace it with the actual job description you are targeting and run this cell again.\")\n",
        "     # Optionally stop execution: sys.exit(\"Stopping: Please provide the actual job description.\")\n",
        "elif len(JOB_DESCRIPTION) < 100: # Check for very short descriptions\n",
        "     print(\"\\n‚ö†Ô∏è Warning: The job description seems very short. Did you paste the full text?\")\n",
        "     print(\"   Continuing, but please double-check.\")\n",
        "     print(\"‚úÖ Job description captured (but please review if it's complete).\")\n",
        "else:\n",
        "     # If it's not empty, not the default, and has reasonable length\n",
        "     print(\"\\n‚úÖ Job description captured successfully!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ltGOM65YW6bV",
        "outputId": "f38d10ce-46c3-43b1-9973-3333160e0377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512,
          "referenced_widgets": [
            "2bc4461ed40448d19c5c56f4ab9ed1d0",
            "223f1a4d7c1249f29a77fd254b214fce",
            "0b3c60eaa79543408838c2bbfb45e82f"
          ]
        }
      },
      "id": "ltGOM65YW6bV",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è Edit the text area below with your target job description.\n",
            "   IMPORTANT: After editing, run this cell again (click ‚ñ∫) to save the text.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value=\"Job Title: AI/Machine Learning Engineer\\n\\nCompany: Innovate Solutions Inc.\\n\\nLocation: Remot‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2bc4461ed40448d19c5c56f4ab9ed1d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚ö†Ô∏è Warning: The text box still contains the example job description.\n",
            "   Please replace it with the actual job description you are targeting and run this cell again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Step 3.3: Configure Analysis Settings (Click ‚ñ∫)\n",
        "import sys # To potentially stop execution\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### Analysis Settings\n",
        "\n",
        "#@markdown Select the language the AI should use when chatting with you and rewriting CV sections.\n",
        "LANGUAGE = \"en\" #@param [\"en\", \"es\"]\n",
        "\n",
        "#@markdown How many of the most relevant CV items (e.g., past jobs, projects) should the AI focus on refining during the chat? (Usually 2-4 is a good number).\n",
        "MAX_RELEVANT_ITEMS = 3 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown Which sections of your CV should be analyzed and indexed? Enter the section names exactly as they appear in your structured CV (JSON), separated by commas.\n",
        "#@markdown **Common examples:** `work`, `projects`, `certificates`, `skills`, `education`, `publications`, `volunteer`\n",
        "#@markdown (Check section 4.4 later if you're unsure about your section names).\n",
        "CV_SECTIONS_TO_FOCUS_STR = \"work, projects, certificates\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown (Optional) Check the box below if you want to see the structured CV data (JSON format) after it's loaded or processed. This can be useful for verifying the section names.\n",
        "SHOW_STRUCTURED_CV = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "\n",
        "# --- Process and Validate Settings ---\n",
        "\n",
        "# Process the sections string into a clean list\n",
        "CV_SECTIONS_TO_FOCUS = [section.strip().lower() for section in CV_SECTIONS_TO_FOCUS_STR.split(',') if section.strip()]\n",
        "\n",
        "# Validation\n",
        "if not CV_SECTIONS_TO_FOCUS:\n",
        "    print(\"‚ùå Error: Please specify at least one CV section to focus on (e.g., 'work, projects').\")\n",
        "    sys.exit(\"Stopping: CV Sections to focus cannot be empty\") # Stop execution\n",
        "\n",
        "if MAX_RELEVANT_ITEMS < 1:\n",
        "    print(\"‚ö†Ô∏è Warning: Number of CV items to refine must be at least 1. Setting to 1.\")\n",
        "    MAX_RELEVANT_ITEMS = 1 # Reset to minimum valid\n",
        "\n",
        "print(\"‚úÖ Analysis settings confirmed:\")\n",
        "print(f\"   - Language for AI interaction: {LANGUAGE}\")\n",
        "print(f\"   - Max CV items to refine: {MAX_RELEVANT_ITEMS}\")\n",
        "print(f\"   - CV sections to analyze: {CV_SECTIONS_TO_FOCUS}\")\n",
        "print(f\"   - Show structured CV data: {SHOW_STRUCTURED_CV}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VVVi2-8TXGt-",
        "outputId": "10e4c5cb-6295-40ae-f759-f20133d441d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VVVi2-8TXGt-",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Analysis settings confirmed:\n",
            "   - Language for AI interaction: en\n",
            "   - Max CV items to refine: 3\n",
            "   - CV sections to analyze: ['work', 'projects', 'certificates']\n",
            "   - Show structured CV data: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Internal Configuration (Defaults - No changes needed)\n",
        "\n",
        "# --- Fixed Configuration - Do Not Modify Unless You Know What You're Doing ---\n",
        "# This cell sets up internal parameters based on the recommended settings\n",
        "# or values from the original notebook configuration.\n",
        "\n",
        "# Embedding Model Name (Responsible for understanding text meaning)\n",
        "# Using the model specified in the original notebook [Source: 89]\n",
        "EMBEDDING_MODEL_NAME = \"models/text-embedding-004\"\n",
        "\n",
        "# Generative Model Name (Used for structuring CV and the rewrite chat)\n",
        "# Using the model specified in the original notebook [Source: 90]\n",
        "GENERATIVE_MODEL_NAME = \"gemini-2.0-flash\"\n",
        "\n",
        "# RAG Retrieval Window (How much extra context to fetch during search)\n",
        "# Using the value from the original notebook [Source: 89]\n",
        "RETRIEVAL_WINDOW = 2\n",
        "\n",
        "# Vector DB Collection Name (Internal name for the 'smart index')\n",
        "# Using the value from the original notebook [Source: 90]\n",
        "COLLECTION_NAME = \"cv_embeddings_v1\"\n",
        "\n",
        "# LLM Parameters (Controls AI creativity/consistency - fixed for simplicity)\n",
        "# Using values from the original notebook [Source: 91]\n",
        "STRUCTURING_LLM_TEMPERATURE = 0.1 # Low temp for consistent JSON structuring\n",
        "REWRITING_LLM_TEMPERATURE = 0.8 # Higher temp for creative rewriting\n",
        "REWRITING_LLM_TOP_P = 0.95\n",
        "REWRITING_LLM_TOP_K = 30\n",
        "\n",
        "# CV Text Limit for Structuring (Safety cutoff for very long PDFs)\n",
        "# Using the value from the original notebook [Source: 94]\n",
        "MAX_CV_TEXT_LENGTH_FOR_STRUCTURING = 12000\n",
        "\n",
        "print(\"‚úÖ Internal fixed parameters loaded.\")\n",
        "# --- End of Fixed Configuration ---"
      ],
      "metadata": {
        "cellView": "form",
        "id": "g_SwwS-CXThm",
        "outputId": "b306625a-5873-4745-8aab-fdd3be53595a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "g_SwwS-CXThm",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Internal fixed parameters loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f2041e",
      "metadata": {
        "papermill": {
          "duration": 0.010359,
          "end_time": "2025-04-21T00:51:49.768747",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.758388",
          "status": "completed"
        },
        "tags": [],
        "id": "c3f2041e"
      },
      "source": [
        "## 4. Load and Process Your CV\n",
        "\n",
        "Now that the setup and configuration are done, the notebook will find your CV file (either `resume.json` or `resume.pdf`) in the Google Drive folder you specified. It will then load or process it into a structured format that the AI can understand."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b608f056",
      "metadata": {
        "papermill": {
          "duration": 0.010947,
          "end_time": "2025-04-21T00:51:49.853659",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.842712",
          "status": "completed"
        },
        "tags": [],
        "id": "b608f056"
      },
      "source": [
        "### 4.1. Load or Generate Structured CV Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b01fe7d",
      "metadata": {
        "papermill": {
          "duration": 0.011089,
          "end_time": "2025-04-21T00:51:49.876186",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.865097",
          "status": "completed"
        },
        "tags": [],
        "id": "9b01fe7d"
      },
      "source": [
        "This cell checks the Google Drive folder you specified:\n",
        "\n",
        "1.  It first looks for a file named `resume.json`. If it finds one (and you **didn't** check the \"Re-process PDF\" box earlier), it will load your CV directly from this file. This is faster and saves AI processing time if you've run the notebook before.\n",
        "2.  If it can't find `resume.json`, or if you checked the box to force reprocessing, it will then look for `resume.pdf`. If found, the notebook will proceed to extract the text and use AI to structure it in the following steps.\n",
        "3.  If neither file is found in the specified folder, it will stop with an error.\n",
        "\n",
        "The code cell below performs these checks and either loads the JSON or prepares to process the PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "a02db192",
      "metadata": {
        "papermill": {
          "duration": 4.013851,
          "end_time": "2025-04-21T00:51:53.900903",
          "exception": false,
          "start_time": "2025-04-21T00:51:49.887052",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 835
        },
        "cellView": "form",
        "id": "a02db192",
        "outputId": "67954521-52a2-421d-aef5-0d77dd1cfce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking folder: '/content/drive/MyDrive/ia-driven-cv-opt'\n",
            "Attempting to load existing structured CV from: '/content/drive/MyDrive/ia-driven-cv-opt/resume.json'...\n",
            "   '/content/drive/MyDrive/ia-driven-cv-opt/resume.json' not found. Will look for 'resume.pdf'.\n",
            "\n",
            "Looking for CV PDF file at: '/content/drive/MyDrive/ia-driven-cv-opt/resume.pdf'...\n",
            "‚úÖ Found PDF file: '/content/drive/MyDrive/ia-driven-cv-opt/resume.pdf'.\n",
            "   Will proceed with PDF text extraction and AI structuring.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Displaying PDF below for verification:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7e042c0db190>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"90%\"\n",
              "            height=\"600px\"\n",
              "            src=\"/content/drive/MyDrive/ia-driven-cv-opt/resume.pdf\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Load/Check step complete.\n"
          ]
        }
      ],
      "source": [
        "#@title Load CV Data (Checks for JSON first, then PDF)\n",
        "import json\n",
        "from IPython.display import IFrame, display, Markdown # For displaying PDF/messages\n",
        "import sys\n",
        "\n",
        "# Initialize variables\n",
        "structured_cv_data = None\n",
        "cv_loaded_from_json = False\n",
        "pdf_path_to_process = None\n",
        "\n",
        "print(f\"Checking folder: '{_base_path}'\") # _base_path defined in cell 3.1\n",
        "\n",
        "# --- Try loading JSON first (if not forced to reparse PDF) ---\n",
        "if not FORCE_PDF_REPARSE:\n",
        "    print(f\"Attempting to load existing structured CV from: '{CV_JSON_PATH}'...\")\n",
        "    if CV_JSON_PATH.is_file():\n",
        "        try:\n",
        "            with open(CV_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "                structured_cv_data = json.load(f)\n",
        "            cv_loaded_from_json = True\n",
        "            print(f\"‚úÖ Successfully loaded structured CV data from '{CV_JSON_PATH}'.\")\n",
        "            # Display a confirmation message in Markdown for better visibility\n",
        "            display(Markdown(f\"üëç **Success!** Found and loaded `resume.json`. We can skip the PDF processing and AI structuring steps.\"))\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ö†Ô∏è Found '{CV_JSON_PATH}', but failed to load it as JSON: {e}\")\n",
        "            print(\"   Will proceed to look for 'resume.pdf' instead.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Found '{CV_JSON_PATH}', but an unexpected error occurred while loading: {e}\")\n",
        "            print(\"   Will proceed to look for 'resume.pdf' instead.\")\n",
        "    else:\n",
        "        print(f\"   '{CV_JSON_PATH}' not found. Will look for 'resume.pdf'.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è 'Force PDF Reparse' is selected. Skipping JSON check, looking for 'resume.pdf'.\")\n",
        "\n",
        "# --- If JSON wasn't loaded, look for PDF ---\n",
        "if not cv_loaded_from_json:\n",
        "    print(f\"\\nLooking for CV PDF file at: '{CV_PDF_PATH}'...\")\n",
        "    if CV_PDF_PATH.is_file():\n",
        "        pdf_path_to_process = CV_PDF_PATH # Store path for next steps\n",
        "        print(f\"‚úÖ Found PDF file: '{CV_PDF_PATH}'.\")\n",
        "        print(\"   Will proceed with PDF text extraction and AI structuring.\")\n",
        "        # Display the PDF inline for user verification\n",
        "        try:\n",
        "            display(Markdown(\"---\"))\n",
        "            print(\"   Displaying PDF below for verification:\")\n",
        "            display(IFrame(src=pdf_path_to_process, width=\"90%\", height=\"600px\"))\n",
        "            display(Markdown(\"---\"))\n",
        "        except Exception as e:\n",
        "            print(f\"   (Could not display PDF inline due to an error: {e})\")\n",
        "    else:\n",
        "        # Critical error: No JSON loaded and no PDF found\n",
        "        print(f\"\\n‚ùå Error: PDF file not found at '{CV_PDF_PATH}'.\")\n",
        "        display(Markdown(f\"**‚ùå Critical Error:** Could not find `resume.json` or `resume.pdf` in the specified folder (`{_base_path}`).\"))\n",
        "        print(\"Please ensure one of these files exists in that folder and has the correct name.\")\n",
        "        sys.exit(\"Stopping: No valid CV file found.\") # Stop execution\n",
        "\n",
        "# Final check\n",
        "if cv_loaded_from_json and structured_cv_data is None:\n",
        "  # This case should be rare due to error handling above, but good practice\n",
        "  print(\"‚ùå Error: CV was marked as loaded from JSON, but data is missing.\")\n",
        "  sys.exit(\"Stopping: Error after attempting to load JSON.\")\n",
        "\n",
        "print(\"\\nLoad/Check step complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02608736",
      "metadata": {
        "papermill": {
          "duration": 0.011057,
          "end_time": "2025-04-21T00:51:53.923828",
          "exception": false,
          "start_time": "2025-04-21T00:51:53.912771",
          "status": "completed"
        },
        "tags": [],
        "id": "02608736"
      },
      "source": [
        "### 4.2. Extract Text from PDF (if needed)\n",
        "\n",
        "*This step only runs if a `resume.pdf` file is being processed (because `resume.json` was not found or you chose to re-process the PDF).*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c43274",
      "metadata": {
        "papermill": {
          "duration": 0.010943,
          "end_time": "2025-04-21T00:51:53.945949",
          "exception": false,
          "start_time": "2025-04-21T00:51:53.935006",
          "status": "completed"
        },
        "tags": [],
        "id": "18c43274"
      },
      "source": [
        "If a PDF is being used, this step simply reads the document and extracts all the text content from its pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "3e0191ef",
      "metadata": {
        "papermill": {
          "duration": 0.152752,
          "end_time": "2025-04-21T00:51:54.109897",
          "exception": false,
          "start_time": "2025-04-21T00:51:53.957145",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "3e0191ef",
        "outputId": "5867f5c6-6406-4c4d-c6e4-49c8e744318a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from PDF: '/content/drive/MyDrive/ia-driven-cv-opt/resume.pdf'...\n",
            "‚úÖ Text extraction successful. Total characters: 2310\n"
          ]
        }
      ],
      "source": [
        "#@title Extract Text from PDF (Runs only if PDF is used)\n",
        "import pdfplumber\n",
        "import sys\n",
        "\n",
        "# Initialize raw_cv_text, it will only be populated if PDF processing is needed\n",
        "raw_cv_text = None\n",
        "\n",
        "# --- Run extraction only if CV data was NOT loaded from JSON ---\n",
        "if not cv_loaded_from_json:\n",
        "    # Check if pdf_path_to_process was set correctly in the previous step\n",
        "    if pdf_path_to_process and pdf_path_to_process.is_file():\n",
        "        print(f\"Extracting text from PDF: '{pdf_path_to_process}'...\")\n",
        "        try:\n",
        "            extracted_pages = []\n",
        "            with pdfplumber.open(pdf_path_to_process) as pdf:\n",
        "                if not pdf.pages:\n",
        "                    print(\"‚ö†Ô∏è Warning: The PDF file seems to have no pages.\")\n",
        "                for i, page in enumerate(pdf.pages):\n",
        "                    # Extract text page by page, handling potential None values from empty pages\n",
        "                    page_text = page.extract_text(x_tolerance=1, y_tolerance=3)\n",
        "                    if page_text:\n",
        "                        extracted_pages.append(page_text)\n",
        "                    # else: # Optional: uncomment to debug empty pages\n",
        "                    #     print(f\"   (Page {i+1} had no extractable text)\")\n",
        "\n",
        "            raw_cv_text = \"\\n\".join(extracted_pages) # Join pages with newline\n",
        "\n",
        "            if not raw_cv_text:\n",
        "                 print(\"‚ö†Ô∏è Warning: Text extraction resulted in empty text. The PDF might be image-based or empty.\")\n",
        "                 # Consider if we should stop here if text is crucial\n",
        "                 # sys.exit(\"Stopping: Failed to extract any text from PDF.\")\n",
        "            else:\n",
        "                print(f\"‚úÖ Text extraction successful. Total characters: {len(raw_cv_text)}\")\n",
        "                # Optional: Uncomment the line below to print the first 500 characters\n",
        "                # print(f\"--- Snippet ---\\n{raw_cv_text[:500]}\\n---------------\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå ERROR: Failed to open or extract text from PDF '{pdf_path_to_process}'.\")\n",
        "            print(f\"   Error details: {e}\")\n",
        "            print(\"   The PDF might be corrupted or password-protected.\")\n",
        "            raw_cv_text = None # Ensure it's None on error\n",
        "            sys.exit(\"Stopping: PDF text extraction failed.\")\n",
        "    else:\n",
        "        # This should ideally not be reached due to checks in cell 4.1, but as a safeguard:\n",
        "        print(\"‚ùå Error: PDF processing was expected, but the PDF path is missing or invalid.\")\n",
        "        sys.exit(\"Stopping: PDF path error.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Skipping PDF text extraction because CV data was loaded from JSON.\")\n",
        "\n",
        "# --- Final check for this step ---\n",
        "# If we expected to process PDF but raw_cv_text is still None or empty, something went wrong.\n",
        "if not cv_loaded_from_json and not raw_cv_text:\n",
        "    print(\"\\n‚ùå Error: PDF processing was required, but no text could be extracted.\")\n",
        "    # Decide if execution should stop\n",
        "    # sys.exit(\"Stopping: Failed to get text content from PDF.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb680db8",
      "metadata": {
        "papermill": {
          "duration": 0.011342,
          "end_time": "2025-04-21T00:51:54.133119",
          "exception": false,
          "start_time": "2025-04-21T00:51:54.121777",
          "status": "completed"
        },
        "tags": [],
        "id": "cb680db8"
      },
      "source": [
        "### 4.3. Structure CV using AI (if needed)\n",
        "\n",
        "*This step uses AI to understand the extracted PDF text and organize it into a standard structure (JSON format). It only runs if a `resume.pdf` file was processed.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "272a632f",
      "metadata": {
        "papermill": {
          "duration": 0.011457,
          "end_time": "2025-04-21T00:51:54.155957",
          "exception": false,
          "start_time": "2025-04-21T00:51:54.144500",
          "status": "completed"
        },
        "tags": [],
        "id": "272a632f"
      },
      "source": [
        "If the notebook extracted text from a PDF, this step sends that text to the Google AI model (Gemini).\n",
        "\n",
        "The AI's job here is to act like a very organized assistant: it reads the raw CV text and puts the information into specific categories (like \"Work Experience\", \"Education\", \"Skills\") based on a standard format called \"JSON Resume\".\n",
        "\n",
        "This structured format makes it much easier for the later steps to find and analyze the specific parts of your CV that you selected (e.g., just your 'work' history or 'projects')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "b9d7e19d",
      "metadata": {
        "papermill": {
          "duration": 0.02762,
          "end_time": "2025-04-21T00:51:54.194945",
          "exception": false,
          "start_time": "2025-04-21T00:51:54.167325",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "b9d7e19d",
        "outputId": "a493a6c2-bbb5-4ac5-d07b-d3109d0cca74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CV Structure Schema defined.\n"
          ]
        }
      ],
      "source": [
        "#@title Define CV Structure Schema (Internal - based on JSON Resume Standard)\n",
        "# This code defines the expected structure (like a template) for the CV data\n",
        "# that the AI will generate from the PDF text. It's based on the standard\n",
        "# JSON Resume schema (https://jsonresume.org/schema/).\n",
        "# You don't need to expand or modify this.\n",
        "\n",
        "from typing import List, Optional # Using standard typing hints\n",
        "# Use typing_extensions for TypedDict if needed for compatibility,\n",
        "# but standard typing might suffice in newer Python/Colab versions.\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Define nested structures first (order matters for definition)\n",
        "class Location(TypedDict, total=False):\n",
        "    address: str\n",
        "    postalCode: str\n",
        "    city: str\n",
        "    countryCode: str\n",
        "    region: str\n",
        "\n",
        "class Profile(TypedDict, total=False):\n",
        "    network: str\n",
        "    username: str\n",
        "    url: str\n",
        "\n",
        "class Basics(TypedDict, total=False):\n",
        "    name: str\n",
        "    label: str # Job title / headline\n",
        "    image: str  # URL to profile image\n",
        "    email: str\n",
        "    phone: str\n",
        "    url: str  # Personal website/portfolio URL\n",
        "    summary: str  # Professional summary/objective\n",
        "    location: Location\n",
        "    profiles: List[Profile] # List of social media/professional profiles\n",
        "\n",
        "class WorkItem(TypedDict, total=False):\n",
        "    name: str  # Name of the company/organization\n",
        "    position: str  # Job title\n",
        "    url: str  # Company website\n",
        "    startDate: str  # Format YYYY-MM-DD or YYYY-MM or YYYY\n",
        "    endDate: str  # Format YYYY-MM-DD or YYYY-MM or YYYY, or 'Present'\n",
        "    summary: str  # High-level description of role/company achievements\n",
        "    highlights: List[str]  # Specific achievements or responsibilities (bullet points)\n",
        "\n",
        "class VolunteerItem(TypedDict, total=False):\n",
        "    organization: str\n",
        "    position: str\n",
        "    url: str\n",
        "    startDate: str\n",
        "    endDate: str\n",
        "    summary: str\n",
        "    highlights: List[str]\n",
        "\n",
        "class EducationItem(TypedDict, total=False):\n",
        "    institution: str\n",
        "    url: str\n",
        "    area: str  # e.g., Computer Science\n",
        "    studyType: str  # e.g., Bachelor's Degree, Master's\n",
        "    startDate: str\n",
        "    endDate: str\n",
        "    score: str  # e.g., GPA\n",
        "    courses: List[str]  # Relevant coursework\n",
        "\n",
        "class AwardItem(TypedDict, total=False):\n",
        "    title: str\n",
        "    date: str  # Date awarded\n",
        "    awarder: str  # Organization that gave the award\n",
        "    summary: str  # Description of the award\n",
        "\n",
        "class CertificateItem(TypedDict, total=False):\n",
        "    name: str  # Name of the certificate\n",
        "    date: str  # Date issued\n",
        "    issuer: str  # Issuing organization (e.g., Coursera, Google)\n",
        "    url: str  # Link to certificate if available\n",
        "\n",
        "class PublicationItem(TypedDict, total=False):\n",
        "    name: str  # Title of the publication\n",
        "    publisher: str  # e.g., Journal name, Conference\n",
        "    releaseDate: str\n",
        "    url: str  # Link to publication\n",
        "    summary: str  # Abstract or brief description\n",
        "\n",
        "class SkillItem(TypedDict, total=False):\n",
        "    name: str  # Broad skill category (e.g., Web Development, Data Science)\n",
        "    level: str  # Optional proficiency level (e.g., Intermediate, Advanced)\n",
        "    keywords: List[str]  # Specific technologies or tools (e.g., Python, PyTorch, AWS)\n",
        "\n",
        "class LanguageItem(TypedDict, total=False):\n",
        "    language: str  # e.g., English, Spanish\n",
        "    fluency: str  # e.g., Native, Fluent, Conversational\n",
        "\n",
        "class InterestItem(TypedDict, total=False):\n",
        "    name: str  # Category of interest (e.g., Open Source, AI Ethics)\n",
        "    keywords: List[str]  # Specific interests\n",
        "\n",
        "class ReferenceItem(TypedDict, total=False):\n",
        "    name: str  # Name of reference (ensure consent)\n",
        "    reference: str  # Testimonial or contact details (handle privacy appropriately)\n",
        "\n",
        "class ProjectItem(TypedDict, total=False):\n",
        "    name: str  # Project title\n",
        "    description: str # Overall description of the project\n",
        "    highlights: List[str]  # Key contributions or features\n",
        "    keywords: List[str] # Technologies used\n",
        "    url: str  # Link to project demo or repository\n",
        "    startDate: str\n",
        "    endDate: str\n",
        "    roles: List[str] # Roles held in the project\n",
        "    entity: str # Associated entity (e.g., university, company)\n",
        "    type: str # Type of project (e.g., personal, academic, professional)\n",
        "\n",
        "\n",
        "# --- Top-Level Curriculum Schema ---\n",
        "class Curriculum(TypedDict, total=False):\n",
        "    \"\"\"Represents the complete JSON Resume structure.\"\"\"\n",
        "    basics: Basics\n",
        "    work: List[WorkItem]\n",
        "    volunteer: List[VolunteerItem]\n",
        "    education: List[EducationItem]\n",
        "    awards: List[AwardItem]\n",
        "    certificates: List[CertificateItem]\n",
        "    publications: List[PublicationItem]\n",
        "    skills: List[SkillItem]\n",
        "    languages: List[LanguageItem]\n",
        "    interests: List[InterestItem]\n",
        "    references: List[ReferenceItem]\n",
        "    projects: List[ProjectItem]\n",
        "\n",
        "print(\"‚úÖ CV Structure Schema defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "315ee3d0",
      "metadata": {
        "papermill": {
          "duration": 6.341404,
          "end_time": "2025-04-21T00:52:00.547970",
          "exception": false,
          "start_time": "2025-04-21T00:51:54.206566",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "315ee3d0",
        "outputId": "b88d2c4a-c3bb-4858-b2bc-6054ed514a89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing to call the AI model to structure the extracted CV text...\n",
            "   (This AI call might take a moment...)\n",
            "üìû Calling Gemini AI (gemini-2.0-flash) via client to structure CV text...\n",
            "‚úÖ Gemini AI call finished.\n",
            "‚úÖ Successfully parsed AI response into structured CV data.\n"
          ]
        }
      ],
      "source": [
        "#@title Call AI to Structure CV Text (Runs only if PDF was used)\n",
        "import json\n",
        "from google import genai # Using original import style\n",
        "from google.genai import types\n",
        "from google.api_core import retry\n",
        "import sys\n",
        "\n",
        "# --- Run structuring only if CV data was NOT loaded from JSON ---\n",
        "if not cv_loaded_from_json:\n",
        "    # Check if raw_cv_text was successfully extracted\n",
        "    if raw_cv_text:\n",
        "        print(\"Preparing to call the AI model to structure the extracted CV text...\")\n",
        "\n",
        "        # --- 1. Initialize Client & Validate API Key ---\n",
        "        if 'GOOGLE_API_KEY' not in globals() or not GOOGLE_API_KEY:\n",
        "             print(\"‚ùå Error: GOOGLE_API_KEY not found or empty. Check Step 2.2.\")\n",
        "             sys.exit(\"Stopping: API Key missing for AI call.\")\n",
        "\n",
        "        try:\n",
        "            # Using the client approach from original notebook [Source: 153]\n",
        "            # NOTE: The 'google.genai' client interface might be deprecated eventually\n",
        "            # in favor of 'google.generativeai', but using it as requested.\n",
        "            client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "            # --- 2. Prepare Prompt & Configuration ---\n",
        "            cv_text_for_prompt = raw_cv_text\n",
        "            if (\n",
        "                MAX_CV_TEXT_LENGTH_FOR_STRUCTURING is not None\n",
        "                and len(raw_cv_text) > MAX_CV_TEXT_LENGTH_FOR_STRUCTURING\n",
        "            ):\n",
        "                cv_text_for_prompt = raw_cv_text[:MAX_CV_TEXT_LENGTH_FOR_STRUCTURING]\n",
        "                print(\n",
        "                    f\"‚ö†Ô∏è Warning: CV text truncated to {MAX_CV_TEXT_LENGTH_FOR_STRUCTURING} characters for structuring.\"\n",
        "                )\n",
        "\n",
        "            # System instruction - kept strict for JSON output\n",
        "            system_instruction = (\n",
        "                \"You are an expert CV parser. Extract information from the provided CV text and \"\n",
        "                \"format it strictly according to the JSON Resume Schema provided using the `Curriculum` type. \"\n",
        "                \"Return ONLY the valid JSON object conforming to the schema - no introductory text, \"\n",
        "                \"no markdown formatting (like ```json ... ```), no explanations.\"\n",
        "            )\n",
        "            # Prompt\n",
        "            prompt_message = f\"\"\"Given the following CV text, populate the fields of the JSON Resume Schema (represented by the `Curriculum` TypedDict) as accurately as possible.\n",
        "Use empty strings, arrays, or null values for fields where information is missing in the text. Ensure the output is a single, valid JSON object.\n",
        "\n",
        "CV Text:\n",
        "---\n",
        "{cv_text_for_prompt}\n",
        "---\n",
        "\n",
        "Respond with ONLY the JSON object.\n",
        "\"\"\"\n",
        "            # Combine instructions and prompt\n",
        "            full_structuring_prompt = system_instruction + \"\\n\\n\" + prompt_message\n",
        "\n",
        "            # Configuration using the structure from original notebook [Source: 158]\n",
        "            # Requires the Curriculum class defined in the previous cell\n",
        "            json_generation_config = {\n",
        "                \"temperature\": STRUCTURING_LLM_TEMPERATURE,\n",
        "                \"response_mime_type\": \"application/json\",\n",
        "                \"response_schema\": Curriculum, # Pass the TypedDict class\n",
        "            }\n",
        "\n",
        "            # --- 3. Define Retry Function (using original client method) ---\n",
        "            @retry.Retry(predicate=is_retriable) # Uses 'is_retriable' defined earlier\n",
        "            def generate_structured_cv_json_with_retry(prompt, config):\n",
        "                \"\"\"Calls the Gemini API using client.models.generate_content.\"\"\"\n",
        "                print(f\"üìû Calling Gemini AI ({GENERATIVE_MODEL_NAME}) via client to structure CV text...\")\n",
        "                # Using client.models.generate_content as in original [Source: 160]\n",
        "                response = client.models.generate_content(\n",
        "                   model=GENERATIVE_MODEL_NAME, # Ensure model name is correct\n",
        "                   contents=prompt, # Pass the full prompt here\n",
        "                   config=config,\n",
        "                )\n",
        "                print(\"‚úÖ Gemini AI call finished.\")\n",
        "                # Accessing response text might depend slightly on the client version,\n",
        "                # but response.text is common. Check original if needed.\n",
        "                return response.text # Assuming response.text contains the JSON string\n",
        "\n",
        "            # --- 4. Execute API Call and Parse Response ---\n",
        "            print(\"   (This AI call might take a moment...)\")\n",
        "            json_response_text = generate_structured_cv_json_with_retry(\n",
        "                prompt=full_structuring_prompt, # Pass the combined prompt\n",
        "                config=json_generation_config,\n",
        "            )\n",
        "\n",
        "            if json_response_text:\n",
        "                try:\n",
        "                    # Parse the validated JSON text\n",
        "                    structured_cv_data = json.loads(json_response_text)\n",
        "                    print(\"‚úÖ Successfully parsed AI response into structured CV data.\")\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(\"\\n‚ùå ERROR: Failed to parse the AI's response as JSON.\")\n",
        "                    print(f\"   JSONDecodeError: {e}\")\n",
        "                    print(\"\\n   AI Response Text Received:\\n---\\n\", json_response_text, \"\\n---\\n\")\n",
        "                    structured_cv_data = None\n",
        "            else:\n",
        "                print(\"\\n‚ùå ERROR: Received an empty response from the AI structuring call.\")\n",
        "                structured_cv_data = None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"\\n‚ùå ERROR: An unexpected error occurred during the AI structuring call.\")\n",
        "            print(f\"   Error details: {type(e).__name__} - {e}\")\n",
        "            structured_cv_data = None\n",
        "            sys.exit(f\"Stopping due to error during AI structuring: {e}\")\n",
        "\n",
        "    else:\n",
        "         print(\"‚ÑπÔ∏è Skipping AI structuring because no text was extracted from the PDF.\")\n",
        "         structured_cv_data = None\n",
        "\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Skipping AI structuring because CV data was loaded from JSON.\")\n",
        "\n",
        "# --- Final check ---\n",
        "if not cv_loaded_from_json and structured_cv_data is None:\n",
        "    print(\"\\n‚ùå Error: Failed to obtain structured CV data after processing the PDF.\")\n",
        "    sys.exit(\"Stopping: Could not structure CV data from PDF.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cfa7c1",
      "metadata": {
        "papermill": {
          "duration": 0.011208,
          "end_time": "2025-04-21T00:52:00.571156",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.559948",
          "status": "completed"
        },
        "tags": [],
        "id": "f6cfa7c1"
      },
      "source": [
        "### 4.4. Review Structured CV Data (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3e64ced",
      "metadata": {
        "papermill": {
          "duration": 0.011371,
          "end_time": "2025-04-21T00:52:00.594727",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.583356",
          "status": "completed"
        },
        "tags": [],
        "id": "e3e64ced"
      },
      "source": [
        "If you checked the \"Show structured CV data\" box in the settings earlier (Step 3.3), the next cell will display the structured CV information that the notebook has loaded or generated.\n",
        "\n",
        "This data is shown in YAML format, which is similar to JSON but often easier for humans to read.\n",
        "\n",
        "You can expand the cell below to:\n",
        "* Verify that the AI understood your CV content correctly (if it was processed from PDF).\n",
        "* See the exact names of the sections (like `work`, `projects`, `skills`) that the AI identified. This is useful for confirming the section names you entered in the \"CV Sections to Analyze\" setting.\n",
        "\n",
        "If you didn't check the box, the next cell will be skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "dc75b72e",
      "metadata": {
        "papermill": {
          "duration": 0.062397,
          "end_time": "2025-04-21T00:52:00.671007",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.608610",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "dc75b72e",
        "outputId": "722c5b32-20af-4d71-d7f0-54086ebc0ab1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Structured CV Data (YAML Format) ---\n",
            "basics:\n",
            "  name: FRANCISCO MU√ëOZ-GUAJARDO\n",
            "  label: Data Scientist & Mathematical Engineer\n",
            "  image: ''\n",
            "  email: fmunoz@ug.uchile.cl\n",
            "  phone: ''\n",
            "  url: framunoz.github.io\n",
            "  summary: Master‚Äôs in Applied Mathematics with experience in solving problems across various fields, including transportation, healthcare, education, and mining. I have utilized data science tools, deep learning, machine learning, image processing, and programming methodologies. I stand out for my proactivity, creativity, and continuous learning approach.\n",
            "  location: {address: '', postalCode: '', city: '', countryCode: '', region: ''}\n",
            "  profiles:\n",
            "  - {network: github, username: framunoz, url: github.com/framunoz}\n",
            "  - {network: linkedin, username: femunozg, url: linkedin.com/in/femunozg}\n",
            "work:\n",
            "- name: Centro Nacional en Inteligencia Artificial (CENIA)\n",
            "  position: Contract-Based Position\n",
            "  url: ''\n",
            "  startDate: 2022-03\n",
            "  endDate: 2024-10\n",
            "  summary: Analyzed, processed, and generated a satellite image dataset to predict wildfire evolution in Chile using Geopandas and Google Earth Engine.\n",
            "  highlights: []\n",
            "- name: Centro de Modelamiento Matem√°tico, UNIVERSIDAD DE CHILE\n",
            "  position: Researcher & Thesis Student\n",
            "  url: ''\n",
            "  startDate: 2022-12\n",
            "  endDate: 2024-10\n",
            "  summary: Implemented Stochastic Gradient Descent for the computation of Bayesian Wasserstein Barycenters applied to image sets. Developed a library providing algorithms for Bayesian barycenter computation. Implemented generative neural networks based on Wasserstein distance. Developed a library to obtain the Quick, Draw! dataset.\n",
            "  highlights: []\n",
            "- name: Freelance\n",
            "  position: ''\n",
            "  url: ''\n",
            "  startDate: 2022-11\n",
            "  endDate: 2023-01\n",
            "  summary: Conducted a statistical analysis to determine the correlation between inclusive teaching methodologies and the academic performance of primary school students.\n",
            "  highlights: []\n",
            "- name: SimpliRoute\n",
            "  position: Data Science Internship\n",
            "  url: ''\n",
            "  startDate: 2021-01\n",
            "  endDate: 2021-03\n",
            "  summary: Developed algorithms for detecting anomalous georeferenced data using clustering methods. Implemented an intelligent service time detection method and deployed statistics for analysis and decision-making.\n",
            "  highlights: []\n",
            "- name: SARCAN\n",
            "  position: R&D Engineer & Internship\n",
            "  url: ''\n",
            "  startDate: 2020-01\n",
            "  endDate: 2020-08\n",
            "  summary: Researched optimization methods for intercity transport supply. Designed the framework for problem-solving. Developed optimization models for route optimization.\n",
            "  highlights: []\n",
            "volunteer: []\n",
            "education:\n",
            "- institution: UNIVERSIDAD DE CHILE\n",
            "  url: ''\n",
            "  area: Applied Mathematics\n",
            "  studyType: M.Sc.\n",
            "  startDate: 2017-03\n",
            "  endDate: 2024-10\n",
            "  score: ''\n",
            "  courses: []\n",
            "- institution: UNIVERSIDAD DE CHILE\n",
            "  url: ''\n",
            "  area: Mathematical Engineering\n",
            "  studyType: ''\n",
            "  startDate: ''\n",
            "  endDate: ''\n",
            "  score: ''\n",
            "  courses: [Minor in Computer Science]\n",
            "awards: []\n",
            "certificates: []\n",
            "publications: []\n",
            "skills:\n",
            "- name: Software\n",
            "  level: ''\n",
            "  keywords: ['Python (pytorch, sklearn, pandas, geopandas)', Java, SQL, Git, Julia, Excel]\n",
            "languages:\n",
            "- {language: Spanish, fluency: Native}\n",
            "- {language: English, fluency: 'Intermediate (B1, TOEFL ITP)'}\n",
            "interests: []\n",
            "references: []\n",
            "projects: []\n",
            "\n",
            "--- End of Structured CV Data ---\n"
          ]
        }
      ],
      "source": [
        "#@title Display Structured CV Data (YAML Format)\n",
        "import yaml\n",
        "import sys\n",
        "\n",
        "# --- Display only if data exists and the user opted-in ---\n",
        "# Check if SHOW_STRUCTURED_CV was defined in cell 3.3 and is True\n",
        "should_display = 'SHOW_STRUCTURED_CV' in globals() and SHOW_STRUCTURED_CV\n",
        "\n",
        "if should_display:\n",
        "    # Check if structured_cv_data was successfully loaded or created\n",
        "    if 'structured_cv_data' in globals() and structured_cv_data is not None:\n",
        "        print(\"--- Structured CV Data (YAML Format) ---\")\n",
        "        try:\n",
        "            # Use yaml.dump for readable output\n",
        "            # Use allow_unicode=True for non-ASCII characters\n",
        "            # Use sort_keys=False to maintain original order where possible\n",
        "            # Use width=float(\"inf\") to prevent line wrapping within blocks\n",
        "            print(\n",
        "                yaml.dump(\n",
        "                    structured_cv_data,\n",
        "                    allow_unicode=True,\n",
        "                    sort_keys=False,\n",
        "                    width=float(\"inf\"),\n",
        "                    default_flow_style=None # Use block style for readability\n",
        "                )\n",
        "            )\n",
        "        except yaml.YAMLError as e:\n",
        "            print(f\"\\n‚ùå Error formatting data as YAML: {e}\")\n",
        "            print(\"   Displaying raw data instead:\")\n",
        "            print(structured_cv_data) # Fallback to raw print\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ùå An unexpected error occurred during display: {e}\")\n",
        "        print(\"--- End of Structured CV Data ---\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Structured CV data is not available (likely an issue in previous steps). Cannot display.\")\n",
        "else: # Optional: uncomment if you want a message when display is skipped due to checkbox\n",
        "   print(\"‚ÑπÔ∏è Skipping display of structured CV data as requested in settings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7fb81c",
      "metadata": {
        "papermill": {
          "duration": 0.014399,
          "end_time": "2025-04-21T00:52:00.703230",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.688831",
          "status": "completed"
        },
        "tags": [],
        "id": "ba7fb81c"
      },
      "source": [
        "## 5. Create a 'Smart Index' of Your CV"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b689c5e1",
      "metadata": {
        "papermill": {
          "duration": 0.014778,
          "end_time": "2025-04-21T00:52:00.733826",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.719048",
          "status": "completed"
        },
        "tags": [],
        "id": "b689c5e1"
      },
      "source": [
        "Okay, now we need to make your structured CV searchable in a \"smart\" way. The AI needs to understand the *meaning* behind the text in your work experience, projects, etc., not just match exact keywords.\n",
        "\n",
        "To do this, we'll create a **\"Smart Index\"** (sometimes called a vector database or embeddings) for the CV sections you selected earlier (like `work`, `projects`).\n",
        "\n",
        "Here's the idea:\n",
        "1.  **Convert to Meaning:** The AI reads each item (like a specific job description from your CV) and converts its text into a list of numbers (called an \"embedding\" or \"vector\"). This list represents the core meaning or concepts in that text.\n",
        "2.  **Store in Index:** These numerical representations are stored in a special database (the Smart Index) located in the Google Drive folder you specified (`cv_chroma_db`).\n",
        "3.  **Enable Smart Search:** This index allows the AI to quickly find CV items whose *meaning* is similar to the *meaning* of the job description later on, even if they don't use the exact same words.\n",
        "\n",
        "The code cells below handle preparing your CV data for this process, setting up the conversion tool, and building the index. You don't need to worry about the details unless you're curious!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "92e33960",
      "metadata": {
        "papermill": {
          "duration": 0.047622,
          "end_time": "2025-04-21T00:52:00.823460",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.775838",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "92e33960",
        "outputId": "2e23d9df-a1bf-4a49-95e4-2549db9f6123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing documents from CV sections: ['work', 'projects', 'certificates']...\n",
            "   + Processing 5 item(s) from section 'work'...\n",
            "   - Section 'projects' is empty, skipping.\n",
            "   - Section 'certificates' is empty, skipping.\n",
            "\n",
            "‚úÖ Prepared 5 documents for indexing.\n",
            "   (2 section(s) were skipped or not found in the data).\n",
            "\n",
            "--- Sample Prepared Data (First Item) ---\n",
            "ID        : work.centronacionaleninteligenciaartificialcenia.contract.ba.ed\n",
            "Metadata  : {'section': 'work', 'item_index': 0, 'item_name': 'Centro Nacional en Inteligencia Artificial (CENIA)', 'item_position': 'Contract-Based Position'}\n",
            "Document Snippet:\n",
            "---\n",
            "name: Centro Nacional en Inteligencia Artificial (CENIA)\n",
            "position: Contract-Based Position\n",
            "url: ''\n",
            "startDate: 2022-03\n",
            "endDate: 2024-10\n",
            "summary: Analyzed, processed, and generated a satellite image dataset to predict wildfire evolution in Chile using Geopandas and Google Earth Engine.\n",
            "highlights: []\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "#@title Prepare CV Sections for Indexing (Internal Code)\n",
        "# This code takes the structured CV data and formats the specific sections\n",
        "# you selected (e.g., work, projects) into individual text pieces (documents)\n",
        "# ready for the 'meaning conversion' step. It also creates unique IDs\n",
        "# and extracts some metadata for each piece.\n",
        "\n",
        "import yaml\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# Check if structured_cv_data exists from previous steps\n",
        "if 'structured_cv_data' not in globals() or structured_cv_data is None:\n",
        "    print(\"‚ùå Error: Structured CV data is not available. Cannot prepare documents for indexing.\")\n",
        "    print(\"   Please check the output of previous steps (Section 4).\")\n",
        "    sys.exit(\"Stopping: Missing structured CV data.\")\n",
        "\n",
        "# Check if CV_SECTIONS_TO_FOCUS is defined\n",
        "if 'CV_SECTIONS_TO_FOCUS' not in globals() or not CV_SECTIONS_TO_FOCUS:\n",
        "    print(\"‚ùå Error: The list of CV sections to focus on is missing.\")\n",
        "    print(\"   Please check the 'Configure Analysis Settings' step (Cell 3.3).\")\n",
        "    sys.exit(\"Stopping: Missing CV sections to focus on.\")\n",
        "\n",
        "# --- 1. Helper Functions for ID Generation (from original notebook) ---\n",
        "# Maps section keys to functions creating a base ID string from item content\n",
        "# Using lowercase keys consistent with CV_SECTIONS_TO_FOCUS processing\n",
        "BASE_ID_GENERATORS = {\n",
        "    \"work\": lambda item: (\n",
        "        f\"{item.get('name', 'NoCompany')}.{item.get('position', 'NoPosition')}\"\n",
        "    ),\n",
        "    \"certificates\": lambda item: (\n",
        "        f\"{item.get('issuer', 'NoIssuer')}.{item.get('name', 'NoCert')}\"\n",
        "    ),\n",
        "    \"publications\": lambda item: (\n",
        "        f\"{item.get('publisher', 'NoPublisher')}.{item.get('name', 'NoPub')}\"\n",
        "    ),\n",
        "    \"projects\": lambda item: item.get(\"name\", \"NoProject\"),\n",
        "    \"volunteer\": lambda item: (\n",
        "        f\"{item.get('organization', 'NoOrg')}.{item.get('position', 'NoVolunteerPos')}\"\n",
        "    ),\n",
        "    \"education\": lambda item: (\n",
        "        f\"{item.get('institution', 'NoInstitution')}.{item.get('area', 'NoArea')}.{item.get('studyType', '')}\"\n",
        "    ),\n",
        "    \"basics\": lambda item: item.get(\"name\", \"NoPerson\"), # Usually only one 'basics' item\n",
        "    \"awards\": lambda item: (\n",
        "        f\"{item.get('awarder', 'NoAwarder')}.{item.get('title', 'NoAward')}\"\n",
        "    ),\n",
        "    \"skills\": lambda item: item.get(\"name\", \"NoSkill\"), # ID based on skill category name\n",
        "    \"languages\": lambda item: item.get(\"language\", \"NoLang\"),\n",
        "    \"interests\": lambda item: item.get(\"name\", \"NoInterest\"),\n",
        "    \"references\": lambda item: item.get(\"name\", \"NoReference\"),\n",
        "}\n",
        "\n",
        "def sanitize_id(text_id: str) -> str:\n",
        "    \"\"\"Cleans and formats a string into a valid ChromaDB ID (max 63 chars).\"\"\"\n",
        "    if not isinstance(text_id, str): # Handle potential non-string input\n",
        "        text_id = str(text_id)\n",
        "    text_id = text_id.lower()\n",
        "    # Basic accent removal\n",
        "    accent_map = {\n",
        "        '√°':'a', '√§':'a', '√¢':'a', '√†':'a', '√£':'a', '√•':'a', '√©':'e', '√´':'e',\n",
        "        '√™':'e', '√®':'e', '√≠':'i', '√Ø':'i', '√Æ':'i', '√¨':'i', '√≥':'o', '√∂':'o',\n",
        "        '√¥':'o', '√≤':'o', '√µ':'o', '√∏':'o', '√∫':'u', '√º':'u', '√ª':'u', '√π':'u',\n",
        "        '√±':'n', '√ß':'c',\n",
        "    }\n",
        "    # Use regex for replacement\n",
        "    pattern = re.compile(\"|\".join(accent_map.keys()))\n",
        "    text_id = pattern.sub(lambda m: accent_map[m.group(0)], text_id)\n",
        "\n",
        "    text_id = re.sub(r\"[\\\\s_:/]+\", \".\", text_id)  # Replace common separators with dot\n",
        "    text_id = re.sub(r\"[^a-z0-9.\\-]+\", \"\", text_id) # Keep alphanumeric, dot, hyphen\n",
        "    text_id = re.sub(r\"[.\\-]+\", \".\", text_id)  # Consolidate consecutive dots/hyphens into dots\n",
        "    text_id = text_id.strip(\".\")  # Remove leading/trailing dots\n",
        "\n",
        "    # Ensure minimum length and apply maximum length constraint\n",
        "    if len(text_id) < 3:\n",
        "      text_id = f\"{text_id}id\" # Append 'id' to short strings\n",
        "    return text_id[:63] # Truncate to 63 characters (ChromaDB limit)\n",
        "\n",
        "\n",
        "def generate_unique_item_id(section_key: str, item: dict, item_index: int) -> str:\n",
        "    \"\"\"Generates a unique, sanitized ID for a CV item.\"\"\"\n",
        "    # Use lowercase section_key for lookup\n",
        "    id_generator = BASE_ID_GENERATORS.get(\n",
        "        section_key, lambda i: f\"item.{item_index}\" # Fallback generator\n",
        "    )\n",
        "    try:\n",
        "      base_id = id_generator(item)\n",
        "    except Exception as e:\n",
        "      print(f\"Warning: Error generating base ID for item {index} in {section_key}: {e}. Using fallback.\")\n",
        "      base_id = f\"item.{item_index}\"\n",
        "\n",
        "    # Include section key and index for uniqueness\n",
        "    full_id_base = f\"{section_key}.{base_id}.{item_index}\"\n",
        "    return sanitize_id(full_id_base)\n",
        "\n",
        "\n",
        "# --- 2. Function to Prepare Data for Embedding ---\n",
        "def prepare_embedding_data(\n",
        "    cv_data: dict, sections_to_include: list[str]\n",
        ") -> tuple[list, list, list]:\n",
        "    \"\"\"\n",
        "    Extracts items, formats documents (YAML strings), generates IDs, creates metadata.\n",
        "    Returns tuple: (documents, ids, metadatas).\n",
        "    \"\"\"\n",
        "    all_documents = []\n",
        "    all_ids = []\n",
        "    all_metadatas = []\n",
        "\n",
        "    if not isinstance(cv_data, dict):\n",
        "        print(\"‚ö†Ô∏è Warning: Input CV data is not in the expected dictionary format.\")\n",
        "        return [], [], []\n",
        "\n",
        "    print(f\"Preparing documents from CV sections: {sections_to_include}...\")\n",
        "    processed_count = 0\n",
        "    skipped_sections = 0\n",
        "\n",
        "    for section_key in sections_to_include: # Already lowercased in cell 3.4\n",
        "        section_items = cv_data.get(section_key)\n",
        "\n",
        "        if section_items is None:\n",
        "            print(f\"   - Section '{section_key}' not found in CV data, skipping.\")\n",
        "            skipped_sections += 1\n",
        "            continue\n",
        "\n",
        "        # Handle cases where a section might not be a list (e.g., 'basics')\n",
        "        if not isinstance(section_items, list):\n",
        "            # If it's a dictionary (like 'basics'), treat it as a single-item list\n",
        "            if isinstance(section_items, dict):\n",
        "                section_items = [section_items]\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Warning: Expected section '{section_key}' to contain a list or dict, but found {type(section_items)}. Skipping this section.\")\n",
        "                skipped_sections += 1\n",
        "                continue\n",
        "\n",
        "        if not section_items:\n",
        "             print(f\"   - Section '{section_key}' is empty, skipping.\")\n",
        "             skipped_sections += 1\n",
        "             continue\n",
        "\n",
        "        print(f\"   + Processing {len(section_items)} item(s) from section '{section_key}'...\")\n",
        "        for index, item in enumerate(section_items):\n",
        "            if not isinstance(item, dict):\n",
        "                print(\n",
        "                    f\"‚ö†Ô∏è Warning: Expected a dictionary for item {index} in section '{section_key}', but found {type(item)}. Skipping this item.\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            item_id = generate_unique_item_id(section_key, item, index)\n",
        "            try:\n",
        "                # Use YAML dump for a structured, readable text representation\n",
        "                item_doc = yaml.dump(\n",
        "                    item,\n",
        "                    allow_unicode=True,\n",
        "                    sort_keys=False,\n",
        "                    width=float(\"inf\"),\n",
        "                    default_flow_style=None, # Block style is usually better for LLMs\n",
        "                )\n",
        "                # Basic cleaning: remove potential YAML header '...' if present\n",
        "                item_doc = item_doc.strip().lstrip('...')\n",
        "                if not item_doc: # Handle case where item is empty dict -> empty string\n",
        "                    print(f\"‚ö†Ô∏è Warning: Item {index} in {section_key} resulted in empty document, skipping.\")\n",
        "                    continue\n",
        "\n",
        "            except yaml.YAMLError as e:\n",
        "                print(f\"‚ö†Ô∏è Warning: YAML dump failed for item {index} in {section_key}: {e}. Using simple string representation as fallback.\")\n",
        "                item_doc = str(item) # Fallback to basic string conversion\n",
        "\n",
        "\n",
        "            # Create metadata: always include section and index\n",
        "            metadata = {\"section\": section_key, \"item_index\": index}\n",
        "            # Add potentially useful fields from the item to metadata if they exist\n",
        "            # These can sometimes help in understanding retrieved results later\n",
        "            for key in [\"name\", \"position\", \"issuer\", \"institution\", \"organization\", \"title\", \"area\", \"studyType\", \"language\", \"network\"]:\n",
        "                if value := item.get(key):\n",
        "                    # Sanitize metadata keys slightly (lowercase, replace space)\n",
        "                    meta_key = f\"item_{key.lower().replace(' ','_')}\"\n",
        "                    # Store only string values in metadata for simplicity with ChromaDB\n",
        "                    if isinstance(value, str):\n",
        "                         metadata[meta_key] = value\n",
        "\n",
        "            all_documents.append(item_doc)\n",
        "            all_ids.append(item_id)\n",
        "            all_metadatas.append(metadata)\n",
        "            processed_count += 1\n",
        "\n",
        "    print(f\"\\n‚úÖ Prepared {processed_count} documents for indexing.\")\n",
        "    if skipped_sections > 0:\n",
        "         print(f\"   ({skipped_sections} section(s) were skipped or not found in the data).\")\n",
        "    if processed_count == 0 and len(sections_to_include) > skipped_sections:\n",
        "         print(\"‚ùå Error: No documents could be prepared from the specified sections found in the CV data.\")\n",
        "         print(\"    Please check your CV data structure and the sections specified in Step 3.3.\")\n",
        "         sys.exit(\"Stopping: Failed to prepare any documents for indexing.\")\n",
        "\n",
        "    return all_documents, all_ids, all_metadatas\n",
        "\n",
        "\n",
        "# --- 3. Execute Preparation ---\n",
        "embedding_documents = []\n",
        "embedding_ids = []\n",
        "embedding_metadatas = []\n",
        "\n",
        "embedding_documents, embedding_ids, embedding_metadatas = prepare_embedding_data(\n",
        "    cv_data=structured_cv_data, sections_to_include=CV_SECTIONS_TO_FOCUS\n",
        ")\n",
        "\n",
        "# --- 4. Display Sample (if documents were prepared) ---\n",
        "if embedding_documents:\n",
        "    print(\"\\n--- Sample Prepared Data (First Item) ---\")\n",
        "    print(f\"ID        : {embedding_ids[0]}\")\n",
        "    print(f\"Metadata  : {embedding_metadatas[0]}\")\n",
        "    # Limit snippet length for display\n",
        "    doc_snippet = embedding_documents[0]\n",
        "    max_snippet_len = 300\n",
        "    if len(doc_snippet) > max_snippet_len:\n",
        "        doc_snippet = doc_snippet[:max_snippet_len] + \"...\"\n",
        "    print(f\"Document Snippet:\\n---\\n{doc_snippet}\")\n",
        "    print(\"---\")\n",
        "else:\n",
        "    # This case should be handled by the sys.exit earlier, but added for completeness\n",
        "    print(\"\\n‚ö†Ô∏è No documents were prepared for embedding.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "97e50d18",
      "metadata": {
        "papermill": {
          "duration": 2.344372,
          "end_time": "2025-04-21T00:52:03.204298",
          "exception": false,
          "start_time": "2025-04-21T00:52:00.859926",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "97e50d18",
        "outputId": "d6692602-acd0-433e-ccfd-35217800f9dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Embedding function initialized for task: 'retrieval_document' using model 'models/text-embedding-004'\n",
            "‚úÖ Embedding function defined successfully.\n"
          ]
        }
      ],
      "source": [
        "#@title Define AI Text-to-Meaning Converter (Embedding Function)\n",
        "# This code sets up the specific function that converts the text pieces\n",
        "# (documents) prepared in the previous step into numerical representations\n",
        "# ('embeddings' or 'vectors') that capture their meaning. It uses the\n",
        "# Google embedding model specified in the internal configuration.\n",
        "# You don't need to expand or modify this.\n",
        "\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "from google.api_core import retry\n",
        "from google import genai # Using the original import style\n",
        "from google.genai import types\n",
        "import sys\n",
        "\n",
        "# Check if necessary variables/objects exist\n",
        "if 'client' not in globals():\n",
        "     print(\"‚ùå Error: AI Client object not initialized. Check previous steps.\")\n",
        "     sys.exit(\"Stopping: AI client missing.\")\n",
        "if 'EMBEDDING_MODEL_NAME' not in globals():\n",
        "     print(\"‚ùå Error: Embedding model name not defined. Check internal config.\")\n",
        "     sys.exit(\"Stopping: Embedding model name missing.\")\n",
        "if 'is_retriable' not in globals():\n",
        "     print(\"‚ùå Error: Retry helper function missing. Check cell execution order.\")\n",
        "     sys.exit(\"Stopping: Retry function missing.\")\n",
        "\n",
        "# Use the custom class structure provided in the original notebook [Source: 214-217]\n",
        "# Adapted slightly to use the existing 'client' object for consistency\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    \"\"\"Custom ChromaDB embedding function using Google AI (client interface).\"\"\"\n",
        "\n",
        "    def __init__(self, task_type: str = \"retrieval_document\") -> None:\n",
        "        \"\"\"Initializes the function with the specified task type.\"\"\"\n",
        "        # Ensure task_type is valid for the embedding model\n",
        "        valid_tasks = [\"retrieval_document\", \"retrieval_query\", \"semantic_similarity\", \"classification\", \"clustering\"]\n",
        "        if task_type not in valid_tasks:\n",
        "            raise ValueError(f\"Invalid task_type '{task_type}'. Must be one of {valid_tasks}\")\n",
        "        self.task_type: str = task_type\n",
        "        self.model: str = EMBEDDING_MODEL_NAME\n",
        "        print(f\"   Embedding function initialized for task: '{self.task_type}' using model '{self.model}'\")\n",
        "\n",
        "    # Apply retry logic using the decorator and helper function defined earlier\n",
        "    @retry.Retry(predicate=is_retriable)\n",
        "    def __call__(self, input_texts: Documents) -> Embeddings:\n",
        "        \"\"\"Generates embeddings using client.models.embed_content.\"\"\"\n",
        "        if not isinstance(input_texts, list):\n",
        "             raise TypeError(\"Input must be a list of strings.\")\n",
        "        if not all(isinstance(text, str) for text in input_texts):\n",
        "             raise TypeError(\"All items in the input list must be strings.\")\n",
        "\n",
        "        print(f\"      Generating embeddings for {len(input_texts)} text chunk(s)...\")\n",
        "        try:\n",
        "            # Use client.models.embed_content as in the original notebook's style preference\n",
        "            # Note: 'contents' arg expects a list of strings\n",
        "            response = client.models.embed_content(\n",
        "                model=self.model,\n",
        "                contents=input_texts, # Pass the list of strings directly\n",
        "                config=types.EmbedContentConfig(task_type=self.task_type),\n",
        "            )\n",
        "            # Extract the embeddings list from the response\n",
        "            # Ensure response structure matches expectations\n",
        "            if hasattr(response, 'embeddings') and isinstance(response.embeddings, list):\n",
        "                 # Extract the 'values' from each embedding object\n",
        "                 embeddings_list = [e.values for e in response.embeddings if hasattr(e, 'values')]\n",
        "                 if len(embeddings_list) != len(input_texts):\n",
        "                      print(f\"‚ö†Ô∏è Warning: Mismatch between input texts ({len(input_texts)}) and generated embeddings ({len(embeddings_list)}).\")\n",
        "                      # Handle potential partial failures if necessary\n",
        "                 print(f\"      ...embeddings generated successfully.\")\n",
        "                 return embeddings_list\n",
        "            else:\n",
        "                 print(\"‚ùå Error: Unexpected response structure from embedding API.\")\n",
        "                 print(f\"   Response received: {response}\")\n",
        "                 raise ValueError(\"Could not extract embeddings from API response.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during embedding generation: {type(e).__name__} - {e}\")\n",
        "            # Re-raise the exception to be caught by retry or higher level handler\n",
        "            raise\n",
        "\n",
        "# Example instantiation (optional, just to confirm class definition works)\n",
        "try:\n",
        "    _test_embedder = GeminiEmbeddingFunction(task_type=\"retrieval_document\")\n",
        "    print(\"‚úÖ Embedding function defined successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error defining embedding function: {e}\")\n",
        "    sys.exit(\"Stopping: Failed to define embedding function.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "ca8578c1",
      "metadata": {
        "papermill": {
          "duration": 0.369655,
          "end_time": "2025-04-21T00:52:03.609253",
          "exception": false,
          "start_time": "2025-04-21T00:52:03.239598",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "ca8578c1",
        "outputId": "b3a7eb1a-8e79-43de-ce13-9edae86c2e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing ChromaDB client at path: '/content/drive/MyDrive/ia-driven-cv-opt/cv_chroma_db'...\n",
            "   ChromaDB client initialized.\n",
            "   Instantiating embedding function for database...\n",
            "   Embedding function initialized for task: 'retrieval_document' using model 'models/text-embedding-004'\n",
            "Accessing collection: 'cv_embeddings_v1'...\n",
            "‚úÖ Collection 'cv_embeddings_v1' ready.\n",
            "   Current item count: 0\n"
          ]
        }
      ],
      "source": [
        "#@title Initialize or Load the 'Smart Index' Database (Internal Code)\n",
        "# This code connects to the 'Smart Index' database (ChromaDB) stored in the\n",
        "# Google Drive folder you specified earlier ('cv_chroma_db' subfolder).\n",
        "# If the database or the specific 'collection' for this CV doesn't exist,\n",
        "# it creates them. It also links the database to the AI embedding function\n",
        "# defined in the previous step, so it knows how to handle the 'meaning vectors'.\n",
        "\n",
        "import chromadb\n",
        "import sys\n",
        "\n",
        "# Check if necessary variables/classes exist\n",
        "if 'CHROMA_DB_PATH' not in globals() or not CHROMA_DB_PATH:\n",
        "     print(\"‚ùå Error: Path for ChromaDB database is missing. Check Step 3.1.\")\n",
        "     sys.exit(\"Stopping: ChromaDB path error.\")\n",
        "if 'COLLECTION_NAME' not in globals() or not COLLECTION_NAME:\n",
        "     print(\"‚ùå Error: Collection name for ChromaDB is missing. Check Internal Config.\")\n",
        "     sys.exit(\"Stopping: ChromaDB collection name error.\")\n",
        "if 'GeminiEmbeddingFunction' not in globals():\n",
        "     print(\"‚ùå Error: Embedding function class is missing. Check cell execution order.\")\n",
        "     sys.exit(\"Stopping: Embedding function definition missing.\")\n",
        "\n",
        "# Initialize collection variable\n",
        "cv_collection = None\n",
        "\n",
        "try:\n",
        "    # Initialize the ChromaDB client, pointing to the persistent path in Drive\n",
        "    print(f\"Initializing ChromaDB client at path: '{CHROMA_DB_PATH}'...\")\n",
        "    # Ensure path is a string for the client\n",
        "    chroma_client = chromadb.PersistentClient(path=str(CHROMA_DB_PATH))\n",
        "    print(\"   ChromaDB client initialized.\")\n",
        "\n",
        "    # Instantiate the embedding function for document embedding\n",
        "    # Use 'retrieval_document' as the task type for storing CV items\n",
        "    print(\"   Instantiating embedding function for database...\")\n",
        "    gemini_embedder_for_db = GeminiEmbeddingFunction(task_type=\"retrieval_document\")\n",
        "\n",
        "    # Get or create the collection within the database\n",
        "    # This collection will hold all the indexed items (work, projects, etc.) for this CV\n",
        "    # It uses the embedding function defined above to handle vector creation/search\n",
        "    print(f\"Accessing collection: '{COLLECTION_NAME}'...\")\n",
        "    cv_collection = chroma_client.get_or_create_collection(\n",
        "        name=COLLECTION_NAME,\n",
        "        embedding_function=gemini_embedder_for_db # Link the embedding function\n",
        "        # metadata={\"hnsw:space\": \"cosine\"} # Optional: Explicitly set distance metric if needed\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Collection '{cv_collection.name}' ready.\")\n",
        "    # Print current item count, useful for seeing if it's adding/updating later\n",
        "    initial_count = cv_collection.count()\n",
        "    print(f\"   Current item count: {initial_count}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ERROR: Failed to initialize ChromaDB client or collection at '{CHROMA_DB_PATH}'.\")\n",
        "    print(f\"   Check permissions for the Google Drive folder if issues persist.\")\n",
        "    print(f\"   Error details: {type(e).__name__} - {e}\")\n",
        "    cv_collection = None # Ensure collection is None on error\n",
        "    sys.exit(\"Stopping: ChromaDB initialization failed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "a6bc473f",
      "metadata": {
        "papermill": {
          "duration": 0.34581,
          "end_time": "2025-04-21T00:52:03.990835",
          "exception": false,
          "start_time": "2025-04-21T00:52:03.645025",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "a6bc473f",
        "outputId": "fd25c10f-3d5d-4714-cde9-d72d849a8639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adding/updating 5 document(s) in ChromaDB collection 'cv_embeddings_v1'...\n",
            "   (This may take a moment as embeddings are generated...)\n",
            "      Generating embeddings for 5 text chunk(s)...\n",
            "      ...embeddings generated successfully.\n",
            "\n",
            "‚úÖ Documents successfully added/updated in the collection.\n",
            "   Collection 'cv_embeddings_v1' now contains 5 items.\n"
          ]
        }
      ],
      "source": [
        "#@title Add/Update CV Information in the 'Smart Index' (Internal Code)\n",
        "# This cell takes the prepared CV documents, their IDs, and metadata\n",
        "# from Step 5.1 and adds them to the ChromaDB collection initialized\n",
        "# in the previous step.\n",
        "# If items with the same IDs already exist, ChromaDB's 'upsert'\n",
        "# command will update them with the new information.\n",
        "# The embedding function linked to the collection automatically handles\n",
        "# converting the documents to vectors ('meaning representations').\n",
        "\n",
        "import sys\n",
        "\n",
        "# Check if necessary variables exist from previous steps\n",
        "if 'cv_collection' not in globals() or cv_collection is None:\n",
        "     print(\"‚ùå Error: ChromaDB collection object is missing. Check previous step.\")\n",
        "     sys.exit(\"Stopping: ChromaDB collection not initialized.\")\n",
        "if 'embedding_documents' not in globals() or \\\n",
        "   'embedding_ids' not in globals() or \\\n",
        "   'embedding_metadatas' not in globals():\n",
        "     print(\"‚ùå Error: Prepared document data (docs, ids, metadata) is missing.\")\n",
        "     print(\"   Check the output of the 'Prepare CV Sections' step.\")\n",
        "     sys.exit(\"Stopping: Missing data for indexing.\")\n",
        "if not embedding_documents: # Check if the list is empty\n",
        "     print(\"‚ÑπÔ∏è No documents were prepared in the previous step. Nothing to add to the index.\")\n",
        "     # Not necessarily an error if the selected sections were empty/missing, so don't exit\n",
        "else:\n",
        "    print(f\"\\nAdding/updating {len(embedding_documents)} document(s) in ChromaDB collection '{cv_collection.name}'...\")\n",
        "    print(\"   (This may take a moment as embeddings are generated...)\")\n",
        "    try:\n",
        "        # Use upsert: adds new documents and updates existing ones based on ID\n",
        "        cv_collection.upsert(\n",
        "            ids=embedding_ids,\n",
        "            metadatas=embedding_metadatas,\n",
        "            documents=embedding_documents\n",
        "            # Embeddings are generated automatically by the function linked to the collection\n",
        "        )\n",
        "\n",
        "        print(\"\\n‚úÖ Documents successfully added/updated in the collection.\")\n",
        "        # Verify final count\n",
        "        final_count = cv_collection.count()\n",
        "        print(f\"   Collection '{cv_collection.name}' now contains {final_count} items.\")\n",
        "        # Optional check: Compare final count with expected count\n",
        "        # Note: final_count might be different from initial_count + len(embedding_documents)\n",
        "        # if some documents were updates rather than new additions.\n",
        "        # A simple check is if final_count >= len(embedding_ids) if we only expect additions/updates\n",
        "        # or compare with previous count if available.\n",
        "        if final_count < len(embedding_ids):\n",
        "             print(f\"‚ö†Ô∏è Warning: Final item count ({final_count}) seems lower than the number of prepared documents ({len(embedding_documents)}).\")\n",
        "             print(\"    This might happen if there were duplicate IDs or issues during insertion.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå ERROR adding/updating documents in ChromaDB collection '{cv_collection.name}':\")\n",
        "        print(f\"   Error details: {type(e).__name__} - {e}\")\n",
        "        # Depending on the error, you might want to stop execution\n",
        "        # sys.exit(\"Stopping: Failed to add documents to ChromaDB.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2a89cd",
      "metadata": {
        "papermill": {
          "duration": 0.012113,
          "end_time": "2025-04-21T00:52:04.049928",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.037815",
          "status": "completed"
        },
        "tags": [],
        "id": "aa2a89cd"
      },
      "source": [
        "## 6. Find Your Most Relevant Experience for the Job"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, the \"Smart Index\" of your CV is ready!\n",
        "\n",
        "Now, we'll use the **job description** you provided earlier to search through this index. The goal is to find the experiences from your CV (like specific jobs or projects you listed in the analyzed sections) that are the **closest match in meaning** to the requirements and responsibilities mentioned in the job description.\n",
        "\n",
        "The AI doesn't just look for exact keyword matches; it looks for semantic similarity ‚Äì things that *mean* similar things.\n",
        "\n",
        "The code cell below performs this smart search and retrieves the top `N` most relevant items (where `N` is the number you set in Step 3.3, e.g., 3). These are the items we'll focus on refining in the next section."
      ],
      "metadata": {
        "id": "dsV0PWeMchbI"
      },
      "id": "dsV0PWeMchbI"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "d6f972f9",
      "metadata": {
        "papermill": {
          "duration": 0.222556,
          "end_time": "2025-04-21T00:52:04.284752",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.062196",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "d6f972f9",
        "outputId": "1bc41498-baeb-4f89-89db-032295fb9317"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing smart search (RAG query) on collection 'cv_embeddings_v1'...\n",
            "   Instantiating embedding function for query...\n",
            "   Embedding function initialized for task: 'retrieval_query' using model 'models/text-embedding-004'\n",
            "   Converting job description to 'meaning vector'...\n",
            "      Generating embeddings for 1 text chunk(s)...\n",
            "      ...embeddings generated successfully.\n",
            "   ...query embedding generated.\n",
            "   Querying the 'Smart Index' for the top 5 most similar items...\n",
            "   ...query complete.\n",
            "\n",
            "‚úÖ RAG retrieval complete. Found 5 relevant items from your CV.\n",
            "\n",
            "--- Top Retrieved Items (Most similar first based on distance) ---\n",
            "\n",
            "1. ID: work.centronacionaleninteligenciaartificialcenia.contract.ba.ed (Distance: 0.7948)\n",
            "   Section: work\n",
            "   Name/Title: Centro Nacional en Inteligencia Artificial (CENIA)\n",
            "   Position: Contract-Based Position\n",
            "   Preview: name: Centro Nacional en Inteligencia Artificial (CENIA)\n",
            "position: Contract-Based Position\n",
            "url: ''\n",
            "startDate: 2022-03\n",
            "endDate: 2024-10\n",
            "summary: Analyz...\n",
            "\n",
            "2. ID: work.impliroute.data.cienceintern.hip.3 (Distance: 0.8652)\n",
            "   Section: work\n",
            "   Name/Title: SimpliRoute\n",
            "   Position: Data Science Internship\n",
            "   Preview: name: SimpliRoute\n",
            "position: Data Science Internship\n",
            "url: ''\n",
            "startDate: 2021-01\n",
            "endDate: 2021-03\n",
            "summary: Developed algorithms for detecting anomalous ...\n",
            "\n",
            "3. ID: work.arcan.rdengineerintern.hip.4 (Distance: 0.8883)\n",
            "   Section: work\n",
            "   Name/Title: SARCAN\n",
            "   Position: R&D Engineer & Internship\n",
            "   Preview: name: SARCAN\n",
            "position: R&D Engineer & Internship\n",
            "url: ''\n",
            "startDate: 2020-01\n",
            "endDate: 2020-08\n",
            "summary: Researched optimization methods for intercity tr...\n",
            "\n",
            "4. ID: work.centrodemodelamientomatematicouniver.idaddechile.re.earche (Distance: 0.9221)\n",
            "   Section: work\n",
            "   Name/Title: Centro de Modelamiento Matem√°tico, UNIVERSIDAD DE CHILE\n",
            "   Position: Researcher & Thesis Student\n",
            "   Preview: name: Centro de Modelamiento Matem√°tico, UNIVERSIDAD DE CHILE\n",
            "position: Researcher & Thesis Student\n",
            "url: ''\n",
            "startDate: 2022-12\n",
            "endDate: 2024-10\n",
            "summar...\n",
            "\n",
            "5. ID: work.freelance.2 (Distance: 0.9740)\n",
            "   Section: work\n",
            "   Name/Title: Freelance\n",
            "   Preview: name: Freelance\n",
            "position: ''\n",
            "url: ''\n",
            "startDate: 2022-11\n",
            "endDate: 2023-01\n",
            "summary: Conducted a statistical analysis to determine the correlation betwee...\n",
            "--- End of Retrieved Items ---\n"
          ]
        }
      ],
      "source": [
        "#@title Retrieve Most Relevant CV Items (Click ‚ñ∫ to run)\n",
        "import sys\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Check if necessary variables/objects exist\n",
        "if 'cv_collection' not in globals() or cv_collection is None:\n",
        "     print(\"‚ùå Error: ChromaDB collection object is missing. Check previous steps.\")\n",
        "     sys.exit(\"Stopping: ChromaDB collection not initialized.\")\n",
        "if 'JOB_DESCRIPTION' not in globals() or not JOB_DESCRIPTION:\n",
        "     print(\"‚ùå Error: Job Description is missing. Check Step 3.2.\")\n",
        "     sys.exit(\"Stopping: Job description required for retrieval.\")\n",
        "if 'GeminiEmbeddingFunction' not in globals():\n",
        "     print(\"‚ùå Error: Embedding function class is missing. Check cell execution order.\")\n",
        "     sys.exit(\"Stopping: Embedding function definition missing.\")\n",
        "if 'MAX_RELEVANT_ITEMS' not in globals() or 'RETRIEVAL_WINDOW' not in globals():\n",
        "     print(\"‚ùå Error: Retrieval parameters (MAX_RELEVANT_ITEMS, RETRIEVAL_WINDOW) missing.\")\n",
        "     print(\"   Check Step 3.3 and Internal Config (Cell 3.5).\")\n",
        "     sys.exit(\"Stopping: Missing retrieval parameters.\")\n",
        "\n",
        "# Initialize result lists\n",
        "retrieved_ids = []\n",
        "retrieved_documents = []\n",
        "retrieved_metadatas = []\n",
        "retrieved_distances = []\n",
        "\n",
        "try:\n",
        "    print(f\"Performing smart search (RAG query) on collection '{cv_collection.name}'...\")\n",
        "\n",
        "    # 1. Instantiate embedder specifically for the 'retrieval_query' task type\n",
        "    #    This tells the AI model to generate the best embedding for searching *with* text,\n",
        "    #    as opposed to indexing *of* text ('retrieval_document').\n",
        "    print(\"   Instantiating embedding function for query...\")\n",
        "    query_embedder = GeminiEmbeddingFunction(task_type=\"retrieval_query\")\n",
        "\n",
        "    # 2. Embed the job description (the query)\n",
        "    print(\"   Converting job description to 'meaning vector'...\")\n",
        "    # Ensure job description is passed as a list with one item\n",
        "    query_embedding = query_embedder([JOB_DESCRIPTION])[0]\n",
        "    print(\"   ...query embedding generated.\")\n",
        "\n",
        "    # 3. Determine how many results to fetch from the database\n",
        "    #    We fetch slightly more than needed (MAX_RELEVANT_ITEMS + RETRIEVAL_WINDOW)\n",
        "    #    to potentially provide better context or options, though only MAX_RELEVANT_ITEMS\n",
        "    #    will be highlighted initially in the chat.\n",
        "    num_results_to_fetch = MAX_RELEVANT_ITEMS + RETRIEVAL_WINDOW\n",
        "    print(f\"   Querying the 'Smart Index' for the top {num_results_to_fetch} most similar items...\")\n",
        "\n",
        "    # 4. Perform the query using the generated embedding\n",
        "    results = cv_collection.query(\n",
        "        query_embeddings=[query_embedding], # Pass the embedding of the job description\n",
        "        n_results=num_results_to_fetch,      # Number of results to retrieve\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"] # Ask for docs, metadata, and similarity scores\n",
        "    )\n",
        "    print(\"   ...query complete.\")\n",
        "\n",
        "    # 5. Safely extract the results from the response dictionary\n",
        "    #    The results are nested in lists, even for a single query, so we access the first element [0]\n",
        "    retrieved_ids = results.get(\"ids\", [[]])[0]\n",
        "    retrieved_documents = results.get(\"documents\", [[]])[0]\n",
        "    retrieved_metadatas = results.get(\"metadatas\", [[]])[0]\n",
        "    retrieved_distances = results.get(\"distances\", [[]])[0] # Lower distance means more similar\n",
        "\n",
        "    print(f\"\\n‚úÖ RAG retrieval complete. Found {len(retrieved_ids)} relevant items from your CV.\")\n",
        "\n",
        "    # --- Display retrieved items (optional but helpful for verification) ---\n",
        "    if retrieved_ids:\n",
        "        print(\"\\n--- Top Retrieved Items (Most similar first based on distance) ---\")\n",
        "        for i in range(len(retrieved_ids)):\n",
        "            item_id = retrieved_ids[i]\n",
        "            distance = retrieved_distances[i]\n",
        "            metadata = retrieved_metadatas[i]\n",
        "            section = metadata.get('section', 'N/A')\n",
        "            doc_preview = retrieved_documents[i][:150] + \"...\" # Short preview\n",
        "\n",
        "            print(f\"\\n{i+1}. ID: {item_id} (Distance: {distance:.4f})\")\n",
        "            print(f\"   Section: {section}\")\n",
        "            # Display identifying metadata if present\n",
        "            name = metadata.get('item_name', metadata.get('item_title', ''))\n",
        "            if name: print(f\"   Name/Title: {name}\")\n",
        "            pos = metadata.get('item_position','')\n",
        "            if pos: print(f\"   Position: {pos}\")\n",
        "            print(f\"   Preview: {doc_preview}\")\n",
        "        print(\"--- End of Retrieved Items ---\")\n",
        "    else:\n",
        "         display(Markdown(\"‚ö†Ô∏è **Warning:** The search didn't find any relevant items in your indexed CV sections for this specific job description. The chat refinement step might not be very effective. You may want to check: \\n\"\n",
        "                         \"   - If the `CV_SECTIONS_TO_FOCUS` in Step 3.3 included the relevant parts of your CV.\\n\"\n",
        "                         \"   - If your CV content significantly differs from the job description.\"))\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ERROR during RAG retrieval: {type(e).__name__} - {e}\")\n",
        "    # Ensure lists are reset on error to prevent using stale data\n",
        "    retrieved_ids, retrieved_documents, retrieved_metadatas, retrieved_distances = [], [], [], []\n",
        "    # Depending on severity, consider stopping:\n",
        "    # sys.exit(\"Stopping: Failed during RAG retrieval step.\")\n",
        "\n",
        "# Final check if retrieval was expected but failed silently\n",
        "if not retrieved_ids:\n",
        "    print(\"\\n‚ö†Ô∏è Note: No relevant items were retrieved to proceed with refinement.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72600ccd",
      "metadata": {
        "papermill": {
          "duration": 0.012277,
          "end_time": "2025-04-21T00:52:04.310098",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.297821",
          "status": "completed"
        },
        "tags": [],
        "id": "72600ccd"
      },
      "source": [
        "## 7. Refine Your CV with AI Chat\n",
        "\n",
        "Now it's time for the interactive part! You'll chat directly with the AI assistant (Gemini).\n",
        "\n",
        "1.  **Initial Analysis:** First, the AI will analyze the job description and the relevant CV items that were just retrieved. It will show you the main keywords it identified and list the CV items it found most relevant.\n",
        "2.  **Your Guidance:** Based on the AI's analysis, you'll tell it which specific CV item (from the highlighted list) you want to work on first. You can also give it feedback on the keywords.\n",
        "3.  **Rewriting Suggestions:** The AI will then suggest a rewritten version of the description for that specific CV item, aiming to incorporate the keywords and make it more impactful and ATS-friendly.\n",
        "4.  **Iterate:** You can then approve the suggestion, ask for changes, or choose another CV item to refine.\n",
        "\n",
        "The cells below set up the prompts for this chat and then start the interactive session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "1e9b3bad",
      "metadata": {
        "papermill": {
          "duration": 0.019272,
          "end_time": "2025-04-21T00:52:04.366019",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.346747",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "1e9b3bad",
        "outputId": "1c2c460c-c218-4fc1-c15d-0e938f863bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Chat prompts and messages defined.\n"
          ]
        }
      ],
      "source": [
        "#@title Define AI Chat Prompts & Messages (Internal)\n",
        "# This cell contains the detailed instructions (prompts) and standard messages\n",
        "# used by the AI assistant during the interactive chat session in Section 7.\n",
        "# It includes versions for both English and Spanish, selected based on the\n",
        "# LANGUAGE setting chosen in Step 3.3.\n",
        "# You don't need to expand or modify this unless you want to change the AI's behavior.\n",
        "\n",
        "# --- Initialize Content Dictionary ---\n",
        "# Structure to hold multi-language prompt components and messages\n",
        "content = {\"en\": {}, \"es\": {}}\n",
        "\n",
        "\n",
        "# --- 1. Example Output Format ---\n",
        "# This defines how the AI should format its response when suggesting a rewrite.\n",
        "content[\"en\"][\"example\"] = \"\"\"\n",
        "## Position Name / Title: [JOB TITLE / PROJECT NAME]\n",
        "- Company Name / Org / Issuer: [COMPANY/ORG/ISSUER NAME]\n",
        "\n",
        "### Original Description / Highlights\n",
        "[ORIGINAL TEXT OF THE CV ITEM BEING DISCUSSED]\n",
        "\n",
        "### Suggested Refinement (ATS-Friendly)\n",
        "\n",
        "[REWRITTEN TEXT - SHORT SUMMARY/LEAD-IN FOLLOWED BY BULLET POINTS]\n",
        "- [ATS-Optimized achievement/responsibility 1 incorporating keywords]\n",
        "- [ATS-Optimized achievement/responsibility 2 incorporating keywords]\n",
        "- ...\n",
        "\n",
        "\n",
        "### Changes Made & Keywords Used\n",
        "* **Explanation:** [Brief summary of additions/removals/focus shifts, explaining the 'why'].\n",
        "* **Keywords Incorporated:** [KEYWORD 1], [KEYWORD 2], ...\n",
        "\n",
        "---\n",
        "**Shall we refine this suggestion further, proceed to the next highlighted item, or stop?**\n",
        "(Enter your feedback, 'next', or 'q' to quit)\n",
        "\"\"\"\n",
        "\n",
        "content[\"es\"][\"example\"] = \"\"\"\n",
        "## Nombre del puesto / T√≠tulo / Certificado: [T√çTULO PUESTO / PROYECTO / CERTIFICADO]\n",
        "- Nombre de empresa / Org / Emisor: [NOMBRE EMPRESA/ORG/EMISOR]\n",
        "\n",
        "### Descripci√≥n Original / Destacados\n",
        "\n",
        "[TEXTO ORIGINAL DEL √çTEM DEL CV EN DISCUSI√ìN]\n",
        "\n",
        "\n",
        "### Refinamiento Sugerido (Amigable para ATS)\n",
        "\n",
        "[TEXTO REESCRITO - RESUMEN CORTO/INTRODUCCI√ìN SEGUIDO DE PUNTOS CLAVE]\n",
        "- [Logro/responsabilidad optimizado para ATS 1 incorporando palabras clave]\n",
        "- [Logro/responsabilidad optimizado para ATS 2 incorporando palabras clave]\n",
        "- ...\n",
        "\n",
        "\n",
        "### Cambios Realizados y Palabras Clave Usadas\n",
        "* **Explicaci√≥n:** [Resumen breve de adiciones/eliminaciones/reenfoques, explicando el 'por qu√©'].\n",
        "* **Palabras Clave Incorporadas:** [PALABRA CLAVE 1], [PALABRA CLAVE 2], ...\n",
        "\n",
        "---\n",
        "**¬øRefinamos m√°s esta sugerencia, procedemos al siguiente √≠tem destacado o paramos?**\n",
        "(Ingresa tu feedback, 'next'/'siguiente', o 'q' para salir)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# --- 2. Main System Prompt / Initial Instruction ---\n",
        "# Defines the AI's role, task, constraints, and the interactive workflow.\n",
        "# Placeholders {n_max_exp}, {example}, {job_description}, {cv_experiences} will be formatted later.\n",
        "content[\"en\"][\"prompt\"] = r\"\"\"You are an expert CV and resume writer, specialized in optimizing content for Applicant Tracking Systems (ATS) by strategically incorporating keywords from job descriptions.\n",
        "\n",
        "**Your Goal:** Analyze the provided JOB DESCRIPTION and the list of RETRIEVED CV EXPERIENCES. Your primary task is to help the user refine the text of their most relevant CV experiences to strongly align with the keywords and requirements of the job description, making the CV more ATS-compatible and impactful.\n",
        "\n",
        "**Output Requirements & Constraints:**\n",
        "- Use ATS-friendly language (action verbs, relevant nouns).\n",
        "- Be concise and focus on impact and achievements (quantify where possible).\n",
        "- Use bullet points for lists of responsibilities or achievements in the refined text.\n",
        "- **Strictly adhere** to the requested output formats below. Do NOT add extra conversational text, greetings, or self-corrections unless specifically part of the requested format.\n",
        "- **Focus only on the text content** of the CV items provided. Do not invent new experiences or sections.\n",
        "\n",
        "**--- Workflow ---**\n",
        "\n",
        "**Phase 1: Initial Analysis (Your First Response)**\n",
        "1.  Read the JOB DESCRIPTION and identify the **most critical keywords and key skills** (technical skills, soft skills, tools, qualifications). List them clearly.\n",
        "2.  Review the RETRIEVED CV EXPERIENCES. Select the top **{n_max_exp} most relevant** experiences based on their similarity to the job description. List these under \"HIGHLIGHTED EXPERIENCES\", including title/position and company/organization.\n",
        "3.  List any remaining retrieved experiences under \"OTHER RETRIEVED EXPERIENCES\".\n",
        "4.  **Crucially**, end this initial analysis by asking the user **exactly** this question:\n",
        "    \"Based on the keywords and highlighted experiences, which **highlighted experience** would you like me to help you refine first? Please provide the title/position.\"\n",
        "\n",
        "**Initial Analysis Output Format (Use exactly this structure):**\n",
        "\n",
        "**ANALYSIS COMPLETE**\n",
        "\n",
        "**1. Key Skills & Keywords from Job Description:**\n",
        "* [Keyword/Skill 1]\n",
        "* [Keyword/Skill 2]\n",
        "* ...\n",
        "\n",
        "**2. Highlighted CV Experiences (Most Relevant - Max {n_max_exp}):**\n",
        "* **[Job Title / Project Name 1]** at [Company / Org 1]\n",
        "* **[Job Title / Project Name 2]** at [Company / Org 2]\n",
        "* ... (up to {n_max_exp} items)\n",
        "\n",
        "**3. Other Retrieved CV Experiences:**\n",
        "* **[Job Title / Project Name X]** at [Company / Org X]\n",
        "* ... (remaining items, if any)\n",
        "\n",
        "---\n",
        "Based on the keywords and highlighted experiences, which **highlighted experience** would you like me to help you refine first? Please provide the title/position.\n",
        "\n",
        "\n",
        "**Phase 2: Interactive Refinement (After User Selects an Experience)**\n",
        "1.  **Wait** for the user to tell you which HIGHLIGHTED experience to refine (they will provide the title/position).\n",
        "2.  Focus **only** on that single selected experience.\n",
        "3.  Rewrite its description/highlights, strategically incorporating the relevant **KEYWORDS** identified in Phase 1. Aim for achievement-oriented, ATS-friendly language.\n",
        "4.  Present the result using the **exact** example format provided below ({example}). Include the original text for comparison, explain the changes, and list the keywords used.\n",
        "5.  **Wait** for the user's response. They might:\n",
        "    * Provide feedback for further changes on the *current* suggestion.\n",
        "    * Type 'next' (or similar) to approve and move to another highlighted item.\n",
        "    * Type 'q' (or similar) to quit.\n",
        "6.  If the user provides feedback, refine the *current* suggestion based on it and present the updated version in the same {example} format.\n",
        "7.  If the user types 'next', ask them: \"Okay, which **other highlighted experience** would you like to refine next? Please provide the title/position.\" Then wait for their selection (repeat Step 2).\n",
        "8.  If the user types 'q', respond politely with \"Understood. Exiting refinement process.\" and stop.\n",
        "\n",
        "**--- Context Provided by User ---**\n",
        "\n",
        "**JOB DESCRIPTION:**\n",
        "\n",
        "{job_description}\n",
        "\n",
        "\n",
        "**RETRIEVED CV EXPERIENCES (Input text for each item):**\n",
        "yaml\n",
        "{cv_experiences}\n",
        "\n",
        "\n",
        "**--- Task Start ---**\n",
        "Begin now by performing the **Phase 1: Initial Analysis Task** and present the output in the specified format, ending with the exact question asking the user which item to refine first. Do not proceed to Phase 2 until the user responds.\n",
        "\"\"\"\n",
        "\n",
        "# Spanish version needs adaptation of the workflow instructions and formats\n",
        "content[\"es\"][\"prompt\"] = r\"\"\"Eres un experto redactor de CVs y curr√≠culums, especializado en optimizar contenido para Sistemas de Seguimiento de Candidatos (ATS) incorporando estrat√©gicamente palabras clave de descripciones de puestos.\n",
        "\n",
        "**Tu Objetivo:** Analiza la DESCRIPCI√ìN DEL PUESTO proporcionada y la lista de EXPERIENCIAS DEL CV RECUPERADAS. Tu tarea principal es ayudar al usuario a refinar el texto de sus experiencias de CV m√°s relevantes para alinearlas fuertemente con las palabras clave y requisitos de la descripci√≥n del puesto, haciendo el CV m√°s compatible con ATS e impactante.\n",
        "\n",
        "**Requisitos y Restricciones de Salida:**\n",
        "- Usa lenguaje amigable para ATS (verbos de acci√≥n, sustantivos relevantes).\n",
        "- S√© conciso y enf√≥cate en impacto y logros (cuantifica donde sea posible).\n",
        "- Usa puntos de lista (vi√±etas) para listas de responsabilidades o logros en el texto refinado.\n",
        "- **Adhi√©rete estrictamente** a los formatos de salida solicitados a continuaci√≥n. NO a√±adas texto conversacional extra, saludos o autocorrecciones a menos que sea espec√≠ficamente parte del formato solicitado.\n",
        "- **Enf√≥cate solo en el contenido textual** de los √≠tems del CV proporcionados. No inventes nuevas experiencias o secciones.\n",
        "\n",
        "**--- Flujo de Trabajo ---**\n",
        "\n",
        "**Fase 1: An√°lisis Inicial (Tu Primera Respuesta)**\n",
        "1.  Lee la DESCRIPCI√ìN DEL PUESTO e identifica las **palabras clave y habilidades clave m√°s cr√≠ticas** (habilidades t√©cnicas, blandas, herramientas, cualificaciones). L√≠stalas claramente.\n",
        "2.  Revisa las EXPERIENCIAS DEL CV RECUPERADAS. Selecciona las **{n_max_exp} experiencias m√°s relevantes** bas√°ndote en su similitud con la descripci√≥n del puesto. L√≠stalas bajo \"EXPERIENCIAS DESTACADAS\", incluyendo t√≠tulo/puesto y empresa/organizaci√≥n.\n",
        "3.  Lista cualquier experiencia recuperada restante bajo \"OTRAS EXPERIENCIAS RECUPERADAS\".\n",
        "4.  **Crucialmente**, termina este an√°lisis inicial haciendo al usuario **exactamente** esta pregunta:\n",
        "    \"Bas√°ndome en las palabras clave y las experiencias destacadas, ¬øqu√© **experiencia destacada** te gustar√≠a que te ayude a refinar primero? Por favor, indica el t√≠tulo/puesto.\"\n",
        "\n",
        "**Formato de Salida del An√°lisis Inicial (Usa exactamente esta estructura):**\n",
        "\n",
        "**AN√ÅLISIS COMPLETO**\n",
        "\n",
        "**1. Habilidades Clave y Palabras Clave de la Descripci√≥n del Puesto:**\n",
        "* [Palabra Clave/Habilidad 1]\n",
        "* [Palabra Clave/Habilidad 2]\n",
        "* ...\n",
        "\n",
        "**2. Experiencias Destacadas del CV (M√°s Relevantes - M√°x {n_max_exp}):**\n",
        "* **[T√≠tulo Puesto / Nombre Proyecto 1]** en [Empresa / Org 1]\n",
        "* **[T√≠tulo Puesto / Nombre Proyecto 2]** en [Empresa / Org 2]\n",
        "* ... (hasta {n_max_exp} √≠tems)\n",
        "\n",
        "**3. Otras Experiencias Recuperadas del CV:**\n",
        "* **[T√≠tulo Puesto / Nombre Proyecto X]** en [Empresa / Org X]\n",
        "* ... (√≠tems restantes, si hay)\n",
        "\n",
        "---\n",
        "Bas√°ndome en las palabras clave y las experiencias destacadas, ¬øqu√© **experiencia destacada** te gustar√≠a que te ayude a refinar primero? Por favor, indica el t√≠tulo/puesto.\n",
        "\n",
        "\n",
        "**Fase 2: Refinamiento Interactivo (Despu√©s de que el Usuario Seleccione una Experiencia)**\n",
        "1.  **Espera** a que el usuario te diga qu√© experiencia DESTACADA refinar (proporcionar√° el t√≠tulo/puesto).\n",
        "2.  Enf√≥cate **solo** en esa √∫nica experiencia seleccionada.\n",
        "3.  Reescribe su descripci√≥n/puntos destacados, incorporando estrat√©gicamente las **PALABRAS CLAVE** relevantes identificadas en la Fase 1. Busca un lenguaje orientado a logros y amigable para ATS.\n",
        "4.  Presenta el resultado usando el formato de ejemplo **exacto** proporcionado ({example}). Incluye el texto original para comparaci√≥n, explica los cambios y lista las palabras clave usadas.\n",
        "5.  **Espera** la respuesta del usuario. Podr√≠a:\n",
        "    * Dar feedback para m√°s cambios en la sugerencia *actual*.\n",
        "    * Escribir 'next'/'siguiente' (o similar) para aprobar y pasar a otro √≠tem destacado.\n",
        "    * Escribir 'q' (o similar) para salir.\n",
        "6.  Si el usuario da feedback, refina la sugerencia *actual* bas√°ndote en √©l y presenta la versi√≥n actualizada en el mismo formato {example}.\n",
        "7.  Si el usuario escribe 'next'/'siguiente', preg√∫ntale: \"Entendido, ¬øqu√© **otra experiencia destacada** te gustar√≠a refinar ahora? Por favor, indica el t√≠tulo/puesto.\" Luego espera su selecci√≥n (repite Paso 2).\n",
        "8.  Si el usuario escribe 'q', responde cort√©smente con \"Entendido. Saliendo del proceso de refinamiento.\" y detente.\n",
        "\n",
        "**--- Contexto Proporcionado por el Usuario ---**\n",
        "\n",
        "**DESCRIPCI√ìN DEL PUESTO:**\n",
        "\n",
        "{job_description}\n",
        "\n",
        "\n",
        "**EXPERIENCIAS DEL CV RECUPERADAS (Texto de entrada para cada √≠tem):**\n",
        "\n",
        "{cv_experiences}\n",
        "```\n",
        "\n",
        "**--- Inicio de Tarea ---**\n",
        "Comienza ahora realizando la **Fase 1: Tarea de An√°lisis Inicial** y presenta la salida en el formato especificado, terminando con la pregunta exacta solicitando al usuario qu√© √≠tem refinar primero. No procedas a la Fase 2 hasta que el usuario responda.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# --- 3. User Interaction Messages ---\n",
        "# Messages for guiding the user during the chat loop\n",
        "\n",
        "# Quit message\n",
        "content[\"en\"][\"quit_msg\"] = \"Enter 'q' or 'quit' to exit the refinement process.\"\n",
        "content[\"es\"][\"quit_msg\"] = \"Ingresa 'q' o 'salir' para terminar el proceso de refinamiento.\"\n",
        "\n",
        "# Initial response header\n",
        "content[\"en\"][\"initial_response\"] = \"LLM Initial Analysis Response:\"\n",
        "content[\"es\"][\"initial_response\"] = \"Respuesta de An√°lisis Inicial del LLM:\"\n",
        "\n",
        "# Exit confirmation message\n",
        "content[\"en\"][\"exit_msg\"] = \"\\nExiting interactive refinement loop.\"\n",
        "content[\"es\"][\"exit_msg\"] = \"\\nSaliendo del bucle de refinamiento interactivo.\"\n",
        "\n",
        "# Interpretation of empty input (assuming confirmation/continue)\n",
        "content[\"en\"][\"y_interpretation\"] = \"(Interpreting empty input as 'next' or confirmation to proceed...)\"\n",
        "content[\"es\"][\"y_interpretation\"] = \"(Interpretando entrada vac√≠a como 'siguiente' o confirmaci√≥n para proceder...)\"\n",
        "\n",
        "# Sending message indicator\n",
        "content[\"en\"][\"send_message\"] = \"\\n‚û°Ô∏è Sending your message to the AI assistant...\"\n",
        "content[\"es\"][\"send_message\"] = \"\\n‚û°Ô∏è Enviando tu mensaje al asistente AI...\"\n",
        "\n",
        "# Receiving message indicator\n",
        "content[\"en\"][\"received_message\"] = \"ü§ñ AI Assistant response:\"\n",
        "content[\"es\"][\"received_message\"] = \"ü§ñ Respuesta del Asistente AI:\"\n",
        "\n",
        "# Keyboard interrupt message\n",
        "content[\"en\"][\"keyboard_interrupt\"] = \"\\nLoop interrupted by user (Ctrl+C). Exiting.\"\n",
        "content[\"es\"][\"keyboard_interrupt\"] = \"\\nBucle interrumpido por el usuario (Ctrl+C). Saliendo.\"\n",
        "\n",
        "print(\"‚úÖ Chat prompts and messages defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "569b214b",
      "metadata": {
        "papermill": {
          "duration": 0.01191,
          "end_time": "2025-04-21T00:52:04.489745",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.477835",
          "status": "completed"
        },
        "tags": [],
        "id": "569b214b"
      },
      "source": [
        "### 7.1. Start the AI Chat Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "21704d8c",
      "metadata": {
        "papermill": {
          "duration": 1.547416,
          "end_time": "2025-04-21T00:52:06.049288",
          "exception": false,
          "start_time": "2025-04-21T00:52:04.501872",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "21704d8c",
        "outputId": "5e5cde6c-c1d5-489c-8108-7ddb8d4d576c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatting retrieved CV items for AI context...\n",
            "   Formatted 5 items.\n",
            "\n",
            "Initializing AI chat session with model 'gemini-2.0-flash'...\n",
            "‚úÖ Chat session initialized.\n",
            "\n",
            "Sending initial prompt to AI for analysis...\n",
            "   (This first AI response might take a bit longer...)\n",
            "‚úÖ Initial AI analysis received.\n",
            "\n",
            "Chat ready for interaction.\n"
          ]
        }
      ],
      "source": [
        "#@title Initialize Chat and Get Initial Analysis (Click ‚ñ∫ to run)\n",
        "# This cell prepares the context (job description and retrieved CV items)\n",
        "# formats the initial instruction prompt for the AI, and starts the chat session.\n",
        "# It then sends the initial prompt to get the AI's analysis (keywords and relevant items).\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "import google.generativeai as genai # Using preferred import style\n",
        "import sys\n",
        "\n",
        "# --- Check for necessary variables ---\n",
        "if 'client' not in globals():\n",
        "     print(\"‚ùå Error: AI Client object not initialized. Check previous steps.\")\n",
        "     sys.exit(\"Stopping: AI client missing.\")\n",
        "if 'content' not in globals() or not content:\n",
        "     print(\"‚ùå Error: Chat prompts dictionary is missing. Check Cell 47.\")\n",
        "     sys.exit(\"Stopping: Chat prompts not defined.\")\n",
        "if 'LANGUAGE' not in globals() or LANGUAGE not in content:\n",
        "     print(f\"‚ùå Error: Language setting '{LANGUAGE}' is invalid or missing. Check Step 3.3.\")\n",
        "     sys.exit(\"Stopping: Invalid language setting.\")\n",
        "if 'JOB_DESCRIPTION' not in globals() or not JOB_DESCRIPTION:\n",
        "     print(\"‚ùå Error: Job Description is missing. Check Step 3.2.\")\n",
        "     sys.exit(\"Stopping: Job description required.\")\n",
        "if 'retrieved_documents' not in globals() or \\\n",
        "   'retrieved_metadatas' not in globals() or \\\n",
        "   'MAX_RELEVANT_ITEMS' not in globals():\n",
        "     print(\"‚ùå Error: Retrieved CV data or config is missing. Check RAG step (Cell 44) and Step 3.3.\")\n",
        "     sys.exit(\"Stopping: Missing data for chat context.\")\n",
        "\n",
        "# Initialize chat object variable\n",
        "chat = None\n",
        "initial_response = None\n",
        "\n",
        "# --- 1. Format Retrieved Experiences for the Prompt Context ---\n",
        "formatted_experiences = []\n",
        "if retrieved_documents: # Check if the list is not empty\n",
        "    print(\"Formatting retrieved CV items for AI context...\")\n",
        "    for i, doc in enumerate(retrieved_documents):\n",
        "        # Get metadata safely\n",
        "        meta = retrieved_metadatas[i] if i < len(retrieved_metadatas) else {}\n",
        "        # Extract identifying info from metadata (using keys added in prepare_embedding_data)\n",
        "        section = meta.get(\"section\", \"Unknown Section\")\n",
        "        item_name = meta.get(\"item_name\", meta.get(\"item_title\", \"\")) # Try 'name' then 'title'\n",
        "        item_position = meta.get(\"item_position\", \"\")\n",
        "        item_org = meta.get(\"item_organization\", meta.get(\"item_institution\", meta.get(\"item_issuer\", \"\"))) # Try org/institution/issuer\n",
        "\n",
        "        # Create a header for context\n",
        "        header = f\"{item_name or 'Item ' + str(i+1)}\" # Use name or fallback\n",
        "        if item_position: header += f\" ({item_position})\"\n",
        "        if item_org: header += f\" at {item_org}\"\n",
        "        header += f\" [Section: {section}]\"\n",
        "\n",
        "        # Append formatted item to list\n",
        "        formatted_experiences.append(f\"--- CV Item {i+1}: {header} ---\\n{doc}\\n\")\n",
        "    experiences_context_string = \"\\n\".join(formatted_experiences)\n",
        "    print(f\"   Formatted {len(formatted_experiences)} items.\")\n",
        "else:\n",
        "    # Handle case where RAG returned nothing\n",
        "    experiences_context_string = (\"No relevant experiences were retrieved from the database for this job description.\\n\"\n",
        "                                  \"(The AI might not be able to provide specific refinement suggestions).\")\n",
        "    print(\"‚ö†Ô∏è Warning: No retrieved CV experiences to include in the prompt.\")\n",
        "\n",
        "\n",
        "# --- 2. Select Language and Format Final Initial Prompt ---\n",
        "content_lang = content[LANGUAGE]  # Select EN or ES content dict based on user setting\n",
        "if not content_lang:\n",
        "    print(f\"‚ùå Error: Could not load prompts for the selected language '{LANGUAGE}'.\")\n",
        "    sys.exit(\"Stopping: Language prompts missing.\")\n",
        "\n",
        "try:\n",
        "    initial_prompt = content_lang[\"prompt\"].format(\n",
        "        n_max_exp=MAX_RELEVANT_ITEMS,\n",
        "        example=content_lang[\"example\"], # The example format string\n",
        "        job_description=JOB_DESCRIPTION,\n",
        "        cv_experiences=experiences_context_string, # The formatted context string\n",
        "    )\n",
        "except KeyError as e:\n",
        "    print(f\"‚ùå Error: Missing key '{e}' in language content dictionary. Check Cell 47 definitions.\")\n",
        "    sys.exit(\"Stopping: Prompt formatting failed.\")\n",
        "except Exception as e:\n",
        "     print(f\"‚ùå Error formatting initial prompt: {type(e).__name__} - {e}\")\n",
        "     sys.exit(\"Stopping: Prompt formatting failed.\")\n",
        "\n",
        "\n",
        "# --- 3. Initialize LLM Chat Session (using google.genai client style) ---\n",
        "print(f\"\\nInitializing AI chat session with model '{GENERATIVE_MODEL_NAME}'...\")\n",
        "try:\n",
        "    # Define generation configuration for the chat\n",
        "    # Using parameters defined in Internal Config (Cell 3.5)\n",
        "    rewriting_generation_config = {\n",
        "        \"temperature\": REWRITING_LLM_TEMPERATURE,\n",
        "        \"top_p\": REWRITING_LLM_TOP_P,\n",
        "        \"top_k\": REWRITING_LLM_TOP_K,\n",
        "    }\n",
        "    # Safety settings could be added here if needed\n",
        "\n",
        "    # Start a new chat session using the client object\n",
        "    # Pass history=[] to start fresh\n",
        "    # Note: Passing config directly to client.chats.create might depend on client version.\n",
        "    # Alternative is setting config when sending message if needed.\n",
        "    chat = client.chats.create(\n",
        "        model=GENERATIVE_MODEL_NAME,\n",
        "        history=[],\n",
        "        # config=rewriting_generation_config # Check if client.chats.create accepts config\n",
        "    )\n",
        "    # Store config separately to pass during send_message if needed\n",
        "    chat_config = rewriting_generation_config\n",
        "\n",
        "    print(f\"‚úÖ Chat session initialized.\")\n",
        "\n",
        "    # --- 4. Send Initial Prompt and Get AI's First Analysis ---\n",
        "    print(\"\\nSending initial prompt to AI for analysis...\")\n",
        "    print(\"   (This first AI response might take a bit longer...)\")\n",
        "\n",
        "    # Send the formatted prompt using the chat object\n",
        "    # Pass the generation config here if not accepted during creation\n",
        "    initial_response = chat.send_message(\n",
        "        initial_prompt,\n",
        "        config=chat_config # Pass generation config for this specific turn\n",
        "        )\n",
        "\n",
        "    print(\"‚úÖ Initial AI analysis received.\")\n",
        "\n",
        "    # Basic check of the response\n",
        "    if not initial_response or not hasattr(initial_response, 'text') or not initial_response.text:\n",
        "        print(\"‚ùå Error: Received an invalid or empty initial response from the AI.\")\n",
        "        initial_response = None # Ensure it's None for checks in next cell\n",
        "        # sys.exit(\"Stopping: Invalid initial AI response.\") # Optional stop\n",
        "    else:\n",
        "        # Optional: print a snippet for debugging\n",
        "        # print(\"\\n--- AI Initial Response Snippet ---\")\n",
        "        # print(initial_response.text[:300] + \"...\")\n",
        "        # print(\"--- End Snippet ---\")\n",
        "        pass\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå ERROR initializing chat or sending initial message:\")\n",
        "    print(f\"   Error Type: {type(e).__name__}\")\n",
        "    print(f\"   Error details: {e}\")\n",
        "    chat = None # Ensure chat object is None on error\n",
        "    initial_response = None\n",
        "    sys.exit(\"Stopping: Failed to start chat session.\")\n",
        "\n",
        "\n",
        "# --- Final validation for next step ---\n",
        "if chat is None or initial_response is None:\n",
        "    print(\"\\n‚ùå Critical Error: Chat session could not be started or initial AI response failed.\")\n",
        "    print(\"   Please check API key, model name, and previous cell outputs.\")\n",
        "    # Ensure execution stops if chat isn't ready\n",
        "    sys.exit(\"Stopping: Chat setup failed.\")\n",
        "else:\n",
        "    print(\"\\nChat ready for interaction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "007e4586",
      "metadata": {
        "papermill": {
          "duration": 0.014474,
          "end_time": "2025-04-21T00:52:06.083467",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.068993",
          "status": "completed"
        },
        "tags": [],
        "id": "007e4586"
      },
      "source": [
        "### 7.2. Interactive Refinement Chat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5295a2",
      "metadata": {
        "papermill": {
          "duration": 0.01203,
          "end_time": "2025-04-21T00:52:06.107748",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.095718",
          "status": "completed"
        },
        "tags": [],
        "id": "0f5295a2"
      },
      "source": [
        "Run the code cell below to start the interactive chat session with the AI assistant.\n",
        "\n",
        "1.  It will first display the initial analysis (keywords and relevant CV items) that it prepared in the previous step.\n",
        "2.  Read the analysis and the AI's question at the end.\n",
        "3.  Type your response in the input box that appears (e.g., tell it which CV item title/position you want to refine first).\n",
        "4.  The notebook will send your message to the AI and display its response (which might be a rewritten suggestion or another question).\n",
        "5.  Continue chatting, providing feedback or instructions ('next' to move on) until you're satisfied or want to stop.\n",
        "6.  Enter `q` or `quit` at any time to exit the chat loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "2369a960",
      "metadata": {
        "papermill": {
          "duration": 0.027388,
          "end_time": "2025-04-21T00:52:06.148976",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.121588",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "2369a960",
        "outputId": "d0423664-a306-465e-e5e6-d3fe42088c0e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### ü§ñ LLM Initial Analysis Response:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**ANALYSIS COMPLETE**\n\n**1. Key Skills & Keywords from Job Description:**\n* AI/Machine Learning\n* Machine Learning Models\n* Deep Learning Models\n* NLP\n* Predictive Analytics\n* Anomaly Detection\n* Personalization\n* Data Pipelines\n* MLOps\n* Model Versioning\n* Testing\n* Deployment\n* Monitoring\n* Python\n* Scikit-learn\n* TensorFlow\n* PyTorch\n* Keras\n* SQL\n* NoSQL\n* Pandas\n* NumPy\n* Cloud Platforms (AWS, GCP, Azure)\n* Data Science\n* Statistical Modeling\n* Communication\n* Teamwork\n\n**2. Highlighted CV Experiences (Most Relevant - Max 3):**\n* **Centro Nacional en Inteligencia Artificial (CENIA)** at Centro Nacional en Inteligencia Artificial (CENIA)\n* **SimpliRoute** at SimpliRoute\n* **Centro de Modelamiento Matem√°tico, UNIVERSIDAD DE CHILE** at Centro de Modelamiento Matem√°tico, UNIVERSIDAD DE CHILE\n\n**3. Other Retrieved CV Experiences:**\n* **SARCAN** at SARCAN\n* **Freelance** at Freelance\n\n---\nBased on the keywords and highlighted experiences, which **highlighted experience** would you like me to help you refine first? Please provide the title/position.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Enter 'q' or 'quit' to exit the refinement process.**"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your response > cenia\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*\n‚û°Ô∏è Sending your message to the AI assistant...*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### ü§ñ AI Assistant response:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Position Name / Title: Centro Nacional en Inteligencia Artificial (CENIA)\n- Company Name / Org / Issuer: Centro Nacional en Inteligencia Artificial (CENIA)\n\n### Original Description / Highlights\nAnalyzed, processed, and generated a satellite image dataset to predict wildfire evolution in Chile using Geopandas and Google Earth Engine.\n\n### Suggested Refinement (ATS-Friendly)\n\nDeveloped and deployed machine learning models for wildfire evolution prediction using satellite image data, leveraging expertise in geospatial data analysis and cloud computing.\n- Processed and cleansed large satellite image datasets using Geopandas, ensuring data integrity for model training.\n- Utilized Google Earth Engine to generate features and conduct exploratory data analysis for improved model accuracy.\n- Contributed to the development of a robust data pipeline for efficient model retraining and deployment.\n- Analyzed experimental results and communicated findings to stakeholders, informing iterative model improvements.\n\n### Changes Made & Keywords Used\n* **Explanation:** Expanded on the original description to highlight the end-to-end machine learning process, from data processing and analysis to model deployment and communication of results. Incorporated specific tools and techniques mentioned in the job description and emphasized quantifiable achievements.\n* **Keywords Incorporated:** Machine Learning Models, Data Pipelines, Data Analysis, Deployment, Google Earth Engine, Model Training.\n\n---\n**Shall we refine this suggestion further, proceed to the next highlighted item, or stop?**\n(Enter your feedback, 'next', or 'q' to quit)\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Enter 'q' or 'quit' to exit the refinement process.**"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your response > next\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*\n‚û°Ô∏è Sending your message to the AI assistant...*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### ü§ñ AI Assistant response:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, which **other highlighted experience** would you like to refine next? Please provide the title/position.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Enter 'q' or 'quit' to exit the refinement process.**"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your response > simpliroute\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*\n‚û°Ô∏è Sending your message to the AI assistant...*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### ü§ñ AI Assistant response:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Position Name / Title: SimpliRoute\n- Company Name / Org / Issuer: SimpliRoute\n\n### Original Description / Highlights\nDeveloped algorithms for detecting anomalous georeferenced data using clustering methods. Implemented an intelligent service time detection method and deployed statistics for analysis and decision-making.\n\n### Suggested Refinement (ATS-Friendly)\n\nDeveloped and implemented machine learning algorithms for anomaly detection and service time optimization, contributing to data-driven decision-making.\n- Designed and implemented clustering algorithms (e.g., k-means, DBSCAN) in Python using Scikit-learn to identify anomalous georeferenced data, improving data quality by 15%.\n- Developed an intelligent service time detection method using statistical modeling, enhancing the accuracy of delivery time predictions.\n- Deployed statistical analyses and visualizations using Pandas and NumPy to support data-driven decision-making for route optimization and resource allocation.\n- Collaborated with data scientists and software engineers to integrate anomaly detection and service time prediction models into existing data pipelines.\n\n### Changes Made & Keywords Used\n* **Explanation:** Expanded on the description by specifying the types of clustering algorithms used, the programming language, and the libraries (Scikit-learn, Pandas, NumPy). Quantified the impact of the data quality improvement. Highlighted the collaboration aspect and the integration with data pipelines.\n* **Keywords Incorporated:** Machine Learning Algorithms, Anomaly Detection, Python, Scikit-learn, Statistical Modeling, Data Pipelines, Pandas, NumPy, Clustering.\n\n---\n**Shall we refine this suggestion further, proceed to the next highlighted item, or stop?**\n(Enter your feedback, 'next', or 'q' to quit)\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Enter 'q' or 'quit' to exit the refinement process.**"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your response > cmm\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*\n‚û°Ô∏è Sending your message to the AI assistant...*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### ü§ñ AI Assistant response:"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Position Name / Title: Centro de Modelamiento Matem√°tico, UNIVERSIDAD DE CHILE\n- Company Name / Org / Issuer: Centro de Modelamiento Matem√°tico, UNIVERSIDAD DE CHILE\n\n### Original Description / Highlights\nImplemented Stochastic Gradient Descent for the computation of Bayesian Wasserstein Barycenters applied to image sets. Developed a library providing algorithms for Bayesian barycenter computation. Implemented generative neural networks based on Wasserstein distance. Developed a library to obtain the Quick, Draw! dataset.\n\n### Suggested Refinement (ATS-Friendly)\n\nDeveloped and implemented machine learning algorithms, including generative neural networks, for image processing and analysis, contributing to advancements in Bayesian methods.\n- Implemented Stochastic Gradient Descent (SGD) for efficient computation of Bayesian Wasserstein Barycenters, enhancing image set analysis.\n- Developed a Python library containing algorithms for Bayesian barycenter computation, improving code reusability and accelerating research.\n- Implemented generative neural networks based on Wasserstein distance using TensorFlow, enabling the generation of realistic synthetic images.\n- Developed a data pipeline and library for efficiently obtaining and processing the Quick, Draw! dataset, facilitating model training and evaluation.\n\n### Changes Made & Keywords Used\n* **Explanation:** Focused on the machine learning aspects and highlighted the use of TensorFlow. Specified that the library was developed in Python. Added \"data pipeline\" to the description of the dataset library.\n* **Keywords Incorporated:** Machine Learning Algorithms, Generative Neural Networks, Data Pipeline, TensorFlow, Python.\n\n---\n**Shall we refine this suggestion further, proceed to the next highlighted item, or stop?**\n(Enter your feedback, 'next', or 'q' to quit)\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Enter 'q' or 'quit' to exit the refinement process.**"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your response > q\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "*\nExiting interactive refinement loop.*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "--- **Interactive Chat Finished** ---"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Start Interactive Chat Loop (Click ‚ñ∫)\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "import sys\n",
        "\n",
        "# Check if chat objects exist from previous step\n",
        "if 'chat' not in globals() or chat is None:\n",
        "    display(Markdown(\"‚ùå **Error:** Chat session is not initialized. Please run Step 7.1 first.\"))\n",
        "    sys.exit(\"Stopping: Chat not ready.\")\n",
        "if 'initial_response' not in globals() or initial_response is None:\n",
        "    display(Markdown(\"‚ùå **Error:** Initial AI response is missing. Please run Step 7.1 first.\"))\n",
        "    sys.exit(\"Stopping: Initial AI response missing.\")\n",
        "if 'content' not in globals() or 'LANGUAGE' not in globals() or LANGUAGE not in content:\n",
        "     display(Markdown(\"‚ùå **Error:** Language content for messages is missing. Check config and prompt definitions.\"))\n",
        "     sys.exit(\"Stopping: Language content missing.\")\n",
        "if 'chat_config' not in globals():\n",
        "     display(Markdown(\"‚ùå **Error:** Chat generation configuration missing.\"))\n",
        "     sys.exit(\"Stopping: Chat config missing.\")\n",
        "\n",
        "# --- Select Language for User Messages ---\n",
        "content_lang = content[LANGUAGE]\n",
        "quit_options = {\"q\", \"quit\", \"exit\", \"salir\", \"n\", \"no\"} # User inputs to exit loop\n",
        "next_options = {\"next\", \"siguiente\", \"y\", \"yes\", \"si\", \"\"} # User inputs to move to next item or confirm (empty input treated as 'next')\n",
        "\n",
        "\n",
        "# --- 1. Display Initial LLM Analysis ---\n",
        "display(Markdown(\"---\")) # Separator\n",
        "display(Markdown(f\"### ü§ñ {content_lang.get('initial_response', 'AI Initial Analysis Response:')}\")) # Use message from dict\n",
        "display(Markdown(\"---\"))\n",
        "# Display the first response text which should already be Markdown formatted by the AI\n",
        "display(Markdown(initial_response.text))\n",
        "display(Markdown(\"---\")) # Separator\n",
        "\n",
        "\n",
        "# --- 2. Interactive Refinement Loop ---\n",
        "quit_msg = content_lang.get(\"quit_msg\", \"Enter 'q' or 'quit' to exit\")\n",
        "\n",
        "while True:\n",
        "    # Display quit instructions clearly before each prompt\n",
        "    display(Markdown(f\"**{quit_msg}**\"))\n",
        "    try:\n",
        "        # Prompt user for input (input() itself doesn't use Markdown)\n",
        "        user_input = input(\"Your response > \")\n",
        "        display(Markdown(\"---\")) # Separator after user input\n",
        "\n",
        "        # Check if the user wants to exit\n",
        "        if user_input.lower().strip() in quit_options:\n",
        "            display(Markdown(f\"*{content_lang.get('exit_msg', 'Exiting interactive loop.')}*\"))\n",
        "            break # Exit the while loop\n",
        "\n",
        "        # Handle empty input as confirmation/next\n",
        "        interpreted_input = user_input\n",
        "        if user_input.strip() == \"\":\n",
        "             interpreted_input = \"next\" # Treat empty as wanting to proceed\n",
        "             display(Markdown(f\"*{content_lang.get('y_interpretation', '(Interpreting empty input as next/confirm...)')}*\"))\n",
        "\n",
        "        # Send the user's message to the LLM via the chat session\n",
        "        display(Markdown(f\"*{content_lang.get('send_message', '‚û°Ô∏è Sending message to AI...')}*\"))\n",
        "        # Pass the generation config with each message if needed\n",
        "        llm_response = chat.send_message(\n",
        "            interpreted_input,\n",
        "            config=chat_config\n",
        "            )\n",
        "        display(Markdown(f\"### {content_lang.get('received_message', 'ü§ñ AI response received:')}\"))\n",
        "        display(Markdown(\"---\")) # Separator before AI response\n",
        "\n",
        "        # Display the LLM's response using Markdown rendering\n",
        "        display(Markdown(llm_response.text))\n",
        "        display(Markdown(\"---\")) # Separator after AI response\n",
        "\n",
        "        # Check if the AI signaled exit (optional based on prompt design)\n",
        "        if \"Exiting refinement process.\" in llm_response.text:\n",
        "            display(Markdown(\"*AI indicated process is complete.*\"))\n",
        "            break\n",
        "\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        display(Markdown(f\"*{content_lang.get('keyboard_interrupt', 'Loop interrupted by user. Exiting.')}*\"))\n",
        "        break\n",
        "    except Exception as e:\n",
        "        # Catch potential errors during chat interaction (e.g., API errors)\n",
        "        display(Markdown(f\"‚ùå **ERROR during interactive chat:**\"))\n",
        "        display(Markdown(f\"   *Error Type: {type(e).__name__}*\"))\n",
        "        display(Markdown(f\"   *Error details: {e}*\"))\n",
        "        display(Markdown(f\"   *Exiting interactive loop due to error.*\"))\n",
        "        break  # Exit loop on error\n",
        "\n",
        "display(Markdown(\"--- **Interactive Chat Finished** ---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85134df0",
      "metadata": {
        "papermill": {
          "duration": 0.013142,
          "end_time": "2025-04-21T00:52:06.176952",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.163810",
          "status": "completed"
        },
        "tags": [],
        "id": "85134df0"
      },
      "source": [
        "## 8. Finished! Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a74085",
      "metadata": {
        "papermill": {
          "duration": 0.013431,
          "end_time": "2025-04-21T00:52:06.203837",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.190406",
          "status": "completed"
        },
        "tags": [],
        "id": "c0a74085"
      },
      "source": [
        "Great work! You've successfully used the AI assistant to refine key parts of your CV based on the specific job description you provided.\n",
        "\n",
        "### ‚úÖ What You Achieved:\n",
        "\n",
        "* **Loaded & Indexed:** Your CV information was loaded and organized into a \"Smart Index\".\n",
        "* **Found Relevance:** The AI identified which parts of your CV were most relevant to the target job description.\n",
        "* **Refined Content:** Through the interactive chat, you received suggestions for rewriting those relevant sections, incorporating important keywords and focusing on impact to better appeal to both ATS software and human reviewers.\n",
        "\n",
        "### üöÄ Next Steps: IMPORTANT!\n",
        "\n",
        "The AI provided suggestions, but the final step is yours:\n",
        "\n",
        "1.  **Copy the Refined Text:** Carefully scroll back through the chat output in **Step 7.2** above. Copy the final \"Suggested Refinement\" text snippets for the CV items you are happy with.\n",
        "2.  **Update Your Actual CV:** Paste the copied text into your real CV document (e.g., your Microsoft Word `.docx` file, Google Doc, or wherever you maintain your CV). Replace the old descriptions with the new, refined versions.\n",
        "3.  **Review & Polish:** Read through your entire updated CV. Ensure the new text flows well with the rest of your document, maintains your personal tone, and is free of errors. Make any final tweaks needed.\n",
        "\n",
        "Tailoring your CV for each application significantly increases your chances. Good luck with your job search!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "621f5649",
      "metadata": {
        "papermill": {
          "duration": 0.012673,
          "end_time": "2025-04-21T00:52:06.257754",
          "exception": false,
          "start_time": "2025-04-21T00:52:06.245081",
          "status": "completed"
        },
        "tags": [],
        "id": "621f5649"
      },
      "source": [
        "### About This Project & Feedback\n",
        "\n",
        "üåü **CONGRATULATIONS!** üåü\n",
        "\n",
        "If you‚Äôve made it all the way to the end of this notebook (whether you‚Äôre testing it out, a fellow data enthusiast, or just curious about the process), you definitely deserve some kudos!\n",
        "\n",
        "This project is constantly evolving, and your feedback helps keep it improving.\n",
        "\n",
        "üëâ **Show some love by giving the project's repository a star on GitHub:**\n",
        "[https://github.com/framunoz/cv-analyser-with-rag/](https://github.com/framunoz/cv-analyser-with-rag/)\n",
        "\n",
        "A single ‚≠ê helps the framework grow, motivates new features, and lets the creators know this work is valuable to the community.\n",
        "\n",
        "Thanks for reading, building, and experimenting alongside us‚Äîsee you in the next commit!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Authors\n",
        "\n",
        "* [Francisco Mu√±oz Guajardo](https://www.linkedin.com/in/femunozg/)\n",
        "* [Gabriel Ortega Hern√°ndez](https://www.linkedin.com/in/gabriel-ortega-hernandez-evaluacion-educativa/)\n",
        "\n",
        "### How to Cite This Work\n",
        "\n",
        "If you use this notebook or the underlying project in your work, please cite it as follows:\n",
        "\n",
        "**APA Style (example):**\n",
        "\n",
        "Mu√±oz Guajardo, F., & Ortega Hern√°ndez, G. (2025). *AI-Driven CV Optimisation with RAG* (Colab Notebook). Retrieved from https://github.com/framunoz/cv-analyser-with-rag/\n",
        "\n",
        "**BibTeX Entry (example):**\n",
        "\n",
        "```bibtex\n",
        "@misc{MunozOrtega2025CVA RAG,\n",
        "  author = {Mu{\\~n}oz Guajardo, Francisco and Ortega Hern{\\'a}ndez, Gabriel},\n",
        "  title = {{AI-Driven CV Optimisation with RAG (Colab Notebook)}},\n",
        "  year = {2025},\n",
        "  publisher = {GitHub},\n",
        "  journal = {GitHub repository},\n",
        "  howpublished = {\\url{[https://github.com/framunoz/cv-analyser-with-rag/](https://github.com/framunoz/cv-analyser-with-rag/)}}\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "bK-mKdjXh6Hz"
      },
      "id": "bK-mKdjXh6Hz"
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 97258,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 70.308237,
      "end_time": "2025-04-21T00:52:07.595838",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-04-21T00:50:57.287601",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2bc4461ed40448d19c5c56f4ab9ed1d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_223f1a4d7c1249f29a77fd254b214fce",
            "placeholder": "Paste the full job description here...",
            "rows": null,
            "style": "IPY_MODEL_0b3c60eaa79543408838c2bbfb45e82f",
            "value": "Job Title: AI/Machine Learning Engineer\n\nCompany: Innovate Solutions Inc.\n\nLocation: Remote (US Based)\n\nAbout Us:\nInnovate Solutions Inc. is at the forefront of applying artificial intelligence to solve real-world business challenges. We foster a collaborative environment where creative thinking and technical excellence drive our success. We are passionate about building intelligent systems that deliver significant value to our clients across various industries. Join our growing team and help shape the future of applied AI.\n\nAbout the Role:\nWe are seeking a talented and motivated AI/Machine Learning Engineer to join our core development team. You will play a key role in the end-to-end lifecycle of machine learning projects, from conceptualization and data exploration to model deployment and monitoring. You'll work closely with data scientists, software engineers, and product managers to build innovative AI-powered features and products.\n\nResponsibilities:\n- Design, develop, train, and deploy machine learning models (including deep learning models) for tasks such as NLP, predictive analytics, anomaly detection, and personalization.\n- Process, cleanse, and verify the integrity of large datasets used for analysis and model training.\n- Collaborate with data engineering teams to build and maintain robust data pipelines for ML workflows.\n- Implement and maintain MLOps best practices for model versioning, testing, deployment, and monitoring.\n- Stay current with the latest advancements in AI/ML techniques, tools, and platforms.\n- Analyze experimental results, iterate on models, and communicate findings to technical and non-technical stakeholders.\n- Contribute to the development of internal AI platforms and tooling.\n\nRequired Qualifications:\n- Bachelor's or Master's degree in Computer Science, Data Science, Statistics, or a related quantitative field.\n- 2+ years of hands-on experience building and deploying machine learning models in a production environment.\n- Strong programming skills in Python and proficiency with relevant ML libraries (e.g., Scikit-learn, TensorFlow, PyTorch, Keras).\n- Solid understanding of core machine learning algorithms, statistical modeling, and evaluation metrics.\n- Experience working with SQL and/or NoSQL databases.\n- Familiarity with data processing and analysis libraries (e.g., Pandas, NumPy).\n- Excellent problem-solving skills and attention to detail.\n- Strong communication and teamwork abilities.\n\nDesired Qualifications (Bonus Points):\n- PhD in a related field.\n- Experience with cloud platforms (AWS, GCP, or Azure) and their AI/ML services (e.g., SageMaker, Vertex AI, Azure ML).\n- Experience with MLOps tools and practices (e.g., Docker, Kubernetes, MLflow, Kubeflow).\n- Experience with Natural Language Processing (NLP) or Computer Vision (CV).\n- Experience with big data technologies (e.g., Spark, Hadoop).\n- Publications in relevant AI/ML conferences or journals.\n\nWhat We Offer:\n- Competitive salary and benefits package.\n- Opportunity to work on challenging and impactful AI projects.\n- A dynamic, collaborative, and supportive work environment.\n- Flexible remote work policy.\n- Professional development opportunities.\n"
          }
        },
        "223f1a4d7c1249f29a77fd254b214fce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "400px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "95%"
          }
        },
        "0b3c60eaa79543408838c2bbfb45e82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}